<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>(TH) Deep Learning with More Sensation | The Thought of WhyBoyBurin</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="(TH) Deep Learning with More Sensation" />
<meta name="author" content="Burin Sirisrimungkorn" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="แล้วถ้าเราอยากจะสร้าง Deep Learning program ขึ้นมาจริงๆจะต้องทำยังไงนะ" />
<meta property="og:description" content="แล้วถ้าเราอยากจะสร้าง Deep Learning program ขึ้นมาจริงๆจะต้องทำยังไงนะ" />
<link rel="canonical" href="https://burins.github.io/whyboyburin/deep%20learning/2020/09/22/Deep-Learning-with-More-Sensation.html" />
<meta property="og:url" content="https://burins.github.io/whyboyburin/deep%20learning/2020/09/22/Deep-Learning-with-More-Sensation.html" />
<meta property="og:site_name" content="The Thought of WhyBoyBurin" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-22T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://burins.github.io/whyboyburin/deep%20learning/2020/09/22/Deep-Learning-with-More-Sensation.html","@type":"BlogPosting","headline":"(TH) Deep Learning with More Sensation","dateModified":"2020-09-22T00:00:00-05:00","datePublished":"2020-09-22T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://burins.github.io/whyboyburin/deep%20learning/2020/09/22/Deep-Learning-with-More-Sensation.html"},"author":{"@type":"Person","name":"Burin Sirisrimungkorn"},"description":"แล้วถ้าเราอยากจะสร้าง Deep Learning program ขึ้นมาจริงๆจะต้องทำยังไงนะ","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/whyboyburin/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://burins.github.io/whyboyburin/feed.xml" title="The Thought of WhyBoyBurin" /><link rel="shortcut icon" type="image/x-icon" href="/whyboyburin/images/favicon.ico"><link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/whyboyburin/">The Thought of WhyBoyBurin</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/whyboyburin/about/">About Me</a><a class="page-link" href="/whyboyburin/search/">Search</a><a class="page-link" href="/whyboyburin/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">(TH) Deep Learning with More Sensation</h1><p class="page-description">แล้วถ้าเราอยากจะสร้าง Deep Learning program ขึ้นมาจริงๆจะต้องทำยังไงนะ</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-09-22T00:00:00-05:00" itemprop="datePublished">
        Sep 22, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Burin Sirisrimungkorn</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      15 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/whyboyburin/categories/#deep learning">deep learning</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#เมื่อต้องเผชิญหน้ากับ-Dataset">เมื่อต้องเผชิญหน้ากับ Dataset </a>
<ul>
<li class="toc-entry toc-h2"><a href="#MNIST-Dataset">MNIST Dataset </a></li>
<li class="toc-entry toc-h2"><a href="#โอเค-โอเค-เรามาลุยกันเลย">โอเค โอเค เรามาลุยกันเลย </a></li>
<li class="toc-entry toc-h2"><a href="#มาลองดูตัวอย่างกัน">มาลองดูตัวอย่างกัน </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ทำการ-Preprocess-รูปภาพ">ทำการ Preprocess รูปภาพ </a></li>
<li class="toc-entry toc-h1"><a href="#ทำไมเราต้องทำการแปลงคำตอบให้อยู่ในรูปของ-Vector-?">ทำไมเราต้องทำการแปลงคำตอบให้อยู่ในรูปของ Vector ? </a></li>
<li class="toc-entry toc-h1"><a href="#ก่อนมาเริ่มทำการสร้าง-Neural-Network-program-มารู้จัก-Activation-function-กันก่อน">ก่อนมาเริ่มทำการสร้าง Neural Network program มารู้จัก Activation function กันก่อน </a>
<ul>
<li class="toc-entry toc-h2"><a href="#คุณสมบัติของ-Activation-function">คุณสมบัติของ Activation function </a>
<ul>
<li class="toc-entry toc-h3"><a href="#1.-Activation-functions-ต้องเป็น-Continuous-function">1. Activation functions ต้องเป็น Continuous function </a></li>
<li class="toc-entry toc-h3"><a href="#2.-Activation-functions-ที่ดี-จะต้องไม่เปลี่ยนแปลงทิศทาง">2. Activation functions ที่ดี จะต้องไม่เปลี่ยนแปลงทิศทาง </a></li>
<li class="toc-entry toc-h3"><a href="#3.-Activation-functions-เป็น-Non-Linear-functions">3. Activation functions เป็น Non Linear functions </a></li>
<li class="toc-entry toc-h3"><a href="#4.-Activation-functions-ต้องสามารถคำนวณได้ในเชิงใช้งานจริง-รวมถึง-derivative-ของตัว-function-เองด้วย">4. Activation functions ต้องสามารถคำนวณได้ในเชิงใช้งานจริง รวมถึง derivative ของตัว function เองด้วย </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#เรามาดู-Activation-functions-ที่ถูกใช้ใน-Hidden-layer-กัน">เรามาดู Activation functions ที่ถูกใช้ใน Hidden layer กัน </a>
<ul>
<li class="toc-entry toc-h3"><a href="#1.-Sigmoid-function">1. Sigmoid function </a></li>
<li class="toc-entry toc-h3"><a href="#2.-tanh-function">2. tanh function </a></li>
<li class="toc-entry toc-h3"><a href="#3.relu-function">3.relu function </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#เรามาดู-Activation-functions-ที่ถูกใช้ใน-output-layer-กัน">เรามาดู Activation functions ที่ถูกใช้ใน output layer กัน </a>
<ul>
<li class="toc-entry toc-h3"><a href="#1.-ต้องการคำตอบที่เป็น-Numeric">1. ต้องการคำตอบที่เป็น Numeric </a></li>
<li class="toc-entry toc-h3"><a href="#2.-มีคำตอบหลายคำตอบและมีคำตอบที่ถูกต้องมากกว่า-1-คำตอบ">2. มีคำตอบหลายคำตอบและมีคำตอบที่ถูกต้องมากกว่า 1 คำตอบ </a></li>
<li class="toc-entry toc-h3"><a href="#3.-มีคำตอบหลายคำตอบและมีคำตอบที่ถูกต้องเพียงคำตอบเดียว">3. มีคำตอบหลายคำตอบและมีคำตอบที่ถูกต้องเพียงคำตอบเดียว </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#มาเริ่มสร้าง-Neural-Network-program-สำหรับ-Dataset-กันดีกว่า">มาเริ่มสร้าง Neural Network program สำหรับ Dataset กันดีกว่า </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#ยินดีต้อนรับ-Dropout">ยินดีต้อนรับ Dropout </a>
<ul>
<li class="toc-entry toc-h2"><a href="#การสร้าง-neural-network-program-มาเยอะๆหลาย-program-แล้วทำการเฉลี่ยคำตอบ-(Ensembling)">การสร้าง neural network program มาเยอะๆหลาย program แล้วทำการเฉลี่ยคำตอบ (Ensembling) </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#พอแล้ว-Theory-มาลงมือจริงกันเลยดีกว่า">พอแล้ว Theory มาลงมือจริงกันเลยดีกว่า </a></li>
<li class="toc-entry toc-h1"><a href="#สุดท้ายแล้ว-ถ้าเราอยากทำ-Batch-Gradient-Descent-ละจะทำยังไง">สุดท้ายแล้ว ถ้าเราอยากทำ Batch Gradient Descent ละจะทำยังไง </a></li>
<li class="toc-entry toc-h1"><a href="#Acknowledge">Acknowledge </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-09-22-Deep-Learning-with-More-Sensation.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="เมื่อต้องเผชิญหน้ากับ-Dataset">
<a class="anchor" href="#%E0%B9%80%E0%B8%A1%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B9%80%E0%B8%9C%E0%B8%8A%E0%B8%B4%E0%B8%8D%E0%B8%AB%E0%B8%99%E0%B9%89%E0%B8%B2%E0%B8%81%E0%B8%B1%E0%B8%9A-Dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>เมื่อต้องเผชิญหน้ากับ Dataset<a class="anchor-link" href="#%E0%B9%80%E0%B8%A1%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B9%80%E0%B8%9C%E0%B8%8A%E0%B8%B4%E0%B8%8D%E0%B8%AB%E0%B8%99%E0%B9%89%E0%B8%B2%E0%B8%81%E0%B8%B1%E0%B8%9A-Dataset"> </a>
</h1>
<p>เมื่อเราต้องการที่จะสร้าง neural network program สิ่งที่เราต้องมีก็คือ dataset เปรียบได้กับมันคือคลังข้อมูลที่รอให้เราได้เข้าไปเรียนรู้ พอเราจบการเรียนรู้ เราก็จะมีความสามารถในการทำหรือแก้ปัญหาสิ่งนั้นๆตามที่เราได้เรียนหรือศึกษามา</p>
<h2 id="MNIST-Dataset">
<a class="anchor" href="#MNIST-Dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>MNIST Dataset<a class="anchor-link" href="#MNIST-Dataset"> </a>
</h2>
<p>dataset อันนี้จะเป็นการรวมข้อมูลเกี่ยวกับ ตัวเลขที่ถูกเขียนด้วยลายมือมนุษย์ โดยได้มีการเก็บรวบรวมโดย National Institute of Standards and Technology และได้ถูกทำมาจัดรูปแบบและแก้ไขปรับปรุงให้เป็น dataset ที่ใช้สำหรับ machine learning โดย Yann Lecun 
MNIST ได้ถูก Yann Lecun นำไปใช้ในปี 1998 สำหรับการสร้าง neural network ซึ่ง LeNet-5 คือ program ตัวแรกที่ใช้สำหรับการจัดกลุ่มตัวเลขที่เขียนด้วยลายมือมนุษย์ นี่เป็นเหตุการณ์สำคัญอีกหน้าหนึ่งของประวัติศาสตร์ AI</p>
<h2 id="โอเค-โอเค-เรามาลุยกันเลย">
<a class="anchor" href="#%E0%B9%82%E0%B8%AD%E0%B9%80%E0%B8%84-%E0%B9%82%E0%B8%AD%E0%B9%80%E0%B8%84-%E0%B9%80%E0%B8%A3%E0%B8%B2%E0%B8%A1%E0%B8%B2%E0%B8%A5%E0%B8%B8%E0%B8%A2%E0%B8%81%E0%B8%B1%E0%B8%99%E0%B9%80%E0%B8%A5%E0%B8%A2" aria-hidden="true"><span class="octicon octicon-link"></span></a>โอเค โอเค เรามาลุยกันเลย<a class="anchor-link" href="#%E0%B9%82%E0%B8%AD%E0%B9%80%E0%B8%84-%E0%B9%82%E0%B8%AD%E0%B9%80%E0%B8%84-%E0%B9%80%E0%B8%A3%E0%B8%B2%E0%B8%A1%E0%B8%B2%E0%B8%A5%E0%B8%B8%E0%B8%A2%E0%B8%81%E0%B8%B1%E0%B8%99%E0%B9%80%E0%B8%A5%E0%B8%A2"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"จำนวนข้อมูลใน Training set: </span><span class="si">{</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"จำนวนข้อมูลใน Testing set: </span><span class="si">{</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>จำนวนข้อมูลใน Training set: 60000
จำนวนข้อมูลใน Testing set: 10000
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>เราทำการโหลดข้อมูล MNIST จาก Keras ซึ่งเป็น framework ที่ใช้สำหรับสร้าง neural network program โดย Keras จะมี datasets ที่เตรียมไว้ให้บางส่วน (MNIST ก็เป็นหนึ่งในนั้น) จะสังเกตได้ว่าข้อมูลจะถูกแบ่งแยกออกมาเป็น 2 ส่วนนั่นคือ training และ testing set ทำไมกันนะ?
อย่างที่เรารู้ในตอนนี้คือ เราพยายามจะสร้าง prgram ด้วยข้อมูล เมื่อผลลัพธ์ทดสอบดูกับข้อมูลที่เราใช้สร้างแล้วความสามารถในการให้คำตอบเราดีเยี่ยม แต่ แต่... นี่ก็ไม่ได้การันตีว่า พอเรานำไปใช้จริงกับข้อมูลที่ไม่ได้อยู่ในกลุ่ม training จะเกิดอะไรชึ้น ดังนั้น เราเลยต้องมีข้อมูลอีกกลุ่มมาทำเพื่อทำการทดสอบว่าถ้าเป็นข้อมูลที่ไม่ได้อยู่ในกลุ่มที่ใช้เรียนรู้ ความรู้หรือ program ที่เราได้มา ยังจะสามารถแก้ไขปัญหาได้อย่างถูกต้องไหม 
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>แน่นอนว่า training และ testing set ข้อมูลข้างในจะต้องไม่ใช่ตัวเดียวกัน แต่คุณลักษณะของข้อมูลทั้ง 2 กลุ่มยังต้องใกล้เคียงกัน (มาจาก statistical distribution เดี่ยวกัน) เราคงไม่สามารถเอาความรู้ที่เรียนจากภาพถ่ายรูปมุมหนึ่ง ไปใช้กับปัญหาที่เป็นรูปประเภทเดียวกันแต่ถ่ายจากอีกมุมนึงที่ไม่ปรากฏในคลังข้อมูลที่ใช้เรียนรู้
</div>
นอกจากที่ถ้าสังเกตเห็นมันจะมีข้อมูลอีก 2 ตัวแปรที่ขึ้นต้นด้วย "y" มันคืออะไรกันนะ? จริงๆแล้วมันหมายถึงตัวแปรที่เก็บ labels หรือก็คือคำตอบที่ถูกผูกติดกับตัวอย่าง โดยในแต่ละตัวอย่างก็จะถูกผูกกับ label เอ๊ะแล้วค่าในตัวอย่างมันเป็นอะไรได้บ้าง?
เราจะประกาศว่าคำตอบของปัญหานี้เป็นอะไรได้บ้างซึ่งแต่ละคำตอบเราจะเรียกว่า class ดังนั้น MNIST ที่มีคำตอบที่เป็นไปได้คือ 0 - 9 จะมีคำตอบทั้งหมด 10 คำตอบ ถ้าเรามองเป็น predefined set of classes จะได้เป็น {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}
<h2 id="มาลองดูตัวอย่างกัน">
<a class="anchor" href="#%E0%B8%A1%E0%B8%B2%E0%B8%A5%E0%B8%AD%E0%B8%87%E0%B8%94%E0%B8%B9%E0%B8%95%E0%B8%B1%E0%B8%A7%E0%B8%AD%E0%B8%A2%E0%B9%88%E0%B8%B2%E0%B8%87%E0%B8%81%E0%B8%B1%E0%B8%99" aria-hidden="true"><span class="octicon octicon-link"></span></a>มาลองดูตัวอย่างกัน<a class="anchor-link" href="#%E0%B8%A1%E0%B8%B2%E0%B8%A5%E0%B8%AD%E0%B8%87%E0%B8%94%E0%B8%B9%E0%B8%95%E0%B8%B1%E0%B8%A7%E0%B8%AD%E0%B8%A2%E0%B9%88%E0%B8%B2%E0%B8%87%E0%B8%81%E0%B8%B1%E0%B8%99"> </a>
</h2>
<p>เดี๋ยวเราลองมาดูตัวอย่างแรกของ training set กัน</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>จากภาพและผลลัพธ์จะเห็นได้ว่าตัวอย่างอันแรกใน training set เป็นเลข 5 โดยภาพมีขนาด 28 x 28 (height x width เนื่องจาก numpy จะแสดง row ก่อนแล้วค่อย column) แต่ก่อนที่เราจะเริ่มสร้าง neural network program กัน เราต้องทำอะไรเพิ่มอีกหน่อย</p>
<ul>
<li>ทำการ preprocess ค่าของแต่ละตัวอย่าง</li>
<li>ทำการแปลงคำตอบให้อยู่ในรูปของ vector</li>
</ul>
<h1 id="ทำการ-Preprocess-รูปภาพ">
<a class="anchor" href="#%E0%B8%97%E0%B8%B3%E0%B8%81%E0%B8%B2%E0%B8%A3-Preprocess-%E0%B8%A3%E0%B8%B9%E0%B8%9B%E0%B8%A0%E0%B8%B2%E0%B8%9E" aria-hidden="true"><span class="octicon octicon-link"></span></a>ทำการ Preprocess รูปภาพ<a class="anchor-link" href="#%E0%B8%97%E0%B8%B3%E0%B8%81%E0%B8%B2%E0%B8%A3-Preprocess-%E0%B8%A3%E0%B8%B9%E0%B8%9B%E0%B8%A0%E0%B8%B2%E0%B8%9E"> </a>
</h1>
<p>ตอนนี้ค่าที่อธิบายถึงรูปภาพเรามีค่าเป็นยังไงนะ</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>จะเห็นว่ามีค่าตั้งแต่ 0 จนถึง 253 (จริงๆค่าที่เป็นไปได้มากสุดคือ 255 เนื่องจากข้อมูลเก็บเป็น unsign 8 bits) แล้วแบบนี้หมายความว่ายังไง เวลา program เราเรียนรู้ กลุ่มตัวแปรที่มีค่ามากจะเป็นตัวแปรที่มีอำนาจในการตัดสินให้คำตอบ ทั้งที่ตัวแปรอื่นๆที่ค่าน้อยก็มีโอกาสที่เป็นตัวแปรที่มีความสำคัญต่อการผลิตคำตอบ ซึ่งมีผลกับการเรียนรู้ ดังนั้นเราต้องนำค่าตัวแปรต่างๆที่ใช้อธิบายรูปภาพมาทำการ normalization โดยในกรณีนี้คือนำค่าทั้งหมดมาหารด้วย 255 ซึ่งการทำอย่างนี้นอกจากจะทำให้ความสำคัญของแต่ละตัวแปรมีน้ำหนักพอๆกันแล้ว ยังช่วยให้เรียนรู้ได้เร็วขึ้นด้วย (จินตนาการว่ากราฟแสดงความสัมพันธ์ระหว่าง error และ weight 2 ตัว ที่การส่ายไปส่ายมาแทนที่จะตรงไปยังจุดที่เป็นเป้าหมาย)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>เดี๋ยวเรามาทำการ normalization รูปภาพทั้งหมดกันทั้งใน training และ testing set</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))[</span><span class="mi">11000</span><span class="p">:</span><span class="mi">12000</span><span class="p">]</span>

<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"มาดูข้อมูลรูปภาพใน training set กัน :</span><span class="si">{</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"มาดูข้อมูลรูปภาพใน testing set กัน :</span><span class="si">{</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>มาดูข้อมูลรูปภาพใน training set กัน :(1000, 784)
มาดูข้อมูลรูปภาพใน testing set กัน :(10000, 784)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>จะเห็นว่าเราทำการหารด้วย 255 เพื่อทำให้ตัวแปรต่างๆในแต่ละรูปภาพมีความสำคัญต่อคำตอบพอๆกัน นอกจากนี้ถ้าจำได้ neural netwrok program จะรับ inputs เข้ามาเป็น 1D list หรือ vector เนื่องจากรูปภาพเราถูกนำเสนอเป็น 28 x 28 เราทำการแปลงเป็น vector โดยการใช้ reshape method ซึ่งเป็นฟังก์ชันที่ใช้สำหรับปรับ representation ของ array ในกรณีนี้เราปรับจากการนำเสนอในรูปแบบ metric เป็น vector
โดยถ้าสังเกตใน training set จาก 60,000 ตัวอย่าง ในกรณีนี้ผมหยิบมาแค่ 1,000 ตัวอย่างสำหรับใช้สร้าง neural network program (เพ่ือความเร็ว)ถ้าต้องการใช้ตัวอย่างทั้งหมดก็ทำการลบในส่วนการเลือกตัวอย่างออกไปนะครับ ส่วน testing set ยังคงจำนวนตัวอย่างไว้เท่าเดิม</p>
<h1 id="ทำไมเราต้องทำการแปลงคำตอบให้อยู่ในรูปของ-Vector-?">
<a class="anchor" href="#%E0%B8%97%E0%B8%B3%E0%B9%84%E0%B8%A1%E0%B9%80%E0%B8%A3%E0%B8%B2%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%97%E0%B8%B3%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%81%E0%B8%9B%E0%B8%A5%E0%B8%87%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B9%83%E0%B8%AB%E0%B9%89%E0%B8%AD%E0%B8%A2%E0%B8%B9%E0%B9%88%E0%B9%83%E0%B8%99%E0%B8%A3%E0%B8%B9%E0%B8%9B%E0%B8%82%E0%B8%AD%E0%B8%87-Vector-?" aria-hidden="true"><span class="octicon octicon-link"></span></a>ทำไมเราต้องทำการแปลงคำตอบให้อยู่ในรูปของ Vector ?<a class="anchor-link" href="#%E0%B8%97%E0%B8%B3%E0%B9%84%E0%B8%A1%E0%B9%80%E0%B8%A3%E0%B8%B2%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%97%E0%B8%B3%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%81%E0%B8%9B%E0%B8%A5%E0%B8%87%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B9%83%E0%B8%AB%E0%B9%89%E0%B8%AD%E0%B8%A2%E0%B8%B9%E0%B9%88%E0%B9%83%E0%B8%99%E0%B8%A3%E0%B8%B9%E0%B8%9B%E0%B8%82%E0%B8%AD%E0%B8%87-Vector-?"> </a>
</h1>
<p>สังเกตว่าถ้าปัญหาเรามีคำตอบเดียวเราจะแทนที่ด้วย output node แค่ node เดียวแต่ถ้าเรามีหลายคำตอบเราจะมี output nodes มากกว่า 1 node ทำไมเราไม่ใช้ node เดียวแล้วให้ค่าคำตอบเป็น 0 - 9 ละ?
เนื่องจากถ้าเราทำแบบนั้น เท่ากับเราบอก program เราด้วยว่าความสัมพันธ์ระหว่างคำตอบของเราจะมี semantic relationship นั่นคือ คำตอบที่เป็นเลข 8 มีค่ามากกว่า เลข 7 ซึ่งความจริงถ้ามองในมุมตัวเลขก็ถูก และนอกจากนี้คำตอบที่เราได้ยังมีโอกาสที่จะเป็นค่าระหว่างคำตอบจริงๆที่เราต้องการ เช่นคำตอบออกมาเป็น 7.5 แต่คราวนี้ถ้ามองใบริบทที่เรากำลังทำอยู่ เราต้องการแค่อยากรู้ว่าจากรูปที่ได้มาตัว program ต้องบอกเราให้ได้ว่ามันเป็นเลขอะไรโดยไม่สนว่าเลขนั้นจะมีค่ากว่าอีกเลขไหม
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>คำตอบในกรณี MNIST เป็น discrete value และไม่มีความสัมพันธ์กันระหว่าง value
</div>
<p>ซึ่งจะนำไปสู่เรื่องของการเลือกแสดงคำตอบของ output layer โดยถ้าคำตอบเราเป็น numeric หรือเป็น continuous value เราก็สามารถนำเสนอ output layer ได้ด้วย node เดียวให้คำตอบเป็น numeric ในช่วงคำตอบที่สนใจ เช่น [-1, 1] ใช้สามารถ tanh เป็น activation function หรือถ้าไม่มีการกำหนดขอบเขตตายตัวก็ไม่ต้องมี activation function แต่ถ้าคำตอบที่เราต้องการเป็น category หรือ predefined set of classes จะทำให้ output layer เราต้องนำเสนอด้วย nodes ที่มีจำนวนเท่ากับ classes โดยในแต่ละ node จะให้ค่าออกเป็นความน่าจะเป็นว่ามีโอกาสเป็น class นี้หรือเปล่า โดยจะเป็น unrelated probability (แต่ละ node มีความน่าจะเป็นของตัวเองไม่เกี่ยวข้องกัน เช่น sigmoid function) หรือ related probability (แต่ละ node ใน layer มีความน่าจะเป็นที่เกี่ยวข้องกัน เช่น การใช้  softmax function)</p>
<p>สรุปคือคำตอบของปัญหา MNIST เป็นแบบ discrete value และ order ไม่สำคัญเนื่องจากไม่มีความสัมพันธ์ระหว่าง class ดังนั้นเราจะทำการแปลงจากข้อมูลปัจจุบันที่แสดงคำตอบเป็นค่าๆเดียว {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} ให้เป็น vector ที่แต่ละ node คือตัวแทนว่าเป็น node ที่บ่งบอกถึงเลขอะไร เช่น node ที่ 0 เป็นตัวแทนของเลข 0 ส่วน node ที่ 8 เป็นตัวแทนของเลข 8 โดยค่าในแต่ละ node บ่งบอกถึงความน่าจะเป็นที่จะเป็นเลขนั้น 
ขอยกตัวอย่างเช่นเลข 8 เมื่อเราทำการแปลงเป็น vector จะได้เป็น [0, 0, 0, 0, 0, 0, 0, 0, 1, 0] จะเห็นได้ว่า node ที่ไม่ได้เป็นคำตอบจะถูกแทนที่ค่าด้วย 0 ส่วน node ที่เป็นคำตอบจะถูกแทนที่ค่าด้วย 1 ถ้าลองแปลงตัวเลขอื่นๆด้วยจะทำให้เราเห็นรูปแบบบางอย่างนั่นคือตัวเลขที่ไม่ใช่คำตอบจะเป็น 0 ทั้งหมด และตัวเลขที่เป็นคำตอบจะเป็น 1 และ! เนื่องจากคำตอบเรามีแค่คำตอบเดียว node ที่เป็น 1 เลยมีแค่ node เดียวที่เหลือเป็น 0 เราเลยเรียกการแปลงข้อมูลจาก numeric เป็น vector ว่าการทำ one hot encoder แน่นอว่าเรานำเสนอข้อมูลอบบนี้ในกรณีที่ปัญหาเราเป็นแบบ multiclass problem นั่นคือมีแค่คำตอบเดียวที่ถูกต้องจากทั้งหมด แต่... ถ้าปัญหาเราเป็น multilabel multiclass หรือจากคำตอบทั้งหมดสามารถตอบได้มากกว่า 1 คำตอบ node ที่เป็น 1 ก็จะมีมากกว่า 1 node</p>
<p>โอเค เรามาลองทำการ one hot encoder กันดีกว่า</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_train_one_hot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_train_one_hot</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">y_train_one_hot</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train_one_hot</span><span class="p">[</span><span class="mi">11000</span><span class="p">:</span><span class="mi">12000</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"มาดูข้อมูลคำตอบใน training set กัน :</span><span class="si">{</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"คำตอบของตัวอย่างแรกคือ </span><span class="si">{</span><span class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> นั่นคือเลข 5 นั่นเอง"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>มาดูข้อมูลคำตอบใน training set กัน :(1000, 10)
คำตอบของตัวอย่างแรกคือ [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] นั่นคือเลข 5 นั่นเอง
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>แรกเริ่มเราทำการสร้างโครงสร้างของ vectors ราออกมาก่อน(metric เมื่อนำเอา vectors ทั้งหมดมารวมกันเป็นคลังข้อมูลเกี่ยวกับคำตอบ)นั้นคือกำหนดให้แต่ละ vector ที่เป็นตัวแทนของคำตอบในแต่ละตัวอย่างให้ทุก nodes เป็น 0 หมด หลังจากนั้นเราจะไล่แทนค่า 1 คำไปยัง node ที่เป็นตัวแทนคำจอบของแต่ละตัวอย่าง (ในกรณีนี้ vector นึงจะมี node เดียวที่เป็น 0 เนื่องจากคำตอบที่ถูกต้องมีเพียงคำตอบเดียว)
เรามาทำแบบเดียวกันกับคำตอบ testing set กันดูบ้าง</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_test_one_hot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_test_one_hot</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">y_test_one_hot</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">y_test</span> <span class="o">=</span> <span class="n">y_test_one_hot</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"มาดูข้อมูลคำตอบใน testing set กัน :</span><span class="si">{</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"คำตอบของตัวอย่างแรกคือ </span><span class="si">{</span><span class="n">y_test</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>มาดูข้อมูลคำตอบใน testing set กัน :(10000, 10)
คำตอบของตัวอย่างแรกคือ [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>จะเห็นได้ว่าเราเปลี่ยนคำตอบที่แสดงในรูปแบบ numeric ให้กลายเป็น one hot vector ได้สำเร็จ</p>
<h1 id="ก่อนมาเริ่มทำการสร้าง-Neural-Network-program-มารู้จัก-Activation-function-กันก่อน">
<a class="anchor" href="#%E0%B8%81%E0%B9%88%E0%B8%AD%E0%B8%99%E0%B8%A1%E0%B8%B2%E0%B9%80%E0%B8%A3%E0%B8%B4%E0%B9%88%E0%B8%A1%E0%B8%97%E0%B8%B3%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87-Neural-Network-program-%E0%B8%A1%E0%B8%B2%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B8%88%E0%B8%B1%E0%B8%81-Activation-function-%E0%B8%81%E0%B8%B1%E0%B8%99%E0%B8%81%E0%B9%88%E0%B8%AD%E0%B8%99" aria-hidden="true"><span class="octicon octicon-link"></span></a>ก่อนมาเริ่มทำการสร้าง Neural Network program มารู้จัก Activation function กันก่อน<a class="anchor-link" href="#%E0%B8%81%E0%B9%88%E0%B8%AD%E0%B8%99%E0%B8%A1%E0%B8%B2%E0%B9%80%E0%B8%A3%E0%B8%B4%E0%B9%88%E0%B8%A1%E0%B8%97%E0%B8%B3%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87-Neural-Network-program-%E0%B8%A1%E0%B8%B2%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B8%88%E0%B8%B1%E0%B8%81-Activation-function-%E0%B8%81%E0%B8%B1%E0%B8%99%E0%B8%81%E0%B9%88%E0%B8%AD%E0%B8%99"> </a>
</h1>
<p>ก่อนอื่นในแต่ละ layer (ที่ไม่ใช่ output layer) เรายังไม่มีการใช้ activation function ในรอบนี้เราจะมาใช้กัน โดย function ที่เราจะใช้ก็คือ relu นั่นเอง ฟังก์ชันมีความง่ายมากนั่นคือ ถ้าค่ามากกว่า 0 ให้ทำการคงคำตอบค่าเดิมเอาไว้ แต่ถ้าค่าน้อยกว่าหรือเท่ากับ 0 ให้ทำการแปลงคำตอบให้กลายเป็น 0 
</p>
<div class="flash flash-error">
    <svg class="octicon octicon-alert octicon octicon-alert octicon octicon-alert" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.22 1.754a.25.25 0 00-.44 0L1.698 13.132a.25.25 0 00.22.368h12.164a.25.25 0 00.22-.368L8.22 1.754zm-1.763-.707c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0114.082 15H1.918a1.75 1.75 0 01-1.543-2.575L6.457 1.047zM9 11a1 1 0 11-2 0 1 1 0 012 0zm-.25-5.25a.75.75 0 00-1.5 0v2.5a.75.75 0 001.5 0v-2.5z"></path></svg>
    <strong>Warning: </strong>ทำไมต้อง activation function หรือให้เฉพาะเจาะจงทำไมต้อง non linear activation function เนื่องจากถ้าเราไม่มี function นี้ ตัวโปรแกรมเราถึงแม้จะเป็น neural network ที่มี 10 layers ก็ไม่ช่วยอะไรเนื่องจาก neural network แค่ 2 layers ก็สามารถให้คำตอบเดียวกับ neural network ที่มี 10 layers ลองคิดในแง่ของ correlation ว่าถ้า weights คือ correlation ระหว่าง layers โดยจุดประสงค์หลักของ program ในการเรียนรู้คือการหา correlation ระหว่าง inputs และ outputs ถ้าเราไม่มี non linear activation function จะทำให้ correlation ในแต่ละช่วงของตัวโปรแกรม (ในกรณีนี้เรามี hidden layer) สุดท้ายก็ขึ้นอยู่กับ  correlation ที่มีกับ inputs layer (นี่แหละ! ที่ทำให้ถึงแม้เรามี layer เยอะแค่ไหนก็ตาม คำตอบสุดท้ายก็สามารถผลิตได้ด้วย program ที่มีแค่ input และ output layer) แต่ถ้าเรามีการใช้ activation function นั้นเท่ากับว่าในแต่ละ layer มีสิทธิ์ที่จะสร้าง correlation ของตัวเองขึ้นมา โดยไม่ได้ขึ้นอยู่กับแค่ฝั่งของ input layer ทำให้เราสามารถสร้าง correlation ที่ซับซ้อน และสุดท้ายก็สร้าง indirect correlation ได้ โดย correlation จาก input layer เป็นแค่ส่วนหนึ่งของ correlation ทั้งหมด ที่ตัวโปรแกรมพยายามสร้างขึ้นมา (ไม่เหมือนกับการใช้ correlation โดยตรงระหว่าง input layer และ output layer) 
</div>
<p>เหตุผลว่าทำไมต้อง activation function ผมขอลองใช้การเปรียบเทียบดูนะครับ จินตนาการดูครับว่า เรามีสร้อยคออันนึงเล็กๆแต่อยากกจะเซอไพส์แฟน แต่ในตอนนี้มีแค่กระดาษ A4 จำนวนนึง (หากล้องของขวัญไม่ทันแล้ว) เราก็เลยคิดว่างั้นเอาเลยละกัน เราเอา A4 ห่อสร้อยเอาไว้หลายๆชั้นและมีการขยัมให้เป็นก้อนกลมๆ พอเราส่งให้แฟน แฟนก็ตอบว่า "มันคืออะไรเนี่ย" แฟนมี 2 ตัวเลือกคือ</p>
<ul>
<li>คลี่</li>
<li>ดีด</li>
</ul>
<p>โดยการคลี่หนึ่ง ครั้งเรามองเป็น 1 step และการดีดหนึ่งครั้งก็มองเป็น 1 step ดุแล้วปัญหานี้ก็เป็นปัญหาที่ซับซ้อนพอควรเพราะเราต้องคลี่ด้วยมุม, แรงและทิศทางที่ไม่มีกำหดตายตัวจนกระทั้งเจอสสร้อย ทุกครั้งที่เราคลี่เราเปลี่ยนแปลง representation ทุกครั้ง เราคลี่ไปเรื่อยๆ แล้วหยุดคิดแปปนึงว่าจาก representation ปัจจุบันของก้อนกระดาษที่ห่อสร้อยกับก้อนกระดาษที่ผ่านการคลี่แค่ครั้งเดียวก่อนหน้านี้ จะเห็นว่าเรามาไกลแค่ไหนและการคลี่ของเราเมื่อมองมันเป็น list ของ actions มีความซับซ้อนเมื่อเทียบกับตอนเราพึ่งจะคลี่ไปรอบเดียว แต่เราไม่รู้สึกถึงความซับซ้อนเพราะแต่ละ step เราทำแค่ขั้นตอนง่ายๆคือการคลี่ แต่ถ้าเรายังไม่เริ่มคลี่แล้วต้องการทำการคลี่ทีเดียวเพื่อให้ได้ผลลัพธ์เหมือนกับที่เราคลี่ไปแล้ว 25 ครั้ง มันจะเป็นคู่มือขั้นตอนที่ซับซ้อนมากในการทำทีเดียว จะเห็นว่าแต่ละ step ก็เหมือนแต่ละ layer ของ neural network program และการคลี่ก็คือ non linear actibation function ซึ่ง function นี้ผลลัพธ์คือการสะสมความซับซ้อนขึ้นทีละนิด</p>
<p>แต่ถ้าเราใช้ linear activation function หรือไม่ใช้เลยละ มันก็จะเปรียบได้กับการดีดก้อนกระดาษที่ห่อสร้อย representation เราเปลี่ยนนั่นคือก้อนมีการขยับและหยุดโดยทำมุมต่างจากตอนแรก แต่การดีด 25 ครั้งก็ไม่ทำให้เราใกล้กับคำตอบรวมถึง ผลลัพธ์จากการดีด 25 ครั้ง บางทีเราอาจจะสามารถทำได้ด้วยการดีดเพียงครั้งเดียว และที่โชคร้ายคือการดีดไม่ซับซ้อนพอที่จะพาเราไปสู่คำตอบได้ เปรียบได้กับ neural network ที่มีหลาย layer ก็จริงแต่สุดท้ายผลผลิตก็สามารถถูกผลิตได้ด้วย program ที่มี layer เดียว นอกจากนี้ยังไม่มีความซับซ้อนพอที่จะผลิตคำตอบที่เหมาะสมอีกด้วย</p>
<h2 id="คุณสมบัติของ-Activation-function">
<a class="anchor" href="#%E0%B8%84%E0%B8%B8%E0%B8%93%E0%B8%AA%E0%B8%A1%E0%B8%9A%E0%B8%B1%E0%B8%95%E0%B8%B4%E0%B8%82%E0%B8%AD%E0%B8%87-Activation-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>คุณสมบัติของ Activation function<a class="anchor-link" href="#%E0%B8%84%E0%B8%B8%E0%B8%93%E0%B8%AA%E0%B8%A1%E0%B8%9A%E0%B8%B1%E0%B8%95%E0%B8%B4%E0%B8%82%E0%B8%AD%E0%B8%87-Activation-function"> </a>
</h2>
<p>ผมขอแบ่งคุณสมบัติที่ควรมีของ activation function ออกมาเป็น 4 คุณสมบัติ</p>
<h3 id="1.-Activation-functions-ต้องเป็น-Continuous-function">
<a class="anchor" href="#1.-Activation-functions-%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B9%80%E0%B8%9B%E0%B9%87%E0%B8%99-Continuous-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Activation functions ต้องเป็น Continuous function<a class="anchor-link" href="#1.-Activation-functions-%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B9%80%E0%B8%9B%E0%B9%87%E0%B8%99-Continuous-function"> </a>
</h3>
<p>นั่นคือทุก input (infinite domain) ของเราที่ใส่เข้าไปใน function จะต้องมีคำตอบออกมาทั้งหมด เราไม่ควรที่จะใส่ input เข้าไปแล้วไม่มี output ออกมา</p>
<h3 id="2.-Activation-functions-ที่ดี-จะต้องไม่เปลี่ยนแปลงทิศทาง">
<a class="anchor" href="#2.-Activation-functions-%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%94%E0%B8%B5-%E0%B8%88%E0%B8%B0%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B9%84%E0%B8%A1%E0%B9%88%E0%B9%80%E0%B8%9B%E0%B8%A5%E0%B8%B5%E0%B9%88%E0%B8%A2%E0%B8%99%E0%B9%81%E0%B8%9B%E0%B8%A5%E0%B8%87%E0%B8%97%E0%B8%B4%E0%B8%A8%E0%B8%97%E0%B8%B2%E0%B8%87" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. Activation functions ที่ดี จะต้องไม่เปลี่ยนแปลงทิศทาง<a class="anchor-link" href="#2.-Activation-functions-%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%94%E0%B8%B5-%E0%B8%88%E0%B8%B0%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B9%84%E0%B8%A1%E0%B9%88%E0%B9%80%E0%B8%9B%E0%B8%A5%E0%B8%B5%E0%B9%88%E0%B8%A2%E0%B8%99%E0%B9%81%E0%B8%9B%E0%B8%A5%E0%B8%87%E0%B8%97%E0%B8%B4%E0%B8%A8%E0%B8%97%E0%B8%B2%E0%B8%87"> </a>
</h3>
<p>มันหมายความว่ายังไงกันนะ ลองคิดถึงพวกกราฟยกกำลัง เช่น ยกกำลัง 2</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">let_show</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&lt;matplotlib.lines.Line2D at 0x7fa2c0a016d0&gt;]</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3iV9f3/8ec7m0wSEpKQzd4zIMuBiuJEKyqgiFVLq/ir2lbr146ftd9Wa51trRUZiiIu3BMHyh4JI+yVQRICCdlkkHE+vz8S/VENEJJzzn3G+3FduUhOTnK/buz14u59f4YYY1BKKeV+fKwOoJRSqmO0wJVSyk1pgSullJvSAldKKTelBa6UUm7Kz5kHi46ONqmpqc48pFJKub3MzMxjxpiYH77u1AJPTU0lIyPDmYdUSim3JyJ5bb2ut1CUUspNaYErpZSb0gJXSik3pQWulFJuSgtcKaXc1BkLXESSRGSFiOwWkZ0ick/r6w+LSKGIbG39uNzxcZVSSn2nPcMIm4BfG2M2i0gYkCkiX7R+72ljzBOOi6eUUupUzngFbowpMsZsbv28GtgNJDg62MlW7S/huRUHnHlIpZRyeWd1D1xEUoERwIbWl+4WkSwRWSgikaf4mTkikiEiGSUlJR0KuXr/MZ76Yh/F1fUd+nmllPJE7S5wEQkFlgH3GmOqgOeBXsBwoAh4sq2fM8bMM8akG2PSY2J+NBO0XW4YnUSzzfB2ZkGHfl4ppTxRuwpcRPxpKe8lxph3AIwxR40xzcYYG/AiMMZRIXvFhDImLYo3NuVjs+kOQkopBe0bhSLAAmC3Meapk16PP+lt1wI77B/v/5s+Oom80lrW55Q68jBKKeU22nMFPgGYBVz4gyGDj4vIdhHJAiYB9zky6OVD4gkP8uP1jfmOPIxSSrmNMw4jNMasBqSNb31i/zinFuTvy7UjEli6MZ/ymgYiQwKceXillHI5bjUT88bRyTQ023h3S6HVUZRSynJuVeADe4QzLDGC1zcdwhh9mKmUcn1lNQ3cNH89WQUVdv/dblXgANPHJLPv6HE2H7L/X4ZSStnb25n5rDlQSqCfr91/t9sV+FXDehAS4MtrGw5ZHUUppU7LGMPSjfmMSomkX1yY3X+/2xV4aKAfU0ck8FHWYSprG62Oo5RSp7Quu5ScYzXMHJPskN/vdgUOMHNMMieabLy7RWdmKqVc19KN+YQH+XHF0Pgzv7kD3LLABydEMDQxgqUb8/VhplLKJZUeP8FnO4r4ychEgvztf/8b3LTAoeUqfO/RajYfKrc6ilJK/cjbmQU0NhtuOscxt0/AjQv8qmE9CA3047UNOjNTKeVaWh5eHmJ0aiR9Yu3/8PI7blvgIYF+TB3eQx9mKqVczrqDpeSW1jLDQQ8vv+O2BQ4w85yWh5lvb9aHmUop1/Hqhjy6Bvtz+RDHPLz8jlsX+KAeEYxI7sqSDXn6MFMp5RKKq+pZvvMo149y3MPL77h1gQPcfE4K2SU1rDuoy8wqpaz3xqZ8mmyGmeekOPxYbl/gVwyNp2uwP69uyLM6ilLKyzXbWh5entsnmrToEIcfz+0LPMjfl2kjE1m+8yjFVbpnplLKOl/vKeZwZb1Dhw6ezO0LHOCmsSk02QxvbNIhhUop6yzZkEdseCAXD4h1yvE8osDTokOY2DuapRsP0dRsszqOUsoL5ZfV8u2+EqaPTsbP1znV6hEFDnDz2GQOV9bz9Z5iq6MopbzQqxvy8BFx+Njvk3lMgV88IJb4iCBeWa8PM5VSzlXf2Mybm/K5ZGAscRFBTjuuxxS4n68PM8cks2r/MbJLjlsdRynlRT7KKqK8tpFZ4xw/dPBkHlPg0LJbj7+v6FW4UsqpFq/LpU/3UMb17ObU43pUgceEBXLZ4HjeziygtqHJ6jhKKS+wNb+CrIJKZo1LQUScemyPKnCAW8alUF3fxHtbDlsdRSnlBRavyyUkwJdrRyQ4/dgeV+CjUiLpHxfG4nW5uj6KUsqhymoa+CiriGtHJhAW5O/043tcgYsIt4xLZc+Rajbl6mYPSinHeWNTPg1NNmaNTbXk+B5X4ADXjOhBeJAfL6/NtTqKUspDNTXbeGVdLuN6dnPIjvPt4ZEFHhzgx/QxyXy28whFlXVWx1FKeaAvdx/lcGU9t05ItSyDRxY4wKyxKdiM4VUdUqiUcoCX1uaS0LWL09Y9aYvHFnhSVDAX9Y9l6cZ86hubrY6jlPIgu4uqWJ9dxqxxKfj6OHfo4Mk8tsABfjoh9funxEopZS+L1+US5O/D9NFJlubw6AIf36sbfbqH8tLaHB1SqJSyi4raBt7dUsg1wxPoGhxgaZYzFriIJInIChHZLSI7ReSe1tejROQLEdnf+mek4+OeHRFh9vhUdhRWkZmnQwqVUp33xqZ86httzB6fanWUdl2BNwG/NsYMAMYCc0VkIPAg8JUxpg/wVevXLucnIxMID/Jj0Zpcq6MopdxcU7ONxevyGNszigHx4VbHOXOBG2OKjDGbWz+vBnYDCcBU4OXWt70MXOOokJ0RHODHjDHJfLqjiILyWqvjKKXc2PJdRymsqOO2CWlWRwHO8h64iKQCI4ANQKwxpghaSh7ofoqfmSMiGSKSUVJS0rm0HXTL+FREhFfW6ZBCpVTHLVydQ3JUMBdZOHTwZO0ucBEJBZYB9xpjqtr7c8aYecaYdGNMekxMTEcydlpC1y5MGRTH0o2HdJVCpVSHbMuvICOvnFvHp1o6dPBk7SpwEfGnpbyXGGPeaX35qIjEt34/HnDpvcxum5hKVX0TyzYXWh1FKeWGFq3JITTQj+vTE62O8r32jEIRYAGw2xjz1Enf+gCY3fr5bOB9+8ezn5HJkQxLjGDRmhxsNh1SqJRqv6NV9XyUVcT16YmWrDp4Ku25Ap8AzAIuFJGtrR+XA48Bk0VkPzC59WuXJSLcNjGN7JIavt1nzb14pZR7emVdHs3GcKsLDB08md+Z3mCMWQ2c6obPRfaN41iXDY7n0fA9LFidw6T+bT5zVUqp/1LX0MyrG/K4eEAsKd1CrI7zXzx6JuYPBfj5MHt8KqsPHGN3UbufwyqlvNiyzQVU1Dbys3N7Wh3lR7yqwAFmjkmmi78v81flWB1FKeXibDbDwtU5DE2MYHSqy002974Cjwj254b0RD7YVkhxVb3VcZRSLuzrPcVkH6vhjnN7On3D4vbwugIHuG1iGk02w2Kd2KOUOo35q7PpERHEZYPjrI7SJq8s8JRuIVwyMJZXN+TpxB6lVJt2FFayPruMn05Iw9/XNavSNVM5wR3n9qSitpFlmQVWR1FKuaAXV2UTEuDLjWOsXfP7dLy2wNNTIhmW1JX5q3No1ok9SqmTFJTX8lFWEdPHJBPuQhN3fshrC1xE+MV5PckrreXznUesjqOUciELV+citDwvc2VeW+AAlwyKI7VbMC+szNYde5RSAFTWNvL6pkNcNawHCV27WB3ntLy6wH19hNvP7cm2/Ao25pRZHUcp5QJaBjc0u+TEnR/y6gIHuH5UIlEhAcxbmW11FKWUxU40NfPS2lzO7RPNwB7W77hzJl5f4EH+vtwyLoWv9hSz/2i11XGUUhZ6b0shJdUn+Pl5vayO0i5eX+AAt4xLJcjfhxf0Klwpr2WzGeatzGZgfDgTenezOk67aIEDUSEBTB+dzPtbCzlcUWd1HKWUBZbvOsrBkhruvKCXS06bb4sWeKs7zk3DZmDBal3kSilvY4zh+W8PktIt2GWnzbdFC7xVYmQwU4f1YOnGQ5TXNFgdRynlROuyS9mWX8Gc83ri56LT5tviPkmd4Ofn96K2oVkXuVLKyzz/zUGiQwO5bqTr7HfZHlrgJ+kXF8bFA7rz0tocXeRKKS+xo7CSVfuPcfvENIL8fa2Oc1a0wH/gzgt6UV7byOsb862OopRygue/PUhYoB83jU22OspZ0wL/gVEpUYxJjeLFVdk0NNmsjqOUcqCDJcf5ZHsRN49LcelFq05FC7wNcy/sTVFlPe9s1qVmlfJkz39zkEA/H2538UWrTkULvA3n9YlmSEIEz397kKZmvQpXyhMVlNfy3pZCpo9OJjo00Oo4HaIF3gYRYe6kXuSV1vLx9iKr4yilHGDeymxEYM55rr9o1alogZ/CJQPj6NM9lH+vOIhNN3xQyqMUV9fz+qZ8fjIikR4uvmTs6WiBn4KPj3DXpF7sPVrNl7uPWh1HKWVHC1bl0NRs484L3GPRqlPRAj+Nq4b2IDkqmH+tOKAbPijlIcprGnh1fR5XDu1BanSI1XE6RQv8NPx8fZg7qRdZBZV8s6/E6jhKKTtYsDqH2sZm7r6wt9VROk0L/AyuHZFIQtcuPPvlfr0KV8rNVdY28tLaXC4fHE/f2DCr43SaFvgZBPj5cNekXmzNr2D1gWNWx1FKdcLCNTkcP9HkEVffoAXeLtNGJRIfEaRX4Uq5sar6RhauyeGSgbEMiHf97dLa44wFLiILRaRYRHac9NrDIlIoIltbPy53bExrBfr5cucFvcjIK2dddqnVcZRSHfDymlyq65v45UV9rI5iN+25An8JmNLG608bY4a3fnxi31iu54b0JLqHBfLsl/utjqKUOkvV9Y0sWJPDhf27Mzghwuo4dnPGAjfGrATKnJDFpQX5t1yFb8gpY+1BvReulDt5aU0uFbWN3ONBV9/QuXvgd4tIVustlshTvUlE5ohIhohklJS491C8GWOSiQ0P5Jkv9F64Uu6iqr6RF1dlc/GA7gxL6mp1HLvqaIE/D/QChgNFwJOneqMxZp4xJt0Ykx4TE9PBw7mGIH9f5k7qzcbcMtYe1HvhSrmDRatzqapv4t6L+1odxe46VODGmKPGmGZjjA14ERhj31iu68bRScRHBPHUF/v0KlwpF1dZ18j81dlMHhjrUfe+v9OhAheR+JO+vBbYcar3eppAv5ar8My8clbt13vhSrmyBatzqK5v4t6LPeve93faM4xwKbAO6CciBSJyO/C4iGwXkSxgEnCfg3O6lBvSk0jo2kWvwpVyYRW1DSxancOUQXEM6uF5V98Afmd6gzFmRhsvL3BAFrcR4OfD/7mwNw++s52v9xRz0YBYqyMppX7ghZXZHG9o4t7Jnnn1DToTs8OuG5VISrdgnli+T9cLV8rFFFfX89KaXK4a2oP+cZ4x67ItWuAd5O/rw30X92V3URWf7NBde5RyJf9ecZCGZhv3Tfa8kScn0wLvhKuG9aBvbChPfbFP985UykUcrqjjtQ2HmDYykTQ3X+/7TLTAO8HXR/jV5L5kl9Tw7pZCq+MopYB/fr0fg+H/XOQZKw6ejhZ4J106KI4hCRE88+V+TjQ1Wx1HKa+We6yGNzMKmDkmmcTIYKvjOJwWeCeJCPdf2o/CijqWbjhkdRylvNqTX+wjwNeHuR6y3veZaIHbwbl9ohnXsxv//PoAx080WR1HKa+0o7CSD7cd5vaJaXQPC7I6jlNogduBiPDAlH6U1jSwYFWO1XGU8kqPf76XrsH+zDm/p9VRnEYL3E5GJEcyZVAcL67KpvT4CavjKOVV1h48xsp9Jcy9oDfhQf5Wx3EaLXA7+s2lfaltaOK5FQetjqKU1zDG8LfP9hIfEcSscSlWx3EqLXA76t09jGmjEnl1fR75ZbVWx1HKK3y24wjb8iu49+I+BPn7Wh3HqbTA7ey+yX3x8YEnlu+1OopSHq+x2cbfPttDn+6hXDcy0eo4TqcFbmfxEV24fWIa7289zPaCSqvjKOXRlm48RG5pLf9zeX/8fL2vzrzvjJ3g5+f3IiokgL9+sluXm1XKQarrG3n2y/2M7RnFpH7drY5jCS1wBwgP8ueXF/ZmXXYp3+x1731AlXJVL3ybTWlNAw9dPgARsTqOJbTAHWTmOSmkdAvm0U9360JXStnZkcp65q/O5uphPRia6FkbFZ8NLXAHCfDz4bdT+rPv6HHeyiywOo5SHuWJ5Xux2eD+S/tZHcVSWuAOdNngONJTInly+V6dYq+UneworGTZ5gJ+OjGVpCjPX7DqdLTAHUhE+P2VAzl2vIHnvzlgdRyl3J4xhj9/tIvI4ADmTvKOBatORwvcwYYndeWa4T14cVUOBeU6uUepzli+6ygbcsq4b3Jfr5oyfypa4E5w/5T+CPD3z3Vyj1Id1dBk49FPdtOneygzRidZHcclaIE7QULXLsw5ryfvbz1MZl651XGUckuL1+WSW1rLQ1cM8MpJO23RvwUn+cX5vYgND+SRD3fqLvZKnaWS6hM8++V+LugX47WTdtqiBe4kIYF+PHhZf7YVtDxBV0q135PL91LX2MwfrhxodRSXogXuRFOHJTAiuSt/+2wv1fWNVsdRyi1sL6jkjYx8bh2fSq+YUKvjuBQtcCfy8REevmoQx46f4F9f67BCpc7EGMOfPtxJt5AAfnlxH6vjuBwtcCcbltSV60clsnBNDtklx62Oo5RL+2DbYTLyyrn/0n46bLANWuAWuH9KP4L8fHn4w126WqFSp1Bd38hfPt7NkIQIpo3SYYNt0QK3QPewIO6b3JeV+0r4fOdRq+Mo5ZL+8dV+iqtP8MjUQfj6eOdqg2eiBW6RW8al0D8ujD9/tIu6hmar4yjlUvYdrWbRmlxuTE9iRHKk1XFclha4Rfx8ffjT1YMorKjjuRX6QFOp7xhj+OP7OwgO8OWBKd692uCZnLHARWShiBSLyI6TXosSkS9EZH/rn/pPZAec07Mb1wzvwbyV2eQcq7E6jlIu4cOsItZnl3H/pf3oFhpodRyX1p4r8JeAKT947UHgK2NMH+Cr1q9VBzx0+QAC/Xz44/s79IGm8npV9Y38+aNdDE4IZ+Y5KVbHcXlnLHBjzEqg7AcvTwVebv38ZeAaO+fyGt3Dg7h/Sj9W7T/Gh1lFVsdRylJPfr6XY8dP8Ndrh+iDy3bo6D3wWGNMEUDrn6dcnEBE5ohIhohklJTo/pBtuemcFIYmRvDnj3ZRWaczNJV3yiqoYPH6PG4Zm+LV26SdDYc/xDTGzDPGpBtj0mNiYhx9OLfk6yP85ZohlB4/wZPLdclZ5X2abYaH3t1OdGggv/bybdLORkcL/KiIxAO0/llsv0jeaUhiBLeMS+WV9Xlsza+wOo5STvXKulx2FFbxxysH6ozLs9DRAv8AmN36+WzgffvE8W6/vqQv3cMCeXBZFo26k73yEocr6vj753s5r28MVw6NtzqOW2nPMMKlwDqgn4gUiMjtwGPAZBHZD0xu/Vp1UliQP49MHcyeI9W8uCrb6jhKOZwxhj+8twObgb9cMxgRfXB5NvzO9AZjzIxTfOsiO2dRwKWD4pgyKI5nv9zPZYPjSYsOsTqSUg7z8fYivtpTzO+vGOD1O8x3hM7EdEF/mjqIAD8fHnpnu44NVx6roraBhz/YydDECG4dn2p1HLekBe6CYsOD+J/LBrAuu5Q3M/KtjqOUQ/zl492U1zby2E+G6h6XHaR/ay5q+ugkzkmL4n8/2s2Rynqr4yhlV9/uK+GtzALmnNeTgT3CrY7jtrTAXZSPj/C364bSaLPx0Lt6K0V5jur6Rv5nWRa9u4dyz0W6y05naIG7sNToEO6/tD9f7ynm3S2FVsdRyi4e/XQPR6rqeXzaUIL8fa2O49a0wF3creNTGZUSyZ8+3EVxld5KUe5t7YFjvLbhELdPTGOkrvPdaVrgLs7XR3h82lDqGpt56F1dsVC5r+MnmnhgWRap3YL51WSdLm8PWuBuoFdMKA9c2o8vdx/l7cwCq+Mo1SF/+XgXhyvqePKGYXQJ0Fsn9qAF7iZum5DGmLQoHvlwF4UVdVbHUeqsrNhTzNKN+cw5rxejUqKsjuMxtMDdhI+P8MS0YTQbw/1vbcNm01spyj1U1Dbw22VZ9IsN477JOurEnrTA3Uhyt2B+f8VA1h4sZfG6XKvjKNUuf3x/J2U1DTx5wzAC/fTWiT1pgbuZGWOSuKBfDI9+uof9R6utjqPUab23pZAPth3mnov6MDghwuo4HkcL3M2ItIxKCQn045evb+VEU7PVkZRqU35ZLX94bwfpKZHcNam31XE8kha4G+oeFsTj1w1ld1EVTy7fZ3UcpX6k2Wb41ZtbMcDTNw7X/S0dRAvcTV08MJaZ5yTz4qps1h44ZnUcpf7L898cYFNuOY9MHaTLxDqQFrgb+/0VA0iLDuG+N7dSevyE1XGUAiAzr5ynv9zPlUPjuXZEgtVxPJoWuBsLDvDjnzNGUF7TyG/e2qazNJXlKmsb+eXSLcRHBPHXnwzRHXYcTAvczQ3qEcHvrhjAir0lLFidY3Uc5cWMMTz4ThZHq+r554wRujmxE2iBe4BbxqUweWAsf/tsD1kFuqO9ssaSDYf4dMcR7r+0HyN0oSqn0AL3ACLC36cNJSY0kLmvbaayrtHqSMrL7DxcySMf7eK8vjH87NyeVsfxGlrgHqJrcAD/nDmCoop67tf74cqJquobuWvJZiKD/XnqhmH46JBBp9EC9yCjUqJ48LL+LN91lPmr9H64cjxjDA+8lUVBeR3PzRxJdGig1ZG8iha4h7l9YhpTBsXx2Gd7yMgtszqO8nAL1+Ty2c4jPDilP+mpusqgs2mBexgR4fHrh5IY2YW5r22muFp38VGOsSm3jEc/2c0lA2O549w0q+N4JS1wDxQe5M9/bh5FVV0Tc5dspqHJZnUk5WGOVNZz56ubSYoK5okbhul4b4togXuoAfHh/G3aUDbllvOXj3dZHUd5kBNNzdy5JJO6hibmzRql470t5Gd1AOU4Vw/rQVZ+BfNX5zAksSvTRiVaHUl5gIc/2MWWQxU8f9NI+sSGWR3Hq+kVuId78LL+jO/VjYfe3c6WQ+VWx1Fu7pV1uSzdeIg7L+jFZUPirY7j9bTAPZyfrw//mjmS2PBA5rySSVGl7qepOmbtgWM8/OEuLuzfnd9corvKuwItcC8QFRLAgtmjqT3RxM8WZ1DXoJtAqLOTe6yGO5dspmd0CM9O1/W9XUWnClxEckVku4hsFZEMe4VS9tc3Nox/zBjBzsNV/OZt3RRZtV9lXSN3LM5ABBbMHk2YPrR0Gfa4Ap9kjBlujEm3w+9SDnTRgFgenNKfj7OKeGL5XqvjKDfQ0GTjriWZ5B6r4d83jSS5m27O4Ep0FIqXmXNeT3JLa/n3NwdJjgpm+phkqyMpF2WM4XfvbmfNgVKeuH4Y43tFWx1J/UBnr8ANsFxEMkVkTltvEJE5IpIhIhklJSWdPJzqLBHhz1MHcV7fGH733g5W7tP/Jqptz604wFuZBfzywt46BNVFdbbAJxhjRgKXAXNF5LwfvsEYM88Yk26MSY+Jienk4ZQ9+Pn68NzMEfTpHspdSzaz83Cl1ZGUi3lncwFPLN/HtSMSuG9yX6vjqFPoVIEbYw63/lkMvAuMsUco5XhhQf4s+ulowoP8uHXRJg6V1lodSbmIFXuLeeDtLMb36sZj1+m2aK6swwUuIiEiEvbd58AlwA57BVOOFx/RhcW3j6GhycYtCzdwTDdG9npbDpVz16ub6RcXxguzRhHo52t1JHUanbkCjwVWi8g2YCPwsTHmM/vEUs7Su3sYC28dzZGqem57aRPV9bqbj7c6UHyc217aRExYIIt+qsMF3UGHC9wYk22MGdb6McgY8xd7BlPOMyolkn/fNJJdh6u4/WWd6OON8stquXn+Bnx9fFh82xi6hwVZHUm1g87EVABc2D+Wp24czqbcMu5ckqlL0HqRo1X13DR/A3WNzbx6xxhSo0OsjqTaSQtcfe/qYT149NohfLO3hHte30JTs5a4pys9foKb5m+g9PgJXr5tDP3jwq2OpM6CFrj6L9PHJPOHKwfy6Y4j3PvGVi1xD/ZdeeeX1bLg1tEMT+pqdSR1lnQmpvqR2yem0dRs49FP9wDwzI3D8fPVf+s9SVlNAzfN30DOsRoWzB7N2J7drI6kOkALXLXp5+f3wgCPfboHEeHpG4ZpiXuIspoGZr64/vvynthHp8i7Ky1wdUq/OL8XxsDfPttDY5ONZ2cM13HBbq649YHlobJa5s9O1/J2c3pJpU7rzgt68YcrB/LZziPMWZypQwzdWEF5Lde/sI7CijoW/XQ05/bRpS3cnRa4OqPbJ6bx2E+GsHJ/CbMXbdTJPm4ou+Q4N/xnHeU1Dbx6xzm6sqCH0AJX7TJ9TDLP3DiczLxyps9bT3F1vdWRVDttza9g2n/WcaLJxtI5YxmZHGl1JGUnWuCq3aYOT2D+7HSyS2q47vm15ByrsTqSOoMVe4uZMW89IYG+vH3neAb1iLA6krIjLXB1Vib1687SOWOpOdHMtOfX6k73LuytjHx+9nIGPWNCWHbneNJ0hqXH0QJXZ214Ulfe/sU4ggN9mT5vPR9nFVkdSZ3EZjM8/tke7n87i7E9u/H6nLG6tomH0gJXHdIzJpT37prAkIQI5r62medWHMAY3SjZanUNzdy9dDP//uYgM8Yk66qCHk4LXHVYt9BAXr3jHKYO78HfP9/LvW9s1WGGFiqsqOOGF9bx6Y4j/P6KAfz12sH46+Qrj6YTeVSnBPn78syNw+kbG8YTy/ey7+hx5s0aRVKU7l7uTGsPHuPu17bQ0GRj3qx0Jg+MtTqScgL951l1mogwd1JvFt46msLyWq7612q+1c2SncIYw/xV2cxasJHIYH/ev3uClrcX0QJXdjOpX3c+uHsiceFBzF64kcc/26OrGTpQRW0DP1ucyf9+vJuLB3TnvbkT6BUTanUs5URa4MquUqNDePeuCUwfncS/vznI9HnrOVxRZ3Usj5OZV84V/1jNt/uK+eOVA/nPzaP0YaUX0gJXdtclwJfHrhvKs9OHs7uoiinPrOT9rYVWx/IIjc02nlq+lxteWIePD7z9i/HcNjFNd473UvoQUznM1OEJDEvsyq/e3Mo9r2/ly93F/HnqILoGB1gdzS0dKD7OfW9sZXthJT8ZmcDDVw8iXK+6vZoWuHKo1OgQ3vz5OF5Ymc3TX+xjfXYpj1w9iMuGxFsdzW00NtuYtzKbZ7/aT0iAL/+5eSRTBuvfn9ICV07g5+vD3Em9Ob9vDA++k8WdSzZz6aBYHpk6mNhwnSF4OlkFFfx22XZ2F1Vx+ZA4Hr56kM6qVN8TZ86eS09PNwm5jYMAAAc7SURBVBkZGU47nnI9Tc025q/O4ekv9uHnI9xzcR9uHZ9GgJ8+jjlZeU0DTyzfy2sbDxETGsifrxnMpYPirI6lLCIimcaY9B+9rgWurJBXWsMjH+7iqz3F9IwJ4f9eNYjz++oGA03NNl7flM8Ty/dSXd/ErLEp3De5LxFd9F63N9MCVy7pq91HeeSjXeSV1jKxdzS/ndKfIYnet+SpMYbPdx7l8c/3kF1SwzlpUfxp6iD6x4VbHU25AC1w5bJONDWzZP0h/vn1fsprG7liaDy/vLAP/eLCrI7mcMYYVu0/xjNf7mPzoQp6xYTwwJT+XDIwVocGqu9pgSuXV1XfyLxvs1m0JoeahmamDIrj7gt7MzjB867IbTbDir3F/OPrA2zLryA+Ioh7LurDtFGJ+OkCVOoHtMCV2yivaWDRmhwWrc2lur6JsT2juG1CGhcNiMXXx72vSmsbmli2uZBFa3LILqkhKaoLd13Qm+tGJuqDXHVKWuDK7VTVN/L6xkO8vDaPwoo6kqK6cMOoJKalJxIf0cXqeGdl5+FK3soo4J3NBVTVNzE0MYLbJqRxxdB4XfJVnZEWuHJbTc02lu86yivr8liXXYqPwMQ+MVw9rAeTB8a67AiNgvJaPt1+hPe3FbKjsIoAPx8uHRTHreNTGJkcqfe4VbtpgSuPcKi0lrcz81m2uZDCijr8fYWJvaO5sH93LujX3dJ1yG02w66iKr7dV8IXu46yNb8CgCEJEUwblcjU4T10GQHVIQ4pcBGZAjwL+ALzjTGPne79WuDKXowxbCuo5JPtRXy6o4j8spYVD3tGhzAmLYr01CjSUyJJ6RbssCvdxmYbe49Uk5FbRkZeOeuzyzh2/ATQUtqXDYnjiiHxpHTTzYRV59i9wEXEF9gHTAYKgE3ADGPMrlP9jBa4cgRjDDnHavhmbwmr9peQmVdOVX0TAKGBfvSPC6NfXBhp0SEkRgaTFNWFmLBAIoMDznj/ub6xmbKaBo5U1ZNfVktBeR0HS46zp6iaA8XHaWhd7zw+IojRqVGc3zeG8/rGEBMW6PDzVt7jVAXembVQxgAHjDHZrQd4HZgKnLLAlXIEEaFnTCg9Y0K5bWIaNpthf/FxNh8qZ09RFbuPVPPhtsPfl/rJwgL96BLgS4CfDwF+PhgDDU02GpptHK9voq7xx3t8xoYH0j8unHP7RjMwPpz01CgSurrXQ1XlGTpT4AlA/klfFwDn/PBNIjIHmAOQnJzcicMp1T4+PkK/1qvuk1XWNnKorJaC8lqO1TRQXtNAeW0DdQ3NNDTZONFsw0eEAN+WMg8N9CUyJIDI4AC6hwWSFBVMYmQXggN0DTjlGjrzv8S2biz+6H6MMWYeMA9abqF04nhKdUpEsD9DgiO8cqq+8kydGYBaACSd9HUicLhzcZRSSrVXZwp8E9BHRNJEJACYDnxgn1hKKaXOpMO3UIwxTSJyN/A5LcMIFxpjdtotmVJKqdPq1NMYY8wnwCd2yqKUUuos6CIMSinlprTAlVLKTWmBK6WUm9ICV0opN+XU1QhFpATIc9oB7ScaOGZ1CCfzxnMG7zxvbzxncK/zTjHG/GjXb6cWuLsSkYy2FpLxZN54zuCd5+2N5wyecd56C0UppdyUFrhSSrkpLfD2mWd1AAt44zmDd563N54zeMB56z1wpZRyU3oFrpRSbkoLXCml3JQW+FkSkd+IiBGRaKuzOJqI/F1E9ohIloi8KyJdrc7kKCIyRUT2isgBEXnQ6jzOICJJIrJCRHaLyE4RucfqTM4iIr4iskVEPrI6S2dogZ8FEUmiZRPnQ1ZncZIvgMHGmKG0bGD9PxbncYjWDbqfAy4DBgIzRGSgtamcogn4tTFmADAWmOsl5w1wD7Db6hCdpQV+dp4GHqCNreM8kTFmuTHmu52A19Oy65In+n6DbmNMA/DdBt0ezRhTZIzZ3Pp5NS2FlmBtKscTkUTgCmC+1Vk6Swu8nUTkaqDQGLPN6iwWuQ341OoQDtLWBt0eX2QnE5FUYASwwdokTvEMLRdiNquDdJZur30SEfkSiGvjW78DHgIucW4ixzvdORtj3m99z+9o+b/bS5yZzYnatUG3pxKRUGAZcK8xpsrqPI4kIlcCxcaYTBG5wOo8naUFfhJjzMVtvS4iQ4A0YJuIQMuthM0iMsYYc8SJEe3uVOf8HRGZDVwJXGQ8d9KA127QLSL+tJT3EmPMO1bncYIJwNUicjkQBISLyKvGmJstztUhOpGnA0QkF0g3xrjLSmYdIiJTgKeA840xJVbncRQR8aPlIe1FQCEtG3bP9PQ9XqXlauRloMwYc6/VeZyt9Qr8N8aYK63O0lF6D1ydzr+AMOALEdkqIv+xOpAjtD6o/W6D7t3Am55e3q0mALOAC1v/+25tvTJVbkKvwJVSyk3pFbhSSrkpLXCllHJTWuBKKeWmtMCVUspNaYErpZSb0gJXSik3pQWulFJu6v8B8xQcWeTWQLwAAAAASUVORK5CYII=%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>จะเห็นว่าคำตอบที่ได้มีการเปลี่ยนทิศทางจากลบเป็นบวก ถ้าเราเพิ่มค่าแนวแกน x เพิ่มขึ้นเรื่อยๆ แบบนี้แสดงว่าถ้า x เราคือ weight และ y คือคำตอบ คำตอบที่ถูกต้องสามารถหาได้จากการปรับ x ไปด้านซ้ายหรือขวา ตอนนี้แหละเราจะเกิด 2 มุมมอง</p>
<ul>
<li>มุมมองแรก เอ้ยมันดีนะเรามีโอกาสที่จะหาคำตอบที่ถูกต้องเจอมากขึ้น เจ๋งเลย</li>
<li>มุมมองที่สอง แย่แล้วคำตอบที่ถูกต้องสามารถหาได้จากการเลื่อน x ไปทางซ้ายหรือ ขวา แล้วเราควรจะเลื่อนไปด้านไหนดี?</li>
</ul>
<p>ซึ่งพอมามองในมุมมองของ neural network program แล้วหรือเฉพาะเจาจง optmization alogorithm มุมมองที่สองมีความสำคัญและส่งผลต่อการเรียนรู้มากกว่า ดังนั้นเราควรจะเลือก functions ที่ไม่เปลี่ยนทิศทาง</p>
<h3 id="3.-Activation-functions-เป็น-Non-Linear-functions">
<a class="anchor" href="#3.-Activation-functions-%E0%B9%80%E0%B8%9B%E0%B9%87%E0%B8%99-Non-Linear-functions" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. Activation functions เป็น Non Linear functions<a class="anchor-link" href="#3.-Activation-functions-%E0%B9%80%E0%B8%9B%E0%B9%87%E0%B8%99-Non-Linear-functions"> </a>
</h3>
<p>ตามเหตุผลที่อธิบายไปก่อนหน้าที่ถ้าเป็น non linear functions ยิ่ง steps ในโปรแกรมเยอะก็หมายถึง program มีความซับซ้อนมากขึ้น และสามารถแก้ไขปัญหาที่ซับซ้อนได้ ถ้ามองในมุม correlation คือ hidden layer สามารถสร้างปรากฏการณ์ selective correlation ได้ ส่งผลให้แต่ละ layer สร้าง correlation ของตัวเองได้ (ทำให้แต่ละ layer เปรียบเสมือน representation ใหม่ของข้อมูลของเรา) และสุดท้ายส่งผลให้ output layer ไม่ได้ขึ้นอยู่กับแค่ correlation จาก input layer เพียงอย่างเดียว แต่เป็น correlation โดยรวมจาก layers ก่อนหน้าทั้งหมด ซึ่งมีความซับซ้อน</p>
<h3 id="4.-Activation-functions-ต้องสามารถคำนวณได้ในเชิงใช้งานจริง-รวมถึง-derivative-ของตัว-function-เองด้วย">
<a class="anchor" href="#4.-Activation-functions-%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%AA%E0%B8%B2%E0%B8%A1%E0%B8%B2%E0%B8%A3%E0%B8%96%E0%B8%84%E0%B8%B3%E0%B8%99%E0%B8%A7%E0%B8%93%E0%B9%84%E0%B8%94%E0%B9%89%E0%B9%83%E0%B8%99%E0%B9%80%E0%B8%8A%E0%B8%B4%E0%B8%87%E0%B9%83%E0%B8%8A%E0%B9%89%E0%B8%87%E0%B8%B2%E0%B8%99%E0%B8%88%E0%B8%A3%E0%B8%B4%E0%B8%87-%E0%B8%A3%E0%B8%A7%E0%B8%A1%E0%B8%96%E0%B8%B6%E0%B8%87-derivative-%E0%B8%82%E0%B8%AD%E0%B8%87%E0%B8%95%E0%B8%B1%E0%B8%A7-function-%E0%B9%80%E0%B8%AD%E0%B8%87%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2" aria-hidden="true"><span class="octicon octicon-link"></span></a>4. Activation functions ต้องสามารถคำนวณได้ในเชิงใช้งานจริง รวมถึง derivative ของตัว function เองด้วย<a class="anchor-link" href="#4.-Activation-functions-%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%AA%E0%B8%B2%E0%B8%A1%E0%B8%B2%E0%B8%A3%E0%B8%96%E0%B8%84%E0%B8%B3%E0%B8%99%E0%B8%A7%E0%B8%93%E0%B9%84%E0%B8%94%E0%B9%89%E0%B9%83%E0%B8%99%E0%B9%80%E0%B8%8A%E0%B8%B4%E0%B8%87%E0%B9%83%E0%B8%8A%E0%B9%89%E0%B8%87%E0%B8%B2%E0%B8%99%E0%B8%88%E0%B8%A3%E0%B8%B4%E0%B8%87-%E0%B8%A3%E0%B8%A7%E0%B8%A1%E0%B8%96%E0%B8%B6%E0%B8%87-derivative-%E0%B8%82%E0%B8%AD%E0%B8%87%E0%B8%95%E0%B8%B1%E0%B8%A7-function-%E0%B9%80%E0%B8%AD%E0%B8%87%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2"> </a>
</h3>
<p>ง่ายๆก็คือสามารถคำนวณได้ และต้องเร็ว เนื่องจาก activation functions จะต้องถูกเรียกบ่อยมากๆ</p>
<h2 id="เรามาดู-Activation-functions-ที่ถูกใช้ใน-Hidden-layer-กัน">
<a class="anchor" href="#%E0%B9%80%E0%B8%A3%E0%B8%B2%E0%B8%A1%E0%B8%B2%E0%B8%94%E0%B8%B9-Activation-functions-%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%96%E0%B8%B9%E0%B8%81%E0%B9%83%E0%B8%8A%E0%B9%89%E0%B9%83%E0%B8%99-Hidden-layer-%E0%B8%81%E0%B8%B1%E0%B8%99" aria-hidden="true"><span class="octicon octicon-link"></span></a>เรามาดู Activation functions ที่ถูกใช้ใน Hidden layer กัน<a class="anchor-link" href="#%E0%B9%80%E0%B8%A3%E0%B8%B2%E0%B8%A1%E0%B8%B2%E0%B8%94%E0%B8%B9-Activation-functions-%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%96%E0%B8%B9%E0%B8%81%E0%B9%83%E0%B8%8A%E0%B9%89%E0%B9%83%E0%B8%99-Hidden-layer-%E0%B8%81%E0%B8%B1%E0%B8%99"> </a>
</h2>
<p>functions ที่ใช้กันผมจะขอยกตัวอย่างมา 3 functions</p>
<h3 id="1.-Sigmoid-function">
<a class="anchor" href="#1.-Sigmoid-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Sigmoid function<a class="anchor-link" href="#1.-Sigmoid-function"> </a>
</h3>
<p>จะทำการให้คำตอบระหว่าง [0, 1] ซึ่งเหมาะมากในการใช้อธิบายความน่าจะเป็นของ node โดย 1 ก็คือมีโอกาส 100% นั่นทำให้ functions นี้ถูกนำไปใช้ทั้งใน hidden และ output layer</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">output</span>

<span class="k">def</span> <span class="nf">derivative_of_sigmoid</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
    <span class="n">derivative_output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">derivative_output</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.-tanh-function">
<a class="anchor" href="#2.-tanh-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. tanh function<a class="anchor-link" href="#2.-tanh-function"> </a>
</h3>
<p>ในส่วนของ tanh จะให้คำตอบระหว่าง [-1, 1] เนื่องจากมีโอกาสให้คำตอบเป็นจำนวนติดลบด้วย ทำให้ในหลายปัญหามีประโยชน์กว่า sigmoid การทำ weight sum คือการดูว่า inputs ของเราเป็นไปในทิศทางเดียวกับ correlation ที่มีสำหรับ node ที่สนใจไหม (การทำ weight sum มองเป็นการหา similarity หรือแนวโน้มที่ไปทางเดียวกันกับความสัมพันธ์) ถ้าคำตอบเป็นลบแสดงว่ายังมีความสัมพันธ์กันอยู่ระหว่าง input กับ output node แต่เป็นความสัมพันธ์ที่ input มีความตรงกันข้ามกับ output ซึ่ง tanh สามารถอธิบายตรงนี้ได้ แต่ sigmoid จะบอกคำตอบว่าเป็น 0 ซึ่งบอกว่าค่า input ที่เข้ามาปัจจุบันไม่มีความสัมพันธ์กับ output เพราะ sigmoid function ทำให้ node บอกได้แค่ว่ามีความสัมพันธ์ไหม หรือไม่มี</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>

<span class="k">def</span> <span class="nf">derivative_of_tanh</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
    <span class="n">derivative_output</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">output</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">derivative_output</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.relu-function">
<a class="anchor" href="#3.relu-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.relu function<a class="anchor-link" href="#3.relu-function"> </a>
</h3>
<p>จะให้ผลลัพธ์เป็นค่าเดิมเมื่อค่ามากกว่า 0 แต่ถ้าค่าน้อยกว่าหรือเท่ากับ 0 จะให้ค่าคำตอบที่เป็น 0 เท่านั้น ผลกระทบจะเหมือน sigmoid เลยนั่นคือจะบอกได้แค่จาก input ที่ได้มามีความสัมพันธ์ไหม หรือไม่มี แต่ที่ต่างคือตัวสมการ sigmoid จะให้ค่า derivative ที่สูงเมื่อ input เข้าใกล้ 0 แต่เมื่อคำตอบที่ได้จาก sigmoid เข้าใกล้ 1 หรือ 0 จะทำให้ค่า derivative เข้าใกล้ 0 ซึ่งส่งผลให้เกิดความ Stickness ซึ่งส่งผลดีในกรณีที่มีมี noise เข้ามาในช่วงที่ weights เราจุดที่มีความมั่นใจในการสร้างคำตอบแล้ว เช่น คำตอบเข้าใกล้ 0 หรือ 1 การที่มี noise ทำให้เกิด error ก็จะไม่รบกวนคำตอบที่ program สร้างขึ้นมา แต่ แต่... ก็มีผลเสียเหมือนกันคือเมื่อ program เรียนรู้แล้วยังไม่เข้าสู่จุดที่เข้าใจบทเรียน แต่ node ของเราบาง node ไปถึงจุด stickness เรียบร้อยแล้ว (ทั้งที่จริงยังต้องปรับขึ้นหรือลงอีก) ส่งผลทำให้เกิดความยากในการเรียนรู้ อาจถึงขั้นต้อง reset เพื่อเรียนรู้ใหม่ 
ดังนั้น relu จะไม่มีในเรื่องของ stickness เนื่องจากเมื่อค่ามากกว่า 0 ค่า derivative จะมีค่าเป็น 1 และเมื่อน้อยกว่า 0 ค่า derivative จะมีค่าเป็น 0</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span>
    <span class="k">return</span> <span class="n">output</span>

<span class="k">def</span> <span class="nf">derivative_of_relu</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
    <span class="n">derivative_output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">derivative_output</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>ความจริงในส่วนของ derivative function ค่า input จะต้องเป็น x ไม่ใช่ output แต่เพื่อความง่ายในการคำนวณเมื่อสร้าง neural network program เราจะใช้แบบนี้ในการคำนวณ (ไม่ส่งผลต่อคำตอบ เพราะจำนวนติดลบก็ถูกแปลงเป็น 0 และ ค่า 0 ใส่คำไปใน derivative function ของ relu ก็ให้คำตอบเป็น 0 อยู่ดี)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="เรามาดู-Activation-functions-ที่ถูกใช้ใน-output-layer-กัน">
<a class="anchor" href="#%E0%B9%80%E0%B8%A3%E0%B8%B2%E0%B8%A1%E0%B8%B2%E0%B8%94%E0%B8%B9-Activation-functions-%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%96%E0%B8%B9%E0%B8%81%E0%B9%83%E0%B8%8A%E0%B9%89%E0%B9%83%E0%B8%99-output-layer-%E0%B8%81%E0%B8%B1%E0%B8%99" aria-hidden="true"><span class="octicon octicon-link"></span></a>เรามาดู Activation functions ที่ถูกใช้ใน output layer กัน<a class="anchor-link" href="#%E0%B9%80%E0%B8%A3%E0%B8%B2%E0%B8%A1%E0%B8%B2%E0%B8%94%E0%B8%B9-Activation-functions-%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%96%E0%B8%B9%E0%B8%81%E0%B9%83%E0%B8%8A%E0%B9%89%E0%B9%83%E0%B8%99-output-layer-%E0%B8%81%E0%B8%B1%E0%B8%99"> </a>
</h2>
<p>จะขอแบ่งการใช้ activation จากปัญหาออกเป็น 3 รูปแบบนะครับ</p>
<h3 id="1.-ต้องการคำตอบที่เป็น-Numeric">
<a class="anchor" href="#1.-%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B9%80%E0%B8%9B%E0%B9%87%E0%B8%99-Numeric" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. ต้องการคำตอบที่เป็น Numeric<a class="anchor-link" href="#1.-%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B9%80%E0%B8%9B%E0%B9%87%E0%B8%99-Numeric"> </a>
</h3>
<p>ในที่นี้เราสามารถปล่อยให้ node ไม่ต้องมี activation functions ได้เลย หรือถ้าเราต้องการจำกัดขอบเขตของคำตอบก็ทำการใช้ sigmoid หรือ tanh 
ได้</p>
<h3 id="2.-มีคำตอบหลายคำตอบและมีคำตอบที่ถูกต้องมากกว่า-1-คำตอบ">
<a class="anchor" href="#2.-%E0%B8%A1%E0%B8%B5%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B8%AB%E0%B8%A5%E0%B8%B2%E0%B8%A2%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B9%81%E0%B8%A5%E0%B8%B0%E0%B8%A1%E0%B8%B5%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%96%E0%B8%B9%E0%B8%81%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%A1%E0%B8%B2%E0%B8%81%E0%B8%81%E0%B8%A7%E0%B9%88%E0%B8%B2-1-%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. มีคำตอบหลายคำตอบและมีคำตอบที่ถูกต้องมากกว่า 1 คำตอบ<a class="anchor-link" href="#2.-%E0%B8%A1%E0%B8%B5%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B8%AB%E0%B8%A5%E0%B8%B2%E0%B8%A2%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B9%81%E0%B8%A5%E0%B8%B0%E0%B8%A1%E0%B8%B5%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%96%E0%B8%B9%E0%B8%81%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%A1%E0%B8%B2%E0%B8%81%E0%B8%81%E0%B8%A7%E0%B9%88%E0%B8%B2-1-%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A"> </a>
</h3>
<p>ในกรณีนี้ที่มีหลายคำตอบแสดงว่าเราแทนที่คำตอบด้วย vector โดยแต่ละ element ใน vector จะเป็นตัวแทนของคำตอบนั้น และเนื่องจากมีคำตอบที่ถูกต้องมากกว่า 1 คำตอบ เราเลยให้แต่ละ node มีความน่าจะเป็นของตัวเอง ถ้า node ไหนมีความน่าจะเป็นมากกว่า threshold ที่กำหนดก็จะเลือก node นั้นเป็นคำตอบ หมายความว่าแต่ละ node เราจะใช้ sigmoid เป็น activation functionถ้า</p>
<h3 id="3.-มีคำตอบหลายคำตอบและมีคำตอบที่ถูกต้องเพียงคำตอบเดียว">
<a class="anchor" href="#3.-%E0%B8%A1%E0%B8%B5%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B8%AB%E0%B8%A5%E0%B8%B2%E0%B8%A2%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B9%81%E0%B8%A5%E0%B8%B0%E0%B8%A1%E0%B8%B5%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%96%E0%B8%B9%E0%B8%81%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B9%80%E0%B8%9E%E0%B8%B5%E0%B8%A2%E0%B8%87%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B9%80%E0%B8%94%E0%B8%B5%E0%B8%A2%E0%B8%A7" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. มีคำตอบหลายคำตอบและมีคำตอบที่ถูกต้องเพียงคำตอบเดียว<a class="anchor-link" href="#3.-%E0%B8%A1%E0%B8%B5%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B8%AB%E0%B8%A5%E0%B8%B2%E0%B8%A2%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B9%81%E0%B8%A5%E0%B8%B0%E0%B8%A1%E0%B8%B5%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%96%E0%B8%B9%E0%B8%81%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B9%80%E0%B8%9E%E0%B8%B5%E0%B8%A2%E0%B8%87%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A%E0%B9%80%E0%B8%94%E0%B8%B5%E0%B8%A2%E0%B8%A7"> </a>
</h3>
<p>ถ้าเราใช้ sigmoid แล้วทำการสร้าง neural network program ให้เกิดการเรียนรู้ จะเหมือนเป็นการบอก program ว่า "นี่ๆเจ้า program ใน out layer แต่ละ node ช่วยทำให้ค่าอื่นๆที่ไม่ใช่คำตอบที่ถูกต้องเป็น 0 และให้ node ที่เป็นคำตอบี่ถูกต้องเป็น 1" สมมุติเรามี 3 คำตอบ ได้แก่ [0.2, 0.3, 1]  และคำตอบที่ถูกต้องคือคำตอบที่สาม จะเห็นได้ว่า program เราตอบถุกและมั่นใจมากๆ แต่การเรียนรู้ยังไม่หยุดเนื่องจาก node ที่เหลือยังไม่เป็น 0 ใช่แล้วเป้าหมายของ sigmoid ไม่ใช่แค่ขอคำจอบที่ถูกต้องเป็น 1 แต่คำตอบที่ผิดต้องเป็น 0 ด้วย!</p>
<p>ดังนั้นจากตรงนี้เราเลยต้องมีการปรับปรุงให้มีประสิทธิภาพมากขึ้นนั่นคือเราโฟกัสไปแค่ node ที่เป็นคำตอบที่ถูกแล้วพยายามปรับความน่าจะเป็นให้มากขึ้นเรื่อยๆ ส่วน nodes ที่เหลือจะมีการลดลงของความน่าจะเป็นแบบอัตโนมัติ เอ๊ะทำไมกันนะ? เพราะว่าทุก nodes ใน output vector ทำการแชร์ความน่าจะเป็นกัน (หรือมีความน่าจะเป็นแค่อันเดียว ต่างจาก sigmoid ที่ความน่าจะเป็นจะแยกไปเป็นของแต่ละ node) ทำให้การใช้ sigmoid เป็นการบอกว่า "เฮ้ คำตอบไหนเป็นคำตอบที่ถูกต้องที่สุด ก้ปรับให้คำตอบนั้นค่าความน่าจะเป็นออกมามากที่สุด node อื่นที่ไม่ใช่คำตอบก็จะมีความน่าจะเป็นที่น้อยลงเองอัตโนมัติ"</p>
<p>แล้วคำนวณยังไงนะ sigmoid ให้มองเป็น 3 ขั้นตอนง่าย</p>
<ol>
<li>ทำการ weight sum ระหว่าง input vector และ weight vector (ปกติที่เราทำกันมา)</li>
<li>ทำการ คำนวณ exponential function โดยที่เลขฐานเป็น e</li>
<li>นำค่าแต่ละ node มาตั้งและหารด้วย ค่าของ node ทั้งหมดรวมกัน (หาความน่าจะเป็นของแต่ละ node)</li>
</ol>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">expoential_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">expoential_output</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">expoential_output</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="มาเริ่มสร้าง-Neural-Network-program-สำหรับ-Dataset-กันดีกว่า">
<a class="anchor" href="#%E0%B8%A1%E0%B8%B2%E0%B9%80%E0%B8%A3%E0%B8%B4%E0%B9%88%E0%B8%A1%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87-Neural-Network-program-%E0%B8%AA%E0%B8%B3%E0%B8%AB%E0%B8%A3%E0%B8%B1%E0%B8%9A-Dataset-%E0%B8%81%E0%B8%B1%E0%B8%99%E0%B8%94%E0%B8%B5%E0%B8%81%E0%B8%A7%E0%B9%88%E0%B8%B2" aria-hidden="true"><span class="octicon octicon-link"></span></a>มาเริ่มสร้าง Neural Network program สำหรับ Dataset กันดีกว่า<a class="anchor-link" href="#%E0%B8%A1%E0%B8%B2%E0%B9%80%E0%B8%A3%E0%B8%B4%E0%B9%88%E0%B8%A1%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87-Neural-Network-program-%E0%B8%AA%E0%B8%B3%E0%B8%AB%E0%B8%A3%E0%B8%B1%E0%B8%9A-Dataset-%E0%B8%81%E0%B8%B1%E0%B8%99%E0%B8%94%E0%B8%B5%E0%B8%81%E0%B8%A7%E0%B9%88%E0%B8%B2"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="n">hidden_layer1</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">32</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>เริ่มแรกมากำหนด learning rate กันก่อนให้เท่ากับ 0.003 (ค่ามากน้อยขึ้นอยู่กับข้อมูลและโครงสร้าง program เรายิ่งค่ามากยิ่งมีการปับ weight มากในครั้งเดียว และยิ่งค่าน้อยก็ยิ่งปรับ weight น้อย มีผลกับเวลาในการเรียนรู้) ต่อมาคือ iteration (epochs) ใช้บอกว่าเราจะให้โปรแกรมเห็น dataset ทุกตัวอย่างทั้งหมดกี่ครั้ง และสุดท้ายจำนวน node ใน hidden layer</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">weight_0_1</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">hidden_layer1</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.1</span>
<span class="n">weight_1_2</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_layer1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.1</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>weight_0_1 คือ weight ระหว่าง layer 0 (input layer) กับ  layer 1 (hidden layer 1)</li>
<li>weight_1_2 คือ weight ระหว่าง layer 1 (hidden layer 1) กับ  layer 2 (output layer)</li>
</ul>
<p>โดยเราทำการเลื่อนค่าให้มีค่า mean อยู่ที่ - 0.1 และ มีการกะจายของข้อมูลเป็น 0.2 (scale)</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">error</span><span class="p">,</span> <span class="n">correct_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">layer_0</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">layer_1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">layer_0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_0_1</span><span class="p">))</span>
        <span class="n">layer_2</span> <span class="o">=</span> <span class="n">layer_1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_1_2</span><span class="p">)</span>
        
        <span class="n">error</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">layer_2</span> <span class="o">-</span> <span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">correct_count</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">layer_2</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
        
        <span class="n">layer_2_delta</span> <span class="o">=</span> <span class="n">layer_2</span> <span class="o">-</span> <span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">layer_1_delta</span> <span class="o">=</span> <span class="n">derivative_of_relu</span><span class="p">(</span><span class="n">layer_1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">layer_2_delta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_1_2</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
        
        <span class="n">weight_0_1</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">layer_0</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_1_delta</span><span class="p">)</span>
        <span class="n">weight_1_2</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">layer_1</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_2_delta</span><span class="p">)</span>
    
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Error: </span><span class="si">{</span><span class="n">error</span> <span class="o">/</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Accuracy: </span><span class="si">{</span><span class="n">correct_count</span> <span class="o">/</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Error: 0.002805354217390215
Accuracy: 0.998
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>สังเกตว่าเรามีการปรับ weights ในทุกๆตัวอย่างของ training set (1,000 ตัวอย่าง) ดังนั้นเราจะมีการปรับ weights ทั้งหมด 1,000 x 350 (iteration)  เป็น 350,000 ครั้ง! แต่ว่าการทำแบบนี้การปรับ weights แต่ละครั้งด้วยตัวอย่างเดียวอาจทำให้บางครั้งของการปรับ weights เกิด noise ได้ทำให้ weights เดินผิดเพี้ยนไป แต่โดยรวม weights แต่ละค่าก็พยายามมุ่งเข้าหาค่าของ weights ที่ควรจะเป็นถ้าเราลองวาดเป็นกราฟการเดินทางก็จะมี noise เกิดขึ้นบ้างแต่ trend โดยรวมก็ยังเหมือนเดิม จริงๆการปรับ weights สามารถทำได้ 3 แบบ</p>
<ol>
<li>stochastic gradient descent </li>
<li>full gradient descent </li>
<li>batch gradient descent </li>
</ol>
<p>โดยเกิดจากว่าแรกเริ่มใช้ full gradient descent แต่เมื่อคลังข้อมูลเรามีขนาดใหญ่มากขึ้นเราไม่สามารถใส่ข้อมูลทั้งหมดใน memory รวมถึงการอัพเดท weights ครั้งหนึ่งใช้เวลานานมาก เช่นเมื่อต้องสร้าง program จาก ImageNet dataset ซึ่งมีภาพหลักล้านรูป ถ้าเรารอให้ครบทั้ง dataset แบะอัพเดททีหนึ่งเราจะใช้เวลาในการ training นานมากๆ ดังนั้นจึงเกิดแนวคิดว่าแล้วทำไมเราไม่ใส่ตัวอย่างหนึ่งและปรับ weights หนึ่งที ซึ่งจะทำให้เรามีการปรับ weights ได้หลายครั้งมากต่อการใช้ dataset ทั้งหมด 1 รอบ แต่อย่างที่บอกคือ เราอัพเดทได้เร็วก็จริงแต่การอัพเดทแต่ละครั้งมีโอกาสเป็น noise ดังนั้นเราเลยอยู่กึ่งกลางละกันโดยการใช้ batch gradient descent นั่นคือเรามีโอกาสอัพเดทมากกว่าการใช้ full gradient descent  และการอัพเดทแต่ละครั้งก็มีโอกาสเกิด noise น้อยกว่าและมีโอกาสมุ่งเข้าสู่ค่า weights ที่เป็นคำตอบมากกว่า เนื่องจากมีการเฉลี่ยกันของหลายๆตัวอย่างถึงแม้จะไม่ได้แนวโน้มการอัพเดทที่ดีเท่า full dataset แต่ก็เป็นแนวโน้มการอัพเดทที่ดีกว่าการอัพเดทจากทีละหนึ่งตัวอย่าง (หมายความว่าอาศัยจำนวนการอัพเดทน้อยกว่าแบบ stochastic gradient descent อีกด้วย) ยิ่งไปกว่านั้นการทำแบบ batch ยังเป็นการใช้ optimization library สำหรับ linear algebra อย่างคุ้มค่าเมื่อเราทำการอัพเดท weights ทั้งหมด 10 iterations แบบ batch จะมีความเร็วกว่าแบบ stochastic gradient descent อีกด้วย</p>
<p>จะเห็นว่าผลลัพธ์ออกมาเป็น accuracy 99.9% Wow! แต่ แต่... อย่าลืมว่าอันนี้เราทพสอบกับคลังข้อมูลที่ program เราเรียนรู้โดยตรง เดี๋ยวเราลองมาทดสอบกับคลังข้อมูลที่ program ยังไม่เคยเห็นมาก่อน</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">error</span><span class="p">,</span> <span class="n">correct_count</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    
    <span class="n">layer_0</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">layer_1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">layer_0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_0_1</span><span class="p">))</span>
    <span class="n">layer_2</span> <span class="o">=</span> <span class="n">layer_1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_1_2</span><span class="p">)</span>

    <span class="n">error</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">layer_2</span> <span class="o">-</span> <span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">correct_count</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">layer_2</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
    
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Error: </span><span class="si">{</span><span class="n">error</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Accuracy: </span><span class="si">{</span><span class="n">correct_count</span> <span class="o">/</span> <span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Error: 0.40306746534485255
Accuracy: 0.8093
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>จะเห็นว่าเมื่อเราทดสอบกับ training set เราได้ 99.9% แต่พอมาทดสอบกับ testing set กับได้ 82.71% ซึ่งต่างกันพอสมควร งั้นแปลว่า program เรายังเอามาใช้กับข้อมูลที่ไม่เคยเห็นได้ดีพอทำไมกันนะ?</p>
<p>มีหลากหลาย weights configuration ที่ทำให้ค่า error ต่ำและ program ออกมาดีเมื่อทดสอบกับ training set แต่เป้าหมายเราคือการหา weights ที่สามารถทำให้ทดสอบกับข้อมูลที่ไม่เคยเห็นและได้คำตอบที่ถุกต้องนั่นคือเมื่อทดสอบกับ testing set จะต้องได้ผลััพธืออกมาดี (Generalization) weights ที่เฉพาะเจาะจงกับ training set (ทำให้ค่าที่ทดสอบออกมาดี) เราเรียกว่า overfitting (ทำการจำข้อมูลในคลังข้อมูลแล้วมาตอบคำถามเวลาสอบ) ดังนั้นเราต้องทำยังไงที่จะทำให้เราหา weights ที่ generalization (สร้างองค์ความรู้ออกมาเพื่อใช้ผลิตคำตอบ ต่างจากการจำ) ได้กับข้อมูลที่ไม่ได้อยู่ใน training set ซึ่งบ่อยครั้งที่เมื่อเราลด generalization error จะมีราคาที่ต้องแรกคือ training error จะเพิ่มขึ้น แต่ก็เป็นสิ่งที่รับได้เพราะสุดท้าย training error ที่น้อยมากๆ ก็ไม่สามารถนำไปใช้ประโยชน์จริงได้ แต่กลับ generalization error ที่น้อยเราสามารถนำมันไปใช้ประโยชน์ได้และมั่นใจได้ว่า program เราจะสามารถทำงานกับข้อมูลที่มันไม่เคยเห็นได้ แล้วเราจทำยังไงดี
 # ถึงเวลาการทำ regularization
 การทำ regularization มีหลายแบบตั้งแต่การปรับสมการคำนวณค่า error หรือการเปลี่ยนแปลงโครงสร้างของตัว program (ตัวแปร interact กันยังไง) ขอยกตัวอย่างคร่าวๆโดยการปรับปรุง error โดยเป็นการควบคุม capacity ของ program ในการเรียนรู้ ถ้า capacity สูงเกินไปและปัยหาไม่ซับซ้อนตัว program ก็จะเลือกทางเลือกที่ง่ายที่สุดนั่นคือการจดจำตำตอบ ดังนั้นเราจึงต้องจำกัด capacity ให้เหมาะสมที่จะส่งเสริมให้ program เกิดการเรียนรู้และสร้างองค์ความรู้ขึ้นมาจริงๆ</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>error = (output - true_output) ** 2</p>
<p>เราทำการเพิ่มไปอีกสมการหนึ่ง</p>
<p>error = (output - true_output) <strong> 2 + sum(weight </strong> 2)</p>
<p>จะเห็นว่าเป็นการเอา weights มารวมใน error ด้วย เพื่อเป็นการบอกว่าถ้า weights ไหนไม่ได้มีผลกลับคำตอบจริงๆให้ค่ามุ่งหน้าเข้าสู่ 0 รวมถึง weights ที่มีผลคล้ายๆกันกับคำตอบให้ทำการแชร์ค่ากัน เช่น weight_1 แทนที่จะเป็น 1 ก็ให้เหลือ weight_1 กับ weight_2 เป็น 0.5 แทน การทำแบบนี้ weight แต่ละตัวแปรจะไม่สามารถมีค่าที่มากเกินไปได้ weights แต่ละอันต้องมีการแชร์ และ weights ที่เป็น noise ก็จะต้องถูกปิด (ค่าเข้าใกล้ 0)</p>
<p>แต่สำหรับครั้งนี้ผมจะมาเน้นที่การทำ regularization โดยการปรับตัว program (architecture) โดยการใช้สิ่งที่เรียกว่า dropout</p>
<h1 id="ยินดีต้อนรับ-Dropout">
<a class="anchor" href="#%E0%B8%A2%E0%B8%B4%E0%B8%99%E0%B8%94%E0%B8%B5%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%99%E0%B8%A3%E0%B8%B1%E0%B8%9A-Dropout" aria-hidden="true"><span class="octicon octicon-link"></span></a>ยินดีต้อนรับ Dropout<a class="anchor-link" href="#%E0%B8%A2%E0%B8%B4%E0%B8%99%E0%B8%94%E0%B8%B5%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%99%E0%B8%A3%E0%B8%B1%E0%B8%9A-Dropout"> </a>
</h1>
<p>การใช้ dropout คือการ random เพื่อทำการ shut down node ที่เลือกลงไป (แทนค่าเป็น 0) ทำไมถึงใช้งานได้ละ?</p>
<h2 id="การสร้าง-neural-network-program-มาเยอะๆหลาย-program-แล้วทำการเฉลี่ยคำตอบ-(Ensembling)">
<a class="anchor" href="#%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87-neural-network-program-%E0%B8%A1%E0%B8%B2%E0%B9%80%E0%B8%A2%E0%B8%AD%E0%B8%B0%E0%B9%86%E0%B8%AB%E0%B8%A5%E0%B8%B2%E0%B8%A2-program-%E0%B9%81%E0%B8%A5%E0%B9%89%E0%B8%A7%E0%B8%97%E0%B8%B3%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%80%E0%B8%89%E0%B8%A5%E0%B8%B5%E0%B9%88%E0%B8%A2%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A-(Ensembling)" aria-hidden="true"><span class="octicon octicon-link"></span></a>การสร้าง neural network program มาเยอะๆหลาย program แล้วทำการเฉลี่ยคำตอบ (Ensembling)<a class="anchor-link" href="#%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87-neural-network-program-%E0%B8%A1%E0%B8%B2%E0%B9%80%E0%B8%A2%E0%B8%AD%E0%B8%B0%E0%B9%86%E0%B8%AB%E0%B8%A5%E0%B8%B2%E0%B8%A2-program-%E0%B9%81%E0%B8%A5%E0%B9%89%E0%B8%A7%E0%B8%97%E0%B8%B3%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%80%E0%B8%89%E0%B8%A5%E0%B8%B5%E0%B9%88%E0%B8%A2%E0%B8%84%E0%B8%B3%E0%B8%95%E0%B8%AD%E0%B8%9A-(Ensembling)"> </a>
</h2>
<p>เมื่อเราทำการสร้างมาหลายๆ program แน่นอนว่าแต่ละ program ก็จะมีการเรียกรู้ที่ต่างกัน และผิดพลาดต่างกัน เรา overfitting ก็มีการ overfitting ที่ต่างกัน เมื่อ program เรียนรู้จะเรียนรู้สิ่งที่เป็น concept หลักก่อน จากนั้นถ้ายังเหลือ capacity ก็จะเริ่มเรียนรู้รายละเอียดของตัวอย่างนั้นๆ (fine grained detail) เมื่อนำไปใช้ก็จะเกิดข้อผิดพลาดที่แตกต่างกันเพราะมีการ fit noise ที่ต่างกัน ทำให้เมื่อเรานำทั้งหมดมาเฉลี่ยกันจะมีโอกาสที่จะเกิดการ cancel noise ออกไปเหลือแค่ concept หลักที่มีการเรียนรู้ร่วมกัน</p>
<p>แต่การสร้างหลายๆ programs และให้มันเรียนรู้ มันใช้เวลาเยอะมากเลยนะ ดังนั้น dropout จึงเข้ามา นั่นคือเราสร้าง program แค่อันเดียวแล้วทุกครั้งที่มีการใส่ inputs เข้าไปในโปรแกรมเราจะทำการสุ่มปิด nodes ใน program เท่ากับว่าเรามีการสร้าง sub program (sub neural network) ขึ้นมาและที่สำคัญคือ sub program มีจำนวน weights น้อยลงทำให้ลดการเกิด overfitting และถ้าเรามองการทำ dropout คือการสร้าง sub program แสดงว่า sub program หนึ่ง เรียนรู้ batch ของ training set และอีก sub program หนึ่งเรียนรู้อีก batch ของ training set เปรียบเสมือน เราสร้าง 2 program ที่เรียนรู้ข้อมูลคนละส่วน เราเรียกว่า การทำ begging</p>
<p>แต่ dropout ให้เรามากว่านั้นเนื่องจาก sub program ถูกสร้างออกมาจาก program ใหญ่แสดงว่า weights ต้องมีการแชร์กันระหว่าง sub program จำได้ไหม การทำ regularization โดยการปรับสมการ error คือการจำกัดทำให้ค่า weights มุ่งหน้าเข้าสู่ 0 แต่! การทำให้ sub program มีการแชร์ weights กันทำให้ค่า weights ถูกจำกัดและปรับมุ่งหน้าเข้าสู่ค่าที่มันควรจะเป็น (ไม่ใช่ 0)จาก sub program อื่นๆ นี่ยิ่งทำให้ dropout ทรงพลังขึ้นไปอีก 
แล้วสุดท้ายเราจะมาเฉลี่ยกันยังไง? ถ้า program มี hidden layer แค่ 1 layer การสุ่มเอา sub program มา (จำนวนมากหรือทั้งหมดถ้า node มีไม่มาก) แล้วทำการเฉลี่ยคำตอบก่อนเข้า activation function ใน output nodes จะมีค่าเท่ากับการปิดการทำงานของ dropout หรือการใช้ full program ในการผลิตคำตอบ แล้วนำคำตอบที่ได้มาหารด้วย ratio ของ nodes ที่ปิดไปใน hidden layer</p>
<p>แต่ถ้าเรามี hidden layer มากกว่า 2 การทำแบบเดิมจะไม่ให้ค่าที่ทำกันแล้วแต่ก็ยังสามารถเป็นค่าประมาณที่ใช้จริงได้อยู่</p>
<h1 id="พอแล้ว-Theory-มาลงมือจริงกันเลยดีกว่า">
<a class="anchor" href="#%E0%B8%9E%E0%B8%AD%E0%B9%81%E0%B8%A5%E0%B9%89%E0%B8%A7-Theory-%E0%B8%A1%E0%B8%B2%E0%B8%A5%E0%B8%87%E0%B8%A1%E0%B8%B7%E0%B8%AD%E0%B8%88%E0%B8%A3%E0%B8%B4%E0%B8%87%E0%B8%81%E0%B8%B1%E0%B8%99%E0%B9%80%E0%B8%A5%E0%B8%A2%E0%B8%94%E0%B8%B5%E0%B8%81%E0%B8%A7%E0%B9%88%E0%B8%B2" aria-hidden="true"><span class="octicon octicon-link"></span></a>พอแล้ว Theory มาลงมือจริงกันเลยดีกว่า<a class="anchor-link" href="#%E0%B8%9E%E0%B8%AD%E0%B9%81%E0%B8%A5%E0%B9%89%E0%B8%A7-Theory-%E0%B8%A1%E0%B8%B2%E0%B8%A5%E0%B8%87%E0%B8%A1%E0%B8%B7%E0%B8%AD%E0%B8%88%E0%B8%A3%E0%B8%B4%E0%B8%87%E0%B8%81%E0%B8%B1%E0%B8%99%E0%B9%80%E0%B8%A5%E0%B8%A2%E0%B8%94%E0%B8%B5%E0%B8%81%E0%B8%A7%E0%B9%88%E0%B8%B2"> </a>
</h1>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="n">hidden_layer1</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">32</span>
<span class="n">weight_0_1</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">hidden_layer1</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.1</span>
<span class="n">weight_1_2</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_layer1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.1</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>เหมือนเดิม เริ่มแรกมากำหนด learning rate กันก่อนให้เท่ากับ 0.003 (ค่ามากน้อยขึ้นอยู่กับข้อมูลและโครงสร้าง program เรายิ่งค่ามากยิ่งมีการปับ weight มากในครั้งเดียว และยิ่งค่าน้อยก็ยิ่งปรับ weight น้อย มีผลกับเวลาในการเรียนรู้) ต่อมาคือ iteration (epochs) ใช้บอกว่าเราจะให้โปรแกรมเห็น dataset ทุกตัวอย่างทั้งหมดกี่ครั้ง และสุดท้ายจำนวน node ใน hidden layer</p>
<ul>
<li>weight_0_1 คือ weight ระหว่าง layer 0 (input layer) กับ  layer 1 (hidden layer 1)</li>
<li>weight_1_2 คือ weight ระหว่าง layer 1 (hidden layer 1) กับ  layer 2 (output layer)</li>
</ul>
<p>โดยเราทำการเลื่อนค่าให้มีค่า mean อยู่ที่ - 0.1 และ มีการกะจายของข้อมูลเป็น 0.2 (scale)</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">error</span><span class="p">,</span> <span class="n">correct_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">layer_0</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">layer_1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">layer_0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_0_1</span><span class="p">))</span>
        <span class="n">dropout_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">layer_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">layer_1</span> <span class="o">*=</span> <span class="n">dropout_mask</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">layer_2</span> <span class="o">=</span> <span class="n">layer_1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_1_2</span><span class="p">)</span>
        
        <span class="n">error</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">layer_2</span> <span class="o">-</span> <span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">correct_count</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">layer_2</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
        
        <span class="n">layer_2_delta</span> <span class="o">=</span> <span class="n">layer_2</span> <span class="o">-</span> <span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">layer_1_delta</span> <span class="o">=</span> <span class="n">derivative_of_relu</span><span class="p">(</span><span class="n">layer_1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">layer_2_delta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_1_2</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
        <span class="n">layer_1_delta</span> <span class="o">=</span> <span class="n">layer_1_delta</span> <span class="o">*</span> <span class="n">dropout_mask</span>
        
        <span class="n">weight_0_1</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">layer_0</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_1_delta</span><span class="p">)</span>
        <span class="n">weight_1_2</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">layer_1</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_2_delta</span><span class="p">)</span>
    
<span class="n">test_error</span><span class="p">,</span> <span class="n">test_correct_count</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>

    <span class="n">layer_0</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">layer_1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">layer_0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_0_1</span><span class="p">))</span>
    <span class="n">layer_2</span> <span class="o">=</span> <span class="n">layer_1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_1_2</span><span class="p">)</span>

    <span class="n">test_error</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">layer_2</span> <span class="o">-</span> <span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">test_correct_count</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">layer_2</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>


<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Error: </span><span class="si">{</span><span class="n">error</span> <span class="o">/</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Accuracy: </span><span class="si">{</span><span class="n">correct_count</span> <span class="o">/</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Test Error: </span><span class="si">{</span><span class="n">test_error</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Test Accuracy: </span><span class="si">{</span><span class="n">test_correct_count</span> <span class="o">/</span> <span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Error: 0.2771398481038192
Accuracy: 0.869
Test Error: 0.3636057630375286
Test Accuracy: 0.8138
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>จะเห็นได้ว่า dropout ทำให้ผลการทดสอบกับ testing set ดีขึ้น หมายความว่าทำให้การ generalization ดีขึ้น ส่วนมากการทำให้ program มีการ generalization ดีขึ้นจะต้องมีการแลกเปลี่ยนกับผลลัพธ์ metric ของ training set ในกรณีนี้ยังเราสามารถลองเพิ่มจำนวน iterations ได้เพื่อดูว่าเรายังสามารถปรับปรุง program ให้ดีขึ้นไปได้อีกไหม โดยต้องอย่าลืมตรวจสอบ generalization gap (ความต่างของ error ระหว่าง training และ testing set) มีความกว้างมากไหม ถ้ากว้างมากอาจหมายความว่าเรา overfitting เรียบร้อยแล้ว
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>ตอนนี้เรากำลังใช้ testing set ในการปรับตัวแปรที่จำเป็นในการสร้าง program เช่นจำนวน iterations หรือ learning rate การทำแบบนี้เป็นการพยายาม overfitting ทางอ้อมกับ testing set ดังนั้นเพื่อป้องกันสถานการณ์นี้เราจะมี validation set มาขั้นกลาง โดยเราใช้ validation set นี้ในการปรับ hyperparameters (parameters ที่ program ไม่ได้ปรับตรงๆในช่วงเรียนรู้ แต่เป็นตัวแปรสำคัญและมีผลต่อการเรียนรู้)
</div>
<h1 id="สุดท้ายแล้ว-ถ้าเราอยากทำ-Batch-Gradient-Descent-ละจะทำยังไง">
<a class="anchor" href="#%E0%B8%AA%E0%B8%B8%E0%B8%94%E0%B8%97%E0%B9%89%E0%B8%B2%E0%B8%A2%E0%B9%81%E0%B8%A5%E0%B9%89%E0%B8%A7-%E0%B8%96%E0%B9%89%E0%B8%B2%E0%B9%80%E0%B8%A3%E0%B8%B2%E0%B8%AD%E0%B8%A2%E0%B8%B2%E0%B8%81%E0%B8%97%E0%B8%B3-Batch-Gradient-Descent-%E0%B8%A5%E0%B8%B0%E0%B8%88%E0%B8%B0%E0%B8%97%E0%B8%B3%E0%B8%A2%E0%B8%B1%E0%B8%87%E0%B9%84%E0%B8%87" aria-hidden="true"><span class="octicon octicon-link"></span></a>สุดท้ายแล้ว ถ้าเราอยากทำ Batch Gradient Descent ละจะทำยังไง<a class="anchor-link" href="#%E0%B8%AA%E0%B8%B8%E0%B8%94%E0%B8%97%E0%B9%89%E0%B8%B2%E0%B8%A2%E0%B9%81%E0%B8%A5%E0%B9%89%E0%B8%A7-%E0%B8%96%E0%B9%89%E0%B8%B2%E0%B9%80%E0%B8%A3%E0%B8%B2%E0%B8%AD%E0%B8%A2%E0%B8%B2%E0%B8%81%E0%B8%97%E0%B8%B3-Batch-Gradient-Descent-%E0%B8%A5%E0%B8%B0%E0%B8%88%E0%B8%B0%E0%B8%97%E0%B8%B3%E0%B8%A2%E0%B8%B1%E0%B8%87%E0%B9%84%E0%B8%87"> </a>
</h1>
<p>เราทำการนำตัวแปร gradients ของทุก nodes ใน output layer จากแต่ละ example มาหารด้วยจำนวนสมาชิกใน batch เพราะการปรับ weights ของเราจะขึ้นอยู่กับตัวอย่างทั้งหมดใน batch นั้นเราเลยต้องทำการเฉลี่ยออกมา
</p>
<div class="flash flash-warn">
    <svg class="octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10.561 1.5a.016.016 0 00-.01.004L3.286 8.571A.25.25 0 003.462 9H6.75a.75.75 0 01.694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0012.538 7H9.25a.75.75 0 01-.683-1.06l2.008-4.418.003-.006a.02.02 0 00-.004-.009.02.02 0 00-.006-.006L10.56 1.5zM9.504.43a1.516 1.516 0 012.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 01-.871.354h-.302a1.25 1.25 0 01-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004L9.503.429z"></path></svg>
    <strong>Important: </strong>ความจริงเรื่องการหารมันเป็นผลเนื่องมาจากสุดท้ายเราคำนวนหา error ของแต่ละตัวอย่างและนำค่า error ทั้งหมดของแต่ละตัวอย่างมารวมกัน ตรงจุดนี้เราทำการหารด้วยตัวอย่างทั้งหมด นั่นทำให้พอเราหา gradients ของแต่ละ nodes จากแต่ละตัวอย่าง จะต้องหารด้วยจำนวนสมาชิกใน batch แต่ถ้าสมการ error เราไม่มีการหารด้วยตัวอย่างทั้งหมด ตอนเราหา gradients ก็ไม่ต้องหารจำนวนสมาชิกทั้งหมดใน batch
</div>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="n">hidden_layer1</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mi">3000</span><span class="p">,</span> <span class="mi">32</span>
<span class="n">weight_0_1</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">hidden_layer1</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.1</span>
<span class="n">weight_1_2</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_layer1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span> <span class="o">-</span> <span class="mf">0.1</span>
 
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">error</span><span class="p">,</span> <span class="n">correct_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)):</span>
        <span class="n">batch_start</span><span class="p">,</span> <span class="n">batch_end</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">layer_0</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">batch_start</span><span class="p">:</span><span class="n">batch_end</span><span class="p">]</span>
        <span class="n">layer_1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">layer_0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_0_1</span><span class="p">))</span>
        <span class="n">dropout_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">layer_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">layer_1</span> <span class="o">*=</span> <span class="n">dropout_mask</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">layer_2</span> <span class="o">=</span> <span class="n">layer_1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_1_2</span><span class="p">)</span>
        
        <span class="n">error</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">layer_2</span> <span class="o">-</span> <span class="n">y_train</span><span class="p">[</span><span class="n">batch_start</span><span class="p">:</span><span class="n">batch_end</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">correct_count</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">layer_2</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">batch_start</span> <span class="o">+</span> <span class="n">k</span><span class="p">:</span><span class="n">batch_start</span> <span class="o">+</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
        
        <span class="n">layer_2_delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">layer_2</span> <span class="o">-</span> <span class="n">y_train</span><span class="p">[</span><span class="n">batch_start</span><span class="p">:</span><span class="n">batch_end</span><span class="p">])</span> <span class="o">*</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">batch_size</span>
        <span class="n">layer_1_delta</span> <span class="o">=</span> <span class="n">derivative_of_relu</span><span class="p">(</span><span class="n">layer_1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">layer_2_delta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_1_2</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
        <span class="n">layer_1_delta</span> <span class="o">=</span> <span class="n">layer_1_delta</span> <span class="o">*</span> <span class="n">dropout_mask</span>
        
        <span class="n">weight_0_1</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">layer_0</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_1_delta</span><span class="p">)</span>
        <span class="n">weight_1_2</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">layer_1</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_2_delta</span><span class="p">)</span>
    
<span class="n">test_error</span><span class="p">,</span> <span class="n">test_correct_count</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>

    <span class="n">layer_0</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">layer_1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">layer_0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_0_1</span><span class="p">))</span>
    <span class="n">layer_2</span> <span class="o">=</span> <span class="n">layer_1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_1_2</span><span class="p">)</span>

    <span class="n">test_error</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">layer_2</span> <span class="o">-</span> <span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">test_correct_count</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">layer_2</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>


<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Error: </span><span class="si">{</span><span class="n">error</span> <span class="o">/</span> <span class="n">batch_size</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Accuracy: </span><span class="si">{</span><span class="n">correct_count</span> <span class="o">/</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Test Error: </span><span class="si">{</span><span class="n">test_error</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Test Accuracy: </span><span class="si">{</span><span class="n">test_correct_count</span> <span class="o">/</span> <span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Error: 3.449173614715377
Accuracy: 0.835
Test Error: 0.3309046876836692
Test Accuracy: 0.8426
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>สังเกตว่าการใช้ batch gradient descent จะเป็นการใช้ประโยชน์จากการ optimization ของ linear algebra ทำให้การอัพเดท weights 1 ครั้งเป็นการใช้ตัวอย่างเป็นกลุ่ม ส่งผลให้การประมวลผล training set ใน 1 iteration มีความเร็วมากขึ้น ส่งผลให้เราสามารถเพิ่มจำนวน iterations ได้มากขึ้น นอกจากนี้ถ้ามีการดู error ในทุกๆ iterations ตัว error จะมีแนวโน้มที่มีความเรียบเนียน (smooth) กว่าการอัพเดท weights ที่ละตัวอย่าง</p>
<p>ก่อนจากกัน ทุกคนสามารถลองเล่นโดยการปรับ การ random weights ให้เหมาะสมยิ่งขึ้น (ป้องกัน vanishing gradient), หรือการทำ gradient clip (เพื่อป้องกัน exploding gradient) และ activation functions ดูได้ เพื่อดุประสิทธิภาพของตัว program เมื่อมีการปรับเปลี่ยน activation functions เนื่องจากตัวอย่างที่ได้ทำมาทั้งหมดจะเป็นการใช้ relu ส่วน output nodes ไม่ได้ใช้ activation functions ซึ่งจากโจทย์ข้อนี้ควรจะใช้ softmax หรือ sigmoid ใน output layers เพื่อให้คำตอบอยู่ในช่วง [0, 1] 
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>ถ้าใช้ sigmoid หรือ tanh เป็น hidden layer activation function อย่าลืม random weights ให้เข้าใกล้ 0 เพื่อที่ค่า derivatives จะได้ไม่เข้าใกล้ 0 เกินไปจน program ยากที่จะเรียนรู้หรือปรับปรุง weights
</div>
หวังว่าทุกคนที่เข้ามาอ่านจะสนุกและได้ความรู้กันนะครับ
<h1 id="Acknowledge">
<a class="anchor" href="#Acknowledge" aria-hidden="true"><span class="octicon octicon-link"></span></a>Acknowledge<a class="anchor-link" href="#Acknowledge"> </a>
</h1>
<p>Thank you for all the knowledges from</p>
<ul>
<li>Deep Learning: Ian Goodfellow and Yoshua Bengio and Aaron Courville</li>
<li>Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems: Aurelien Geron</li>
<li>Deep Learning with Python: François Chollet</li>
<li>Neural Networks and Deep Learning: Michael Nielsen</li>
<li>Grokking Deep Learning: Andrew W. Trask</li>
<li>Deep Learning Specialization: Andrew Ng</li>
<li>Python Data Science Handbook: Essential Tools for Working with Data: Jake VanderPlas</li>
</ul>
<p>"If you cannot sleep in the night, you need more time to understand it"</p>
<p>Burin Sirisrimungkorn</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="BurinS/whyboyburin"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/whyboyburin/deep%20learning/2020/09/22/Deep-Learning-with-More-Sensation.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/whyboyburin/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/whyboyburin/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/whyboyburin/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>I believe in Mathematics and Science to make Giant Leap for Humankind.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/BurinS" title="BurinS"><svg class="svg-icon grey"><use xlink:href="/whyboyburin/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
