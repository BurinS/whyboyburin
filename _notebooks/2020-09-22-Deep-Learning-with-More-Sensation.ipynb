{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (TH) Deep Learning with More Sensation\n",
    "> แล้วถ้าเราอยากจะสร้าง Deep Learning program ขึ้นมาจริงๆจะต้องทำยังไงนะ\n",
    "\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [deep learning]\n",
    "- author: Burin Sirisrimungkorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from tensorflow.keras import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as let_show\n",
    "import math\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# เมื่อต้องเผชิญหน้ากับ Dataset\n",
    "เมื่อเราต้องการที่จะสร้าง neural network program สิ่งที่เราต้องมีก็คือ dataset เปรียบได้กับมันคือคลังข้อมูลที่รอให้เราได้เข้าไปเรียนรู้ พอเราจบการเรียนรู้ เราก็จะมีความสามารถในการทำหรือแก้ปัญหาสิ่งนั้นๆตามที่เราได้เรียนหรือศึกษามา \n",
    "## MNIST Dataset \n",
    "dataset อันนี้จะเป็นการรวมข้อมูลเกี่ยวกับ ตัวเลขที่ถูกเขียนด้วยลายมือมนุษย์ โดยได้มีการเก็บรวบรวมโดย National Institute of Standards and Technology และได้ถูกทำมาจัดรูปแบบและแก้ไขปรับปรุงให้เป็น dataset ที่ใช้สำหรับ machine learning โดย Yann Lecun \n",
    "MNIST ได้ถูก Yann Lecun นำไปใช้ในปี 1998 สำหรับการสร้าง neural network ซึ่ง LeNet-5 คือ program ตัวแรกที่ใช้สำหรับการจัดกลุ่มตัวเลขที่เขียนด้วยลายมือมนุษย์ นี่เป็นเหตุการณ์สำคัญอีกหน้าหนึ่งของประวัติศาสตร์ AI\n",
    "## โอเค โอเค เรามาลุยกันเลย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "จำนวนข้อมูลใน Training set: 60000\n",
      "จำนวนข้อมูลใน Testing set: 10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "print(f\"จำนวนข้อมูลใน Training set: {x_train.shape[0]}\")\n",
    "print(f\"จำนวนข้อมูลใน Testing set: {x_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราทำการโหลดข้อมูล MNIST จาก Keras ซึ่งเป็น framework ที่ใช้สำหรับสร้าง neural network program โดย Keras จะมี datasets ที่เตรียมไว้ให้บางส่วน (MNIST ก็เป็นหนึ่งในนั้น) จะสังเกตได้ว่าข้อมูลจะถูกแบ่งแยกออกมาเป็น 2 ส่วนนั่นคือ training และ testing set ทำไมกันนะ?\n",
    "อย่างที่เรารู้ในตอนนี้คือ เราพยายามจะสร้าง prgram ด้วยข้อมูล เมื่อผลลัพธ์ทดสอบดูกับข้อมูลที่เราใช้สร้างแล้วความสามารถในการให้คำตอบเราดีเยี่ยม แต่ แต่... นี่ก็ไม่ได้การันตีว่า พอเรานำไปใช้จริงกับข้อมูลที่ไม่ได้อยู่ในกลุ่ม training จะเกิดอะไรชึ้น ดังนั้น เราเลยต้องมีข้อมูลอีกกลุ่มมาทำเพื่อทำการทดสอบว่าถ้าเป็นข้อมูลที่ไม่ได้อยู่ในกลุ่มที่ใช้เรียนรู้ ความรู้หรือ program ที่เราได้มา ยังจะสามารถแก้ไขปัญหาได้อย่างถูกต้องไหม \n",
    "> Note: แน่นอนว่า training และ testing set ข้อมูลข้างในจะต้องไม่ใช่ตัวเดียวกัน แต่คุณลักษณะของข้อมูลทั้ง 2 กลุ่มยังต้องใกล้เคียงกัน (มาจาก statistical distribution เดี่ยวกัน) เราคงไม่สามารถเอาความรู้ที่เรียนจากภาพถ่ายรูปมุมหนึ่ง ไปใช้กับปัญหาที่เป็นรูปประเภทเดียวกันแต่ถ่ายจากอีกมุมนึงที่ไม่ปรากฏในคลังข้อมูลที่ใช้เรียนรู้\n",
    "\n",
    "นอกจากที่ถ้าสังเกตเห็นมันจะมีข้อมูลอีก 2 ตัวแปรที่ขึ้นต้นด้วย \"y\" มันคืออะไรกันนะ? จริงๆแล้วมันหมายถึงตัวแปรที่เก็บ labels หรือก็คือคำตอบที่ถูกผูกติดกับตัวอย่าง โดยในแต่ละตัวอย่างก็จะถูกผูกกับ label เอ๊ะแล้วค่าในตัวอย่างมันเป็นอะไรได้บ้าง?\n",
    "เราจะประกาศว่าคำตอบของปัญหานี้เป็นอะไรได้บ้างซึ่งแต่ละคำตอบเราจะเรียกว่า class ดังนั้น MNIST ที่มีคำตอบที่เป็นไปได้คือ 0 - 9 จะมีคำตอบทั้งหมด 10 คำตอบ ถ้าเรามองเป็น predefined set of classes จะได้เป็น {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
    " \n",
    "## มาลองดูตัวอย่างกัน\n",
    "เดี๋ยวเราลองมาดูตัวอย่างแรกของ training set กัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "มันคือเลข 5\n",
      "รูปภาพมีขนาด 28 x 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa2c09497d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "print(f\"มันคือเลข {y_train[0]}\")\n",
    "print(f\"รูปภาพมีขนาด {x_train.shape[1]} x {x_train.shape[2]}\")\n",
    "let_show.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากภาพและผลลัพธ์จะเห็นได้ว่าตัวอย่างอันแรกใน training set เป็นเลข 5 โดยภาพมีขนาด 28 x 28 (height x width เนื่องจาก numpy จะแสดง row ก่อนแล้วค่อย column) แต่ก่อนที่เราจะเริ่มสร้าง neural network program กัน เราต้องทำอะไรเพิ่มอีกหน่อย\n",
    "\n",
    "- ทำการ preprocess ค่าของแต่ละตัวอย่าง\n",
    "- ทำการแปลงคำตอบให้อยู่ในรูปของ vector\n",
    "\n",
    "# ทำการ Preprocess รูปภาพ\n",
    "ตอนนี้ค่าที่อธิบายถึงรูปภาพเรามีค่าเป็นยังไงนะ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241]]\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "print(x_train[0, 0:14, 0:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จะเห็นว่ามีค่าตั้งแต่ 0 จนถึง 253 (จริงๆค่าที่เป็นไปได้มากสุดคือ 255 เนื่องจากข้อมูลเก็บเป็น unsign 8 bits) แล้วแบบนี้หมายความว่ายังไง เวลา program เราเรียนรู้ กลุ่มตัวแปรที่มีค่ามากจะเป็นตัวแปรที่มีอำนาจในการตัดสินให้คำตอบ ทั้งที่ตัวแปรอื่นๆที่ค่าน้อยก็มีโอกาสที่เป็นตัวแปรที่มีความสำคัญต่อการผลิตคำตอบ ซึ่งมีผลกับการเรียนรู้ ดังนั้นเราต้องนำค่าตัวแปรต่างๆที่ใช้อธิบายรูปภาพมาทำการ normalization โดยในกรณีนี้คือนำค่าทั้งหมดมาหารด้วย 255 ซึ่งการทำอย่างนี้นอกจากจะทำให้ความสำคัญของแต่ละตัวแปรมีน้ำหนักพอๆกันแล้ว ยังช่วยให้เรียนรู้ได้เร็วขึ้นด้วย (จินตนาการว่ากราฟแสดงความสัมพันธ์ระหว่าง error และ weight 2 ตัว ที่การส่ายไปส่ายมาแทนที่จะตรงไปยังจุดที่เป็นเป้าหมาย)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01176471 0.07058824]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      "  0.66666667 0.99215686]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.19215686 0.93333333 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.07058824 0.85882353 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      "  0.99215686 0.80392157]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.05490196 0.00392157 0.60392157\n",
      "  0.99215686 0.35294118]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.54509804\n",
      "  0.99215686 0.74509804]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.04313725\n",
      "  0.74509804 0.99215686]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.1372549  0.94509804]]\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "print(x_train[0, 0:14, 0:14] / 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เดี๋ยวเรามาทำการ normalization รูปภาพทั้งหมดกันทั้งใน training และ testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "มาดูข้อมูลรูปภาพใน training set กัน :(1000, 784)\n",
      "มาดูข้อมูลรูปภาพใน testing set กัน :(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train / 255\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1] * x_train.shape[2]))[11000:12000]\n",
    "\n",
    "x_test = x_test / 255\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1] * x_test.shape[2]))\n",
    "\n",
    "print(f\"มาดูข้อมูลรูปภาพใน training set กัน :{x_train.shape}\")\n",
    "print(f\"มาดูข้อมูลรูปภาพใน testing set กัน :{x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จะเห็นว่าเราทำการหารด้วย 255 เพื่อทำให้ตัวแปรต่างๆในแต่ละรูปภาพมีความสำคัญต่อคำตอบพอๆกัน นอกจากนี้ถ้าจำได้ neural netwrok program จะรับ inputs เข้ามาเป็น 1D list หรือ vector เนื่องจากรูปภาพเราถูกนำเสนอเป็น 28 x 28 เราทำการแปลงเป็น vector โดยการใช้ reshape method ซึ่งเป็นฟังก์ชันที่ใช้สำหรับปรับ representation ของ array ในกรณีนี้เราปรับจากการนำเสนอในรูปแบบ metric เป็น vector\n",
    "โดยถ้าสังเกตใน training set จาก 60,000 ตัวอย่าง ในกรณีนี้ผมหยิบมาแค่ 1,000 ตัวอย่างสำหรับใช้สร้าง neural network program (เพ่ือความเร็ว)ถ้าต้องการใช้ตัวอย่างทั้งหมดก็ทำการลบในส่วนการเลือกตัวอย่างออกไปนะครับ ส่วน testing set ยังคงจำนวนตัวอย่างไว้เท่าเดิม\n",
    "# ทำไมเราต้องทำการแปลงคำตอบให้อยู่ในรูปของ Vector ?\n",
    "สังเกตว่าถ้าปัญหาเรามีคำตอบเดียวเราจะแทนที่ด้วย output node แค่ node เดียวแต่ถ้าเรามีหลายคำตอบเราจะมี output nodes มากกว่า 1 node ทำไมเราไม่ใช้ node เดียวแล้วให้ค่าคำตอบเป็น 0 - 9 ละ?\n",
    "เนื่องจากถ้าเราทำแบบนั้น เท่ากับเราบอก program เราด้วยว่าความสัมพันธ์ระหว่างคำตอบของเราจะมี semantic relationship นั่นคือ คำตอบที่เป็นเลข 8 มีค่ามากกว่า เลข 7 ซึ่งความจริงถ้ามองในมุมตัวเลขก็ถูก และนอกจากนี้คำตอบที่เราได้ยังมีโอกาสที่จะเป็นค่าระหว่างคำตอบจริงๆที่เราต้องการ เช่นคำตอบออกมาเป็น 7.5 แต่คราวนี้ถ้ามองใบริบทที่เรากำลังทำอยู่ เราต้องการแค่อยากรู้ว่าจากรูปที่ได้มาตัว program ต้องบอกเราให้ได้ว่ามันเป็นเลขอะไรโดยไม่สนว่าเลขนั้นจะมีค่ากว่าอีกเลขไหม\n",
    "> Note: คำตอบในกรณี MNIST เป็น discrete value และไม่มีความสัมพันธ์กันระหว่าง value\n",
    "\n",
    "\n",
    "ซึ่งจะนำไปสู่เรื่องของการเลือกแสดงคำตอบของ output layer โดยถ้าคำตอบเราเป็น numeric หรือเป็น continuous value เราก็สามารถนำเสนอ output layer ได้ด้วย node เดียวให้คำตอบเป็น numeric ในช่วงคำตอบที่สนใจ เช่น [-1, 1] ใช้สามารถ tanh เป็น activation function หรือถ้าไม่มีการกำหนดขอบเขตตายตัวก็ไม่ต้องมี activation function แต่ถ้าคำตอบที่เราต้องการเป็น category หรือ predefined set of classes จะทำให้ output layer เราต้องนำเสนอด้วย nodes ที่มีจำนวนเท่ากับ classes โดยในแต่ละ node จะให้ค่าออกเป็นความน่าจะเป็นว่ามีโอกาสเป็น class นี้หรือเปล่า โดยจะเป็น unrelated probability (แต่ละ node มีความน่าจะเป็นของตัวเองไม่เกี่ยวข้องกัน เช่น sigmoid function) หรือ related probability (แต่ละ node ใน layer มีความน่าจะเป็นที่เกี่ยวข้องกัน เช่น การใช้  softmax function)\n",
    "\n",
    "สรุปคือคำตอบของปัญหา MNIST เป็นแบบ discrete value และ order ไม่สำคัญเนื่องจากไม่มีความสัมพันธ์ระหว่าง class ดังนั้นเราจะทำการแปลงจากข้อมูลปัจจุบันที่แสดงคำตอบเป็นค่าๆเดียว {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} ให้เป็น vector ที่แต่ละ node คือตัวแทนว่าเป็น node ที่บ่งบอกถึงเลขอะไร เช่น node ที่ 0 เป็นตัวแทนของเลข 0 ส่วน node ที่ 8 เป็นตัวแทนของเลข 8 โดยค่าในแต่ละ node บ่งบอกถึงความน่าจะเป็นที่จะเป็นเลขนั้น \n",
    "ขอยกตัวอย่างเช่นเลข 8 เมื่อเราทำการแปลงเป็น vector จะได้เป็น [0, 0, 0, 0, 0, 0, 0, 0, 1, 0] จะเห็นได้ว่า node ที่ไม่ได้เป็นคำตอบจะถูกแทนที่ค่าด้วย 0 ส่วน node ที่เป็นคำตอบจะถูกแทนที่ค่าด้วย 1 ถ้าลองแปลงตัวเลขอื่นๆด้วยจะทำให้เราเห็นรูปแบบบางอย่างนั่นคือตัวเลขที่ไม่ใช่คำตอบจะเป็น 0 ทั้งหมด และตัวเลขที่เป็นคำตอบจะเป็น 1 และ! เนื่องจากคำตอบเรามีแค่คำตอบเดียว node ที่เป็น 1 เลยมีแค่ node เดียวที่เหลือเป็น 0 เราเลยเรียกการแปลงข้อมูลจาก numeric เป็น vector ว่าการทำ one hot encoder แน่นอว่าเรานำเสนอข้อมูลอบบนี้ในกรณีที่ปัญหาเราเป็นแบบ multiclass problem นั่นคือมีแค่คำตอบเดียวที่ถูกต้องจากทั้งหมด แต่... ถ้าปัญหาเราเป็น multilabel multiclass หรือจากคำตอบทั้งหมดสามารถตอบได้มากกว่า 1 คำตอบ node ที่เป็น 1 ก็จะมีมากกว่า 1 node\n",
    "\n",
    "โอเค เรามาลองทำการ one hot encoder กันดีกว่า"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "มาดูข้อมูลคำตอบใน training set กัน :(1000, 10)\n",
      "คำตอบของตัวอย่างแรกคือ [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] นั่นคือเลข 5 นั่นเอง\n"
     ]
    }
   ],
   "source": [
    "y_train_one_hot = np.zeros(shape=(y_train.shape[0], 10))\n",
    "\n",
    "for i in range(y_train_one_hot.shape[0]):\n",
    "    y_train_one_hot[i][y_train[i]] = 1\n",
    "\n",
    "y_train = y_train_one_hot[11000:12000]\n",
    "print(f\"มาดูข้อมูลคำตอบใน training set กัน :{y_train.shape}\")\n",
    "print(f\"คำตอบของตัวอย่างแรกคือ {y_train[0]} นั่นคือเลข 5 นั่นเอง\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "แรกเริ่มเราทำการสร้างโครงสร้างของ vectors ราออกมาก่อน(metric เมื่อนำเอา vectors ทั้งหมดมารวมกันเป็นคลังข้อมูลเกี่ยวกับคำตอบ)นั้นคือกำหนดให้แต่ละ vector ที่เป็นตัวแทนของคำตอบในแต่ละตัวอย่างให้ทุก nodes เป็น 0 หมด หลังจากนั้นเราจะไล่แทนค่า 1 คำไปยัง node ที่เป็นตัวแทนคำจอบของแต่ละตัวอย่าง (ในกรณีนี้ vector นึงจะมี node เดียวที่เป็น 0 เนื่องจากคำตอบที่ถูกต้องมีเพียงคำตอบเดียว)\n",
    "เรามาทำแบบเดียวกันกับคำตอบ testing set กันดูบ้าง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "มาดูข้อมูลคำตอบใน testing set กัน :(10000, 10)\n",
      "คำตอบของตัวอย่างแรกคือ [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_test_one_hot = np.zeros(shape=(y_test.shape[0], 10))\n",
    "\n",
    "for i in range(y_test_one_hot.shape[0]):\n",
    "    y_test_one_hot[i][y_test[i]] = 1\n",
    "\n",
    "y_test = y_test_one_hot\n",
    "print(f\"มาดูข้อมูลคำตอบใน testing set กัน :{y_test.shape}\")\n",
    "print(f\"คำตอบของตัวอย่างแรกคือ {y_test[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จะเห็นได้ว่าเราเปลี่ยนคำตอบที่แสดงในรูปแบบ numeric ให้กลายเป็น one hot vector ได้สำเร็จ\n",
    "## ก่อนมาเริ่มทำการสร้าง Neural Network program มารู้จัก Activation function กันก่อน\n",
    "ก่อนอื่นในแต่ละ layer (ที่ไม่ใช่ output layer) เรายังไม่มีการใช้ activation function ในรอบนี้เราจะมาใช้กัน โดย function ที่เราจะใช้ก็คือ relu นั่นเอง ฟังก์ชันมีความง่ายมากนั่นคือ ถ้าค่ามากกว่า 0 ให้ทำการคงคำตอบค่าเดิมเอาไว้ แต่ถ้าค่าน้อยกว่าหรือเท่ากับ 0 ให้ทำการแปลงคำตอบให้กลายเป็น 0 \n",
    "> Warning: ทำไมต้อง activation function หรือให้เฉพาะเจาะจงทำไมต้อง non linear activation function เนื่องจากถ้าเราไม่มี function นี้ ตัวโปรแกรมเราถึงแม้จะเป็น neural network ที่มี 10 layers ก็ไม่ช่วยอะไรเนื่องจาก neural network แค่ 2 layers ก็สามารถให้คำตอบเดียวกับ neural network ที่มี 10 layers ลองคิดในแง่ของ correlation ว่าถ้า weights คือ correlation ระหว่าง layers โดยจุดประสงค์หลักของ program ในการเรียนรู้คือการหา correlation ระหว่าง inputs และ outputs ถ้าเราไม่มี non linear activation function จะทำให้ correlation ในแต่ละช่วงของตัวโปรแกรม (ในกรณีนี้เรามี hidden layer) สุดท้ายก็ขึ้นอยู่กับ  correlation ที่มีกับ inputs layer (นี่แหละ! ที่ทำให้ถึงแม้เรามี layer เยอะแค่ไหนก็ตาม คำตอบสุดท้ายก็สามารถผลิตได้ด้วย program ที่มีแค่ input และ output layer) แต่ถ้าเรามีการใช้ activation function นั้นเท่ากับว่าในแต่ละ layer มีสิทธิ์ที่จะสร้าง correlation ของตัวเองขึ้นมา โดยไม่ได้ขึ้นอยู่กับแค่ฝั่งของ input layer ทำให้เราสามารถสร้าง correlation ที่ซับซ้อน และสุดท้ายก็สร้าง indirect correlation ได้ โดย correlation จาก input layer เป็นแค่ส่วนหนึ่งของ correlation ทั้งหมด ที่ตัวโปรแกรมพยายามสร้างขึ้นมา (ไม่เหมือนกับการใช้ correlation โดยตรงระหว่าง input layer และ output layer) \n",
    "\n",
    "\n",
    "เหตุผลว่าทำไมต้อง activation function ผมขอลองใช้การเปรียบเทียบดูนะครับ จินตนาการดูครับว่า เรามีสร้อยคออันนึงเล็กๆแต่อยากกจะเซอไพส์แฟน แต่ในตอนนี้มีแค่กระดาษ A4 จำนวนนึง (หากล้องของขวัญไม่ทันแล้ว) เราก็เลยคิดว่างั้นเอาเลยละกัน เราเอา A4 ห่อสร้อยเอาไว้หลายๆชั้นและมีการขยัมให้เป็นก้อนกลมๆ พอเราส่งให้แฟน แฟนก็ตอบว่า \"มันคืออะไรเนี่ย\" แฟนมี 2 ตัวเลือกคือ \n",
    "\n",
    "- คลี่\n",
    "- ดีด\n",
    "\n",
    "โดยการคลี่หนึ่ง ครั้งเรามองเป็น 1 step และการดีดหนึ่งครั้งก็มองเป็น 1 step ดุแล้วปัญหานี้ก็เป็นปัญหาที่ซับซ้อนพอควรเพราะเราต้องคลี่ด้วยมุม, แรงและทิศทางที่ไม่มีกำหดตายตัวจนกระทั้งเจอสสร้อย ทุกครั้งที่เราคลี่เราเปลี่ยนแปลง representation ทุกครั้ง เราคลี่ไปเรื่อยๆ แล้วหยุดคิดแปปนึงว่าจาก representation ปัจจุบันของก้อนกระดาษที่ห่อสร้อยกับก้อนกระดาษที่ผ่านการคลี่แค่ครั้งเดียวก่อนหน้านี้ จะเห็นว่าเรามาไกลแค่ไหนและการคลี่ของเราเมื่อมองมันเป็น list ของ actions มีความซับซ้อนเมื่อเทียบกับตอนเราพึ่งจะคลี่ไปรอบเดียว แต่เราไม่รู้สึกถึงความซับซ้อนเพราะแต่ละ step เราทำแค่ขั้นตอนง่ายๆคือการคลี่ แต่ถ้าเรายังไม่เริ่มคลี่แล้วต้องการทำการคลี่ทีเดียวเพื่อให้ได้ผลลัพธ์เหมือนกับที่เราคลี่ไปแล้ว 25 ครั้ง มันจะเป็นคู่มือขั้นตอนที่ซับซ้อนมากในการทำทีเดียว จะเห็นว่าแต่ละ step ก็เหมือนแต่ละ layer ของ neural network program และการคลี่ก็คือ non linear actibation function ซึ่ง function นี้ผลลัพธ์คือการสะสมความซับซ้อนขึ้นทีละนิด\n",
    "\n",
    "แต่ถ้าเราใช้ linear activation function หรือไม่ใช้เลยละ มันก็จะเปรียบได้กับการดีดก้อนกระดาษที่ห่อสร้อย representation เราเปลี่ยนนั่นคือก้อนมีการขยับและหยุดโดยทำมุมต่างจากตอนแรก แต่การดีด 25 ครั้งก็ไม่ทำให้เราใกล้กับคำตอบรวมถึง ผลลัพธ์จากการดีด 25 ครั้ง บางทีเราอาจจะสามารถทำได้ด้วยการดีดเพียงครั้งเดียว และที่โชคร้ายคือการดีดไม่ซับซ้อนพอที่จะพาเราไปสู่คำตอบได้ เปรียบได้กับ neural network ที่มีหลาย layer ก็จริงแต่สุดท้ายผลผลิตก็สามารถถูกผลิตได้ด้วย program ที่มี layer เดียว นอกจากนี้ยังไม่มีความซับซ้อนพอที่จะผลิตคำตอบที่เหมาะสมอีกด้วย\n",
    "\n",
    "## คุณสมบัติของ Activation function\n",
    "ผมขอแบ่งคุณสมบัติที่ควรมีของ activation function ออกมาเป็น 4 คุณสมบัติ\n",
    "### 1. Activation functions ต้องเป็น Continuous function\n",
    "นั่นคือทุก input (infinite domain) ของเราที่ใส่เข้าไปใน function จะต้องมีคำตอบออกมาทั้งหมด เราไม่ควรที่จะใส่ input เข้าไปแล้วไม่มี output ออกมา\n",
    "### 2. Activation functions ที่ดี จะต้องไม่เปลี่ยนแปลงทิศทาง\n",
    "มันหมายความว่ายังไงกันนะ ลองคิดถึงพวกกราฟยกกำลัง เช่น ยกกำลัง 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa2c0a016d0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "x = np.arange(-5,5,0.1)\n",
    "y = x ** 2\n",
    "let_show.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จะเห็นว่าคำตอบที่ได้มีการเปลี่ยนทิศทางจากลบเป็นบวก ถ้าเราเพิ่มค่าแนวแกน x เพิ่มขึ้นเรื่อยๆ แบบนี้แสดงว่าถ้า x เราคือ weight และ y คือคำตอบ คำตอบที่ถูกต้องสามารถหาได้จากการปรับ x ไปด้านซ้ายหรือขวา ตอนนี้แหละเราจะเกิด 2 มุมมอง\n",
    "\n",
    "- มุมมองแรก เอ้ยมันดีนะเรามีโอกาสที่จะหาคำตอบที่ถูกต้องเจอมากขึ้น เจ๋งเลย\n",
    "- มุมมองที่สอง แย่แล้วคำตอบที่ถูกต้องสามารถหาได้จากการเลื่อน x ไปทางซ้ายหรือ ขวา แล้วเราควรจะเลื่อนไปด้านไหนดี?\n",
    "\n",
    "ซึ่งพอมามองในมุมมองของ neural network program แล้วหรือเฉพาะเจาจง optmization alogorithm มุมมองที่สองมีความสำคัญและส่งผลต่อการเรียนรู้มากกว่า ดังนั้นเราควรจะเลือก functions ที่ไม่เปลี่ยนทิศทาง\n",
    "### 3. Activation functions เป็น Non Linear functions\n",
    "ตามเหตุผลที่อธิบายไปก่อนหน้าที่ถ้าเป็น non linear functions ยิ่ง steps ในโปรแกรมเยอะก็หมายถึง program มีความซับซ้อนมากขึ้น และสามารถแก้ไขปัญหาที่ซับซ้อนได้ ถ้ามองในมุม correlation คือ hidden layer สามารถสร้างปรากฏการณ์ selective correlation ได้ ส่งผลให้แต่ละ layer สร้าง correlation ของตัวเองได้ (ทำให้แต่ละ layer เปรียบเสมือน representation ใหม่ของข้อมูลของเรา) และสุดท้ายส่งผลให้ output layer ไม่ได้ขึ้นอยู่กับแค่ correlation จาก input layer เพียงอย่างเดียว แต่เป็น correlation โดยรวมจาก layers ก่อนหน้าทั้งหมด ซึ่งมีความซับซ้อน\n",
    "### 4. Activation functions ต้องสามารถคำนวณได้ในเชิงใช้งานจริง รวมถึง derivative ของตัว function เองด้วย\n",
    "ง่ายๆก็คือสามารถคำนวณได้ และต้องเร็ว เนื่องจาก activation functions จะต้องถูกเรียกบ่อยมากๆ\n",
    "## เรามาดู Activation functions ที่ถูกใช้ใน Hidden layer กันดีกว่า\n",
    "functions ที่ใช้กันผมจะขอยกตัวอย่างมา 3 functions\n",
    "### 1. Sigmoid function\n",
    "จะทำการให้คำตอบระหว่าง [0, 1] ซึ่งเหมาะมากในการใช้อธิบายความน่าจะเป็นของ node โดย 1 ก็คือมีโอกาส 100% นั่นทำให้ functions นี้ถูกนำไปใช้ทั้งใน hidden และ output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    output = 1 / (1 + np.exp(-x))\n",
    "    return output\n",
    "\n",
    "def derivative_of_sigmoid(output):\n",
    "    derivative_output = output * (1 - output)\n",
    "    return derivative_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. tanh function\n",
    "ในส่วนของ tanh จะให้คำตอบระหว่าง [-1, 1] เนื่องจากมีโอกาสให้คำตอบเป็นจำนวนติดลบด้วย ทำให้ในหลายปัญหามีประโยชน์กว่า sigmoid การทำ weight sum คือการดูว่า inputs ของเราเป็นไปในทิศทางเดียวกับ correlation ที่มีสำหรับ node ที่สนใจไหม (การทำ weight sum มองเป็นการหา similarity หรือแนวโน้มที่ไปทางเดียวกันกับความสัมพันธ์) ถ้าคำตอบเป็นลบแสดงว่ายังมีความสัมพันธ์กันอยู่ระหว่าง input กับ output node แต่เป็นความสัมพันธ์ที่ input มีความตรงกันข้ามกับ output ซึ่ง tanh สามารถอธิบายตรงนี้ได้ แต่ sigmoid จะบอกคำตอบว่าเป็น 0 ซึ่งบอกว่าค่า input ที่เข้ามาปัจจุบันไม่มีความสัมพันธ์กับ output เพราะ sigmoid function ทำให้ node บอกได้แค่ว่ามีความสัมพันธ์ไหม หรือไม่มี"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    output = np.tanh(x)\n",
    "    return output\n",
    "\n",
    "def derivative_of_tanh(output):\n",
    "    derivative_output = 1 - (output ** 2)\n",
    "    return derivative_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.relu function\n",
    "จะให้ผลลัพธ์เป็นค่าเดิมเมื่อค่ามากกว่า 0 แต่ถ้าค่าน้อยกว่าหรือเท่ากับ 0 จะให้ค่าคำตอบที่เป็น 0 เท่านั้น ผลกระทบจะเหมือน sigmoid เลยนั่นคือจะบอกได้แค่จาก input ที่ได้มามีความสัมพันธ์ไหม หรือไม่มี แต่ที่ต่างคือตัวสมการ sigmoid จะให้ค่า derivative ที่สูงเมื่อ input เข้าใกล้ 0 แต่เมื่อคำตอบที่ได้จาก sigmoid เข้าใกล้ 1 หรือ 0 จะทำให้ค่า derivative เข้าใกล้ 0 ซึ่งส่งผลให้เกิดความ Stickness ซึ่งส่งผลดีในกรณีที่มีมี noise เข้ามาในช่วงที่ weights เราจุดที่มีความมั่นใจในการสร้างคำตอบแล้ว เช่น คำตอบเข้าใกล้ 0 หรือ 1 การที่มี noise ทำให้เกิด error ก็จะไม่รบกวนคำตอบที่ program สร้างขึ้นมา แต่ แต่... ก็มีผลเสียเหมือนกันคือเมื่อ program เรียนรู้แล้วยังไม่เข้าสู่จุดที่เข้าใจบทเรียน แต่ node ของเราบาง node ไปถึงจุด stickness เรียบร้อยแล้ว (ทั้งที่จริงยังต้องปรับขึ้นหรือลงอีก) ส่งผลทำให้เกิดความยากในการเรียนรู้ อาจถึงขั้นต้อง reset เพื่อเรียนรู้ใหม่ \n",
    "ดังนั้น relu จะไม่มีในเรื่องของ stickness เนื่องจากเมื่อค่ามากกว่า 0 ค่า derivative จะมีค่าเป็น 1 และเมื่อน้อยกว่า 0 ค่า derivative จะมีค่าเป็น 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    output = (x > 0) * x\n",
    "    return output\n",
    "\n",
    "def derivative_of_relu(output):\n",
    "    derivative_output = output > 0\n",
    "    return derivative_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ความจริงในส่วนของ derivative function ค่า input จะต้องเป็น x ไมา่ใช่ output แต่เพื่อความง่ายในการคำนวณเมื่อสร้าง neural network program เราจะใช้แบบนี้ในการคำนวณ (ไม่ส่งผลต่อคำตอบ เพราะจำนวนติดลบก็ถูกแปลงเป็น 0 และ ค่า 0 ใส่คำไปใน derivative function ของ relu ก็ให้คำตอบเป็น 0 อยู่ดี)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## เรามาดู Activation functions ที่ถูกใช้ใน Hidden layer กันดีกว่า\n",
    "จะขอแบ่งการใช้ activation จากปัญหาออกเป็น 3 รูปแบบนะครับ\n",
    "### 1. ต้องการคำตอบที่เป็น Numeric\n",
    "ในที่นี้เราสามารถปล่อยให้ node ไม่ต้องมี activation functions ได้เลย หรือถ้าเราต้องการจำกัดขอบเขตของคำตอบก็ทำการใช้ sigmoid หรือ tanh \n",
    "ได้\n",
    "### 2. มีคำตอบหลายคำตอบและมีคำตอบที่ถูกต้องมากกว่า 1 คำตอบ\n",
    "ในกรณีนี้ที่มีหลายคำตอบแสดงว่าเราแทนที่คำตอบด้วย vector โดยแต่ละ element ใน vector จะเป็นตัวแทนของคำตอบนั้น และเนื่องจากมีคำตอบที่ถูกต้องมากกว่า 1 คำตอบ เราเลยให้แต่ละ node มีความน่าจะเป็นของตัวเอง ถ้า node ไหนมีความน่าจะเป็นมากกว่า threshold ที่กำหนดก็จะเลือก node นั้นเป็นคำตอบ หมายความว่าแต่ละ node เราจะใช้ sigmoid เป็น activation functionถ้า\n",
    "### 3. มีคำตอบหลายคำตอบและมีคำตอบที่ถูกต้องเพียงคำตอบเดียว\n",
    "ถ้าเราใช้ sigmoid แล้วทำการสร้าง neural network program ให้เกิดการเรียนรู้ จะเหมือนเป็นการบอก program ว่า \"นี่ๆเจ้า program ใน out layer แต่ละ node ช่วยทำให้ค่าอื่นๆที่ไม่ใช่คำตอบที่ถูกต้องเป็น 0 และให้ node ที่เป็นคำตอบี่ถูกต้องเป็น 1\" สมมุติเรามี 3 คำตอบ ได้แก่ [0.2, 0.3, 1]  และคำตอบที่ถูกต้องคือคำตอบที่สาม จะเห็นได้ว่า program เราตอบถุกและมั่นใจมากๆ แต่การเรียนรู้ยังไม่หยุดเนื่องจาก node ที่เหลือยังไม่เป็น 0 ใช่แล้วเป้าหมายของ sigmoid ไม่ใช่แค่ขอคำจอบที่ถูกต้องเป็น 1 แต่คำตอบที่ผิดต้องเป็น 0 ด้วย!\n",
    "\n",
    "ดังนั้นจากตรงนี้เราเลยต้องมีการปรับปรุงให้มีประสิทธิภาพมากขึ้นนั่นคือเราโฟกัสไปแค่ node ที่เป็นคำตอบที่ถูกแล้วพยายามปรับความน่าจะเป็นให้มากขึ้นเรื่อยๆ ส่วน nodes ที่เหลือจะมีการลดลงของความน่าจะเป็นแบบอัตโนมัติ เอ๊ะทำไมกันนะ? เพราะว่าทุก nodes ใน output vector ทำการแชร์ความน่าจะเป็นกัน (หรือมีความน่าจะเป็นแค่อันเดียว ต่างจาก sigmoid ที่ความน่าจะเป็นจะแยกไปเป็นของแต่ละ node) ทำให้การใช้ sigmoid เป็นการบอกว่า \"เฮ้ คำตอบไหนเป็นคำตอบที่ถูกต้องที่สุด ก้ปรับให้คำตอบนั้นค่าความน่าจะเป็นออกมามากที่สุด node อื่นที่ไม่ใช่คำตอบก็จะมีความน่าจะเป็นที่น้อยลงเองอัตโนมัติ\"\n",
    "\n",
    "แล้วคำนวณยังไงนะ sigmoid ให้มองเป็น 3 ขั้นตอนง่าย\n",
    "1. ทำการ weight sum ระหว่าง input vector และ weight vector (ปกติที่เราทำกันมา)\n",
    "2. ทำการ คำนวณ exponential function โดยที่เลขฐานเป็น e\n",
    "3. นำค่าแต่ละ node มาตั้งและหารด้วย ค่าของ node ทั้งหมดรวมกัน (หาความน่าจะเป็นของแต่ละ node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    expoential_output = np.exp(x)\n",
    "    output = expoential_output / np.sum(expoential_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## มาเริ่มสร้าง Neural Network program สำหรับ Dataset กันดีกว่า"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, iterations, hidden_layer1 = 0.01, 500, 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เริ่มแรกมากำหนด learning rate กันก่อนให้เท่ากับ 0.003 (ค่ามากน้อยขึ้นอยู่กับข้อมูลและโครงสร้าง program เรายิ่งค่ามากยิ่งมีการปับ weight มากในครั้งเดียว และยิ่งค่าน้อยก็ยิ่งปรับ weight น้อย มีผลกับเวลาในการเรียนรู้) ต่อมาคือ iteration (epochs) ใช้บอกว่าเราจะให้โปรแกรมเห็น dataset ทุกตัวอย่างทั้งหมดกี่ครั้ง และสุดท้ายจำนวน node ใน hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_0_1 = 0.2 * np.random.random(size=(x_train.shape[1], hidden_layer1)) - 0.1\n",
    "weight_1_2 = 0.2 * np.random.random(size=(hidden_layer1, 10)) - 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- weight_0_1 คือ weight ระหว่าง layer 0 (input layer) กับ  layer 1 (hidden layer 1)\n",
    "- weight_1_2 คือ weight ระหว่าง layer 1 (hidden layer 1) กับ  layer 2 (output layer)\n",
    "\n",
    "โดยเราทำการเลื่อนค่าให้มีค่า mean อยู่ที่ - 0.1 และ มีการกะจายของข้อมูลเป็น 0.2 (scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.002805354217390215\n",
      "Accuracy: 0.998\n"
     ]
    }
   ],
   "source": [
    "for j in range(iterations):\n",
    "    error, correct_count = 0, 0\n",
    "    \n",
    "    for i in range(x_train.shape[0]):\n",
    "        layer_0 = x_train[i:i+1]\n",
    "        layer_1 = relu(layer_0.dot(weight_0_1))\n",
    "        layer_2 = layer_1.dot(weight_1_2)\n",
    "        \n",
    "        error += np.sum((layer_2 - y_train[i:i+1]) ** 2)\n",
    "        correct_count += int(np.argmax(layer_2) == np.argmax(y_train[i:i+1]))\n",
    "        \n",
    "        layer_2_delta = layer_2 - y_train[i:i+1]\n",
    "        layer_1_delta = derivative_of_relu(layer_1) * (layer_2_delta.dot(weight_1_2.T))\n",
    "        \n",
    "        weight_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n",
    "        weight_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
    "    \n",
    "print(f'Error: {error / x_train.shape[0]}')\n",
    "print(f'Accuracy: {correct_count / x_train.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สังเกตว่าเรามีการปรับ weights ในทุกๆตัวอย่างของ training set (1,000 ตัวอย่าง) ดังนั้นเราจะมีการปรับ weights ทั้งหมด 1,000 x 350 (iteration)  เป็น 350,000 ครั้ง! แต่ว่าการทำแบบนี้การปรับ weights แต่ละครั้งด้วยตัวอย่างเดียวอาจทำให้บางครั้งของการปรับ weights เกิด noise ได้ทำให้ weights เดินผิดเพี้ยนไป แต่โดยรวม weights แต่ละค่าก็พยายามมุ่งเข้าหาค่าของ weights ที่ควรจะเป็นถ้าเราลองวาดเป็นกราฟการเดินทางก็จะมี noise เกิดขึ้นบ้างแต่ trend โดยรวมก็ยังเหมือนเดิม จริงๆการปรับ weights สามารถทำได้ 3 แบบ\n",
    "1. stochastic gradient descent \n",
    "2. full gradient descent \n",
    "3. batch gradient descent \n",
    "\n",
    "โดยเกิดจากว่าแรกเริ่มใช้ full gradient descent แต่เมื่อคลังข้อมูลเรามีขนาดใหญ่มากขึ้นเราไม่สามารถใส่ข้อมูลทั้งหมดใน memory รวมถึงการอัพเดท weights ครั้งหนึ่งใช้เวลานานมาก เช่นเมื่อต้องสร้าง program จาก ImageNet dataset ซึ่งมีภาพหลักล้านรูป ถ้าเรารอให้ครบทั้ง dataset แบะอัพเดททีหนึ่งเราจะใช้เวลาในการ training นานมากๆ ดังนั้นจึงเกิดแนวคิดว่าแล้วทำไมเราไม่ใส่ตัวอย่างหนึ่งและปรับ weights หนึ่งที ซึ่งจะทำให้เรามีการปรับ weights ได้หลายครั้งมากต่อการใช้ dataset ทั้งหมด 1 รอบ แต่อย่างที่บอกคือ เราอัพเดทได้เร็วก็จริงแต่การอัพเดทแต่ละครั้งมีโอกาสเป็น noise ดังนั้นเราเลยอยู่กึ่งกลางละกันโดยการใช้ batch gradient descent นั่นคือเรามีโอกาสอัพเดทมากกว่าการใช้ full gradient descent  และการอัพเดทแต่ละครั้งก็มีโอกาสเกิด noise น้อยกว่าและมีโอกาสมุ่งเข้าสู่ค่า weights ที่เป็นคำตอบมากกว่า เนื่องจากมีการเฉลี่ยกันของหลายๆตัวอย่างถึงแม้จะไม่ได้แนวโน้มการอัพเดทที่ดีเท่า full dataset แต่ก็เป็นแนวโน้มการอัพเดทที่ดีกว่าการอัพเดทจากทีละหนึ่งตัวอย่าง (หมายความว่าอาศัยจำนวนการอัพเดทน้อยกว่าแบบ stochastic gradient descent อีกด้วย) ยิ่งไปกว่านั้นการทำแบบ batch ยังเป็นการใช้ optimization library สำหรับ linear algebra อย่างคุ้มค่าเมื่อเราทำการอัพเดท weights ทั้งหมด 10 iterations แบบ batch จะมีความเร็วกว่าแบบ stochastic gradient descent อีกด้วย\n",
    "\n",
    "จะเห็นว่าผลลัพธ์ออกมาเป็น accuracy 99.9% Wow! แต่ แต่... อย่าลืมว่าอันนี้เราทพสอบกับคลังข้อมูลที่ program เราเรียนรู้โดยตรง เดี๋ยวเราลองมาทดสอบกับคลังข้อมูลที่ program ยังไม่เคยเห็นมาก่อน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.40306746534485255\n",
      "Accuracy: 0.8093\n"
     ]
    }
   ],
   "source": [
    "error, correct_count = 0.0, 0\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    \n",
    "    layer_0 = x_test[i:i+1]\n",
    "    layer_1 = relu(layer_0.dot(weight_0_1))\n",
    "    layer_2 = layer_1.dot(weight_1_2)\n",
    "\n",
    "    error += np.sum((layer_2 - y_test[i:i+1]) ** 2)\n",
    "    correct_count += int(np.argmax(layer_2) == np.argmax(y_test[i:i+1]))\n",
    "    \n",
    "print(f'Error: {error / float(x_test.shape[0])}')\n",
    "print(f'Accuracy: {correct_count / x_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จะเห็นว่าเมื่อเราทดสอบกับ training set เราได้ 99.9% แต่พอมาทดสอบกับ testing set กับได้ 82.71% ซึ่งต่างกันพอสมควร งั้นแปลว่า program เรายังเอามาใช้กับข้อมูลที่ไม่เคยเห็นได้ดีพอทำไมกันนะ?\n",
    "\n",
    "มีหลากหลาย weights configuration ที่ทำให้ค่า error ต่ำและ program ออกมาดีเมื่อทดสอบกับ training set แต่เป้าหมายเราคือการหา weights ที่สามารถทำให้ทดสอบกับข้อมูลที่ไม่เคยเห็นและได้คำตอบที่ถุกต้องนั่นคือเมื่อทดสอบกับ testing set จะต้องได้ผลััพธืออกมาดี (Generalization) weights ที่เฉพาะเจาะจงกับ training set (ทำให้ค่าที่ทดสอบออกมาดี) เราเรียกว่า overfitting (ทำการจำข้อมูลในคลังข้อมูลแล้วมาตอบคำถามเวลาสอบ) ดังนั้นเราต้องทำยังไงที่จะทำให้เราหา weights ที่ generalization (สร้างองค์ความรู้ออกมาเพื่อใช้ผลิตคำตอบ ต่างจากการจำ) ได้กับข้อมูลที่ไม่ได้อยู่ใน training set ซึ่งบ่อยครั้งที่เมื่อเราลด generalization error จะมีราคาที่ต้องแรกคือ training error จะเพิ่มขึ้น แต่ก็เป็นสิ่งที่รับได้เพราะสุดท้าย training error ที่น้อยมากๆ ก็ไม่สามารถนำไปใช้ประโยชน์จริงได้ แต่กลับ generalization error ที่น้อยเราสามารถนำมันไปใช้ประโยชน์ได้และมั่นใจได้ว่า program เราจะสามารถทำงานกับข้อมูลที่มันไม่เคยเห็นได้ แล้วเราจทำยังไงดี\n",
    " # ถึงเวลาการทำ regularization\n",
    " การทำ regularization มีหลายแบบตั้งแต่การปรับสมการคำนวณค่า error หรือการเปลี่ยนแปลงโครงสร้างของตัว program (ตัวแปร interact กันยังไง) ขอยกตัวอย่างคร่าวๆโดยการปรับปรุง error โดยเป็นการควบคุม capacity ของ program ในการเรียนรู้ ถ้า capacity สูงเกินไปและปัยหาไม่ซับซ้อนตัว program ก็จะเลือกทางเลือกที่ง่ายที่สุดนั่นคือการจดจำตำตอบ ดังนั้นเราจึงต้องจำกัด capacity ให้เหมาะสมที่จะส่งเสริมให้ program เกิดการเรียนรู้และสร้างองค์ความรู้ขึ้นมาจริงๆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "error = (output - true_output) ** 2\n",
    "\n",
    "เราทำการเพิ่มไปอีกสมการหนึ่ง\n",
    "\n",
    "error = (output - true_output) ** 2 + sum(weight ** 2)\n",
    "\n",
    "จะเห็นว่าเป็นการเอา weights มารวมใน error ด้วย เพื่อเป็นการบอกว่าถ้า weights ไหนไม่ได้มีผลกลับคำตอบจริงๆให้ค่ามุ่งหน้าเข้าสู่ 0 รวมถึง weights ที่มีผลคล้ายๆกันกับคำตอบให้ทำการแชร์ค่ากัน เช่น weight_1 แทนที่จะเป็น 1 ก็ให้เหลือ weight_1 กับ weight_2 เป็น 0.5 แทน การทำแบบนี้ weight แต่ละตัวแปรจะไม่สามารถมีค่าที่มากเกินไปได้ weights แต่ละอันต้องมีการแชร์ และ weights ที่เป็น noise ก็จะต้องถูกปิด (ค่าเข้าใกล้ 0)\n",
    "\n",
    "แต่สำหรับครั้งนี้ผมจะมาเน้นที่การทำ regularization โดยการปรับตัว program (architecture) โดยการใช้สิ่งที่เรียกว่า dropout\n",
    "# ยินดีต้อนรับ Dropout\n",
    "การใช้ dropout คือการ random เพื่อทำการ shut down node ที่เลือกลงไป (แทนค่าเป็น 0) ทำไมถึงใช้งานได้ละ?\n",
    "##  การสร้าง neural network program มาเยอะๆหลาย program แล้วทำการเฉลี่ยคำตอบ (Ensembling)\n",
    "เมื่อเราทำการสร้างมาหลายๆ program แน่นอนว่าแต่ละ program ก็จะมีการเรียกรู้ที่ต่างกัน และผิดพลาดต่างกัน เรา overfitting ก็มีการ overfitting ที่ต่างกัน เมื่อ program เรียนรู้จะเรียนรู้สิ่งที่เป็น concept หลักก่อน จากนั้นถ้ายังเหลือ capacity ก็จะเริ่มเรียนรู้รายละเอียดของตัวอย่างนั้นๆ (fine grained detail)เมื่อเรานำทั้งหมดมาเฉลี่ยกันก็จะมีการ cancel ความผิดพลาด ออกไปเหลือแค่ concept หลักที่มีการเรียนรู้ร่วมกัน\n",
    "\n",
    "แต่การสร้างหลายๆ programs และให้มันเรียนรู้ มันใช้เวลายเอะมากเลยนะ ดังนั้น dropout จึงเข้ามานั่นคือเราสร้าง program แค่อันเดียวแล้วทุกครั้งที่มีการใส่ inputs เข้าไปในโปรแกรมเราจะทำการสุ่มปิด nodes ใน program เท่ากับว่าเรามีการสร้าง sub program (sub neural network) ขึ้นมาและที่สำคัญคือ sub program มีจำนวน weights น้อยลงทำให้ลดการเกิด overfitting และถ้าเรามองการทำ dropout คือการสร้าง sub program แสดงว่า sub program หนึ่ง เรียนรู้ batch ของ training set และอีก sub program หนึ่งเรียนรู้อีก batch ของ training set เปรียบเสมือน เราสร้าง 2 program ที่เรียนรู้ข้อมูลคนละส่วน เราเรียกว่า การทำ begging\n",
    "\n",
    "แต่ dropout ให้เรามากว่านั้นเนื่องจาก sub program ถูกสร้างออกมาจาก program ใหญ่แสดงว่า weights ต้องมีการแชร์กันระหว่าง sub program จำได้ไหม การทำ regularization โดยการปรับสมการ error คือการจำกัดทำให้ค่า weights มุ่งหน้าเข้าสู่ 0 แต่! การทำให้ sub program มีการแชร์ weights กันทำให้ค่า weights ถูกจำกัดและปรับมุ่งหน้าเข้าสู่ค่าที่มันควรจะเป็น (ไม่ใช่ 0)จาก sub program อื่นๆ นี่ยิ่งทำให้ dropout ทรงพลังขึ้นไปอีก \n",
    "แล้วสุดท้ายเราจะมาเฉลี่ยกันยังไง? ถ้า program มี hidden layer แค่ 1 layer การสุ่มเอา sub program มา (จำนวนมากหรือทั้งหมดถ้า node มีไม่มาก) แล้วทำการเฉลี่ยคำตอบก่อนเข้า activation function ใน output nodes จะมีค่าเท่ากับการปิดการทำงานของ dropout หรือการใช้ full program ในการผลิตคำตอบ แล้วนำคำตอบที่ได้มาหารด้วย ratio ของ nodes ที่ปิดไปใน hidden layer\n",
    "\n",
    "แต่ถ้าเรามี hidden layer มากกว่า 2 การทำแบบเดิมจะไม่ให้ค่าที่ทำกันแล้วแต่ก็ยังสามารถเป็นค่าประมาณที่ใช้จริงได้อยู่\n",
    "# พอแล้ว Theory มาลงมือจริงกันเลยดีกว่า"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, iterations, hidden_layer1 = 0.01, 500, 32\n",
    "weight_0_1 = 0.2 * np.random.random(size=(x_train.shape[1], hidden_layer1)) - 0.1\n",
    "weight_1_2 = 0.2 * np.random.random(size=(hidden_layer1, 10)) - 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เหมือนเดิม เริ่มแรกมากำหนด learning rate กันก่อนให้เท่ากับ 0.003 (ค่ามากน้อยขึ้นอยู่กับข้อมูลและโครงสร้าง program เรายิ่งค่ามากยิ่งมีการปับ weight มากในครั้งเดียว และยิ่งค่าน้อยก็ยิ่งปรับ weight น้อย มีผลกับเวลาในการเรียนรู้) ต่อมาคือ iteration (epochs) ใช้บอกว่าเราจะให้โปรแกรมเห็น dataset ทุกตัวอย่างทั้งหมดกี่ครั้ง และสุดท้ายจำนวน node ใน hidden layer \n",
    "\n",
    "- weight_0_1 คือ weight ระหว่าง layer 0 (input layer) กับ  layer 1 (hidden layer 1)\n",
    "- weight_1_2 คือ weight ระหว่าง layer 1 (hidden layer 1) กับ  layer 2 (output layer)\n",
    "\n",
    "โดยเราทำการเลื่อนค่าให้มีค่า mean อยู่ที่ - 0.1 และ มีการกะจายของข้อมูลเป็น 0.2 (scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.2771398481038192\n",
      "Accuracy: 0.869\n",
      "Test Error: 0.3636057630375286\n",
      "Test Accuracy: 0.8138\n"
     ]
    }
   ],
   "source": [
    "for j in range(iterations):\n",
    "    error, correct_count = 0, 0\n",
    "    \n",
    "    for i in range(x_train.shape[0]):\n",
    "        layer_0 = x_train[i:i+1]\n",
    "        layer_1 = relu(layer_0.dot(weight_0_1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * (1/0.5)\n",
    "        layer_2 = layer_1.dot(weight_1_2)\n",
    "        \n",
    "        error += np.sum((layer_2 - y_train[i:i+1]) ** 2)\n",
    "        correct_count += int(np.argmax(layer_2) == np.argmax(y_train[i:i+1]))\n",
    "        \n",
    "        layer_2_delta = layer_2 - y_train[i:i+1]\n",
    "        layer_1_delta = derivative_of_relu(layer_1) * (layer_2_delta.dot(weight_1_2.T))\n",
    "        layer_1_delta = layer_1_delta * dropout_mask\n",
    "        \n",
    "        weight_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n",
    "        weight_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
    "    \n",
    "test_error, test_correct_count = 0.0, 0\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "\n",
    "    layer_0 = x_test[i:i+1]\n",
    "    layer_1 = relu(layer_0.dot(weight_0_1))\n",
    "    layer_2 = layer_1.dot(weight_1_2)\n",
    "\n",
    "    test_error += np.sum((layer_2 - y_test[i:i+1]) ** 2)\n",
    "    test_correct_count += int(np.argmax(layer_2) == np.argmax(y_test[i:i+1]))\n",
    "\n",
    "\n",
    "print(f'Error: {error / x_train.shape[0]}')\n",
    "print(f'Accuracy: {correct_count / x_train.shape[0]}')\n",
    "print(f'Test Error: {test_error / float(x_test.shape[0])}')\n",
    "print(f'Test Accuracy: {test_correct_count / x_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จะเห็นได้ว่า dropout ทำให้ผลการทดสอบกับ testing set ดีขึ้น หมายความว่าทำให้การ generalization ดีขึ้น ส่วนมากการทำให้ program มีการ generalization ดีขึ้นจะต้องมีการแลกเปลี่ยนกับผลลัพธ์ metric ของ training set ในกรณีนี้ยังเราสามารถลองเพิ่มจำนวน iterations ได้เพื่อดูว่าเรายังสามารถปรับปรุง program ให้ดีขึ้นไปได้อีกไหม โดยต้องอย่าลืมตรวจสอบ generalization gap (ความต่างของ error ระหว่าง training และ testing set) มีความกว้างมากไหม ถ้ากว้างมากอาจหมายความว่าเรา overfitting เรียบร้อยแล้ว\n",
    "> Note: ตอนนี้เรากำลังใช้ testing set ในการปรับตัวแปรที่จำเป็นในการสร้าง program เช่นจำนวน iterations หรือ learning rate การทำแบบนี้เป็นการพยายาม overfitting ทางอ้อมกับ testing set ดังนั้นเพื่อป้องกันสถานการณ์นี้เราจะมี validation set มาขั้นกลาง โดยเราใช้ validation set นี้ในการปรับ hyperparameters (parameters ที่ program ไม่ได้ปรับตรงๆในช่วงเรียนรู้ แต่เป็นตัวแปรสำคัญและมีผลต่อการเรียนรู้)\n",
    "\n",
    "# สุดท้ายแล้ว ถ้าเราอยากทำ Batch Gradient Descent ละจะทำยังไง\n",
    "เราทำการนำตัวแปร gradients ของทุก nodes ใน output layer จากแต่ละ example มาหารด้วยจำนวนสมาชิกใน batch เพราะการปรับ weights ของเราจะขึ้นอยู่กับตัวอย่างทั้งหมดใน batch นั้นเราเลยต้องทำการเฉลี่ยออกมา\n",
    "> Important: ความจริงเรื่องการหารมันเป็นผลเนื่องมาจากสุดท้ายเราคำนวนหา error ของแต่ละตัวอย่างและนำค่า error ทั้งหมดของแต่ละตัวอย่างมารวมกัน ตรงจุดนี้เราทำการหารด้วยตัวอย่างทั้งหมด นั่นทำให้พอเราหา gradients ของแต่ละ nodes จากแต่ละตัวอย่าง จะต้องหารด้วยจำนวนสมาชิกใน batch แต่ถ้าสมการ error เราไม่มีการหารด้วยตัวอย่างทั้งหมด ตอนเราหา gradients ก็ไม่ต้องหารจำนวนสมาชิกทั้งหมดใน batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 3.449173614715377\n",
      "Accuracy: 0.835\n",
      "Test Error: 0.3309046876836692\n",
      "Test Accuracy: 0.8426\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "alpha, iterations, hidden_layer1 = 0.01, 3000, 32\n",
    "weight_0_1 = 0.2 * np.random.random(size=(x_train.shape[1], hidden_layer1)) - 0.1\n",
    "weight_1_2 = 0.2 * np.random.random(size=(hidden_layer1, 10)) - 0.1\n",
    " \n",
    "for j in range(iterations):\n",
    "    error, correct_count = 0, 0\n",
    "    \n",
    "    for i in range(int(x_train.shape[0] / batch_size)):\n",
    "        batch_start, batch_end = (i * batch_size,(i+1) * batch_size)\n",
    "        layer_0 = x_train[batch_start:batch_end]\n",
    "        layer_1 = relu(layer_0.dot(weight_0_1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * (1/0.5)\n",
    "        layer_2 = layer_1.dot(weight_1_2)\n",
    "        \n",
    "        error += np.sum((layer_2 - y_train[batch_start:batch_end]) ** 2)\n",
    "        for k in range(batch_size):\n",
    "            correct_count += int(np.argmax(layer_2[k:k+1]) == np.argmax(y_train[batch_start + k:batch_start + k + 1]))\n",
    "        \n",
    "        layer_2_delta = (layer_2 - y_train[batch_start:batch_end]) * 1 / batch_size\n",
    "        layer_1_delta = derivative_of_relu(layer_1) * (layer_2_delta.dot(weight_1_2.T))\n",
    "        layer_1_delta = layer_1_delta * dropout_mask\n",
    "        \n",
    "        weight_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n",
    "        weight_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
    "    \n",
    "test_error, test_correct_count = 0.0, 0\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "\n",
    "    layer_0 = x_test[i:i+1]\n",
    "    layer_1 = relu(layer_0.dot(weight_0_1))\n",
    "    layer_2 = layer_1.dot(weight_1_2)\n",
    "\n",
    "    test_error += np.sum((layer_2 - y_test[i:i+1]) ** 2)\n",
    "    test_correct_count += int(np.argmax(layer_2) == np.argmax(y_test[i:i+1]))\n",
    "\n",
    "\n",
    "print(f'Error: {error / batch_size}')\n",
    "print(f'Accuracy: {correct_count / x_train.shape[0]}')\n",
    "print(f'Test Error: {test_error / float(x_test.shape[0])}')\n",
    "print(f'Test Accuracy: {test_correct_count / x_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สังเกตว่าการใช้ batch gradient descent จะเป็นการใช้ประโยชน์จากการ optimization ของ linear algebra ทำให้การอัพเดท weights 1 ครั้งเป็นการใช้ตัวอย่างเป็นกลุ่ม ส่งผลให้การประมวลผล training set ใน 1 iteration มีความเร็วมากขึ้น ส่งผลให้เราสามารถเพิ่มจำนวน iterations ได้มากขึ้น นอกจากนี้ถ้ามีการดู error ในทุกๆ iterations ตัว error จะมีแนวโน้มที่มีความเรียบเนียน (smooth) กว่าการอัพเดท weights ที่ละตัวอย่าง\n",
    "\n",
    "ก่อนจากกัน ทุกคนสามารถลองเล่นโดยการปรับ การ random weights ให้เหมาะสมยิ่งขึ้น (ป้องกัน vanishing gradient), หรือการทำ gradient clip (เพื่อป้องกัน exploding gradient) และ activation functions ดูได้ เพื่อดุประสิทธิภาพของตัว program เมื่อมีการปรับเปลี่ยน activation functions เนื่องจากตัวอย่างที่ได้ทำมาทั้งหมดจะเป็นการใช้ relu ส่วน output nodes ไม่ได้ใช้ activation functions ซึ่งจากโจทย์ข้อนี้ควรจะใช้ softmax หรือ sigmoid ใน output layers เพื่อให้คำตอบอยู่ในช่วง [0, 1] \n",
    "> Note: ถ้าใช้ sigmoid หรือ tanh เป็น hidden layer activation function อย่าลืม random weights ให้เข้าใกล้ 0 เพื่อที่ค่า derivatives จะได้ไม่เข้าใกล้ 0 เกินไปจน program ยากที่จะเรียนรู้หรือปรับปรุง weights\n",
    "\n",
    "หวังว่าทุกคนที่เข้ามาอ่านจะสนุกและได้ความรู้กันนะครับ\n",
    "\n",
    "\"If you cannot sleep in the night, you need more time to understand it\"\n",
    "\n",
    "Burin Sirisrimungkorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
