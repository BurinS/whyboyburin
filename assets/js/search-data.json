{
  
    
        "post0": {
            "title": "(TH) Brief Understanding of The World of Deep Learning",
            "content": "&#3627;&#3618;&#3640;&#3604;! &#3585;&#3656;&#3629;&#3609;&#3592;&#3632; Deep Learning &#3617;&#3634;&#3607;&#3635;&#3588;&#3623;&#3634;&#3617;&#3619;&#3641;&#3657;&#3592;&#3633;&#3585;&#3585;&#3633;&#3610; Machine Learning &#3585;&#3633;&#3609;&#3585;&#3656;&#3629;&#3609; . เมื่อ computer ถือกำเนิดขึ้นมาบนโลกของเรา มนุษย์ก็เริ่มมีความคิดและความหวังและความฝันว่า computer จะต้องมีความ intelligence ให้ได้ โดยอย่างน้อยก็มีความสามารถเทียบเท่ากับมนุษย์ หรือไปให้เหนือกว่า . Warning: ว่าแต่ที่บอกว่าเหมือนมนุษย์นี่หมายความว่ายังไงกันนะ?   . คอมพิวเตอร์สามารถแก้ไขปัญหาที่สามารถอธิบายได้ในรูปแบบของ formal rule หรือ mathematical rule ซึ่งมีขั้นตอนแบบแผนสวยงาม เราสามารถอธิบายเป็น steps ได้ ซึ่งงานที่สามารถอธิบายเป็น formal หรือ mathematical rule มนุษย์มีความท้าทายในการทำงานพวกนี้สุดๆ ลองบวกเลข 942324 + 134185 ภายใน 5 วินาที โดยห้ามใช้ computer หรือเครื่องคิดเลขนะ! แต่ประเด็นคือมนุษย์ดันเก่งเหลือเกินกับงานที่เราก็ไม่รู้จะอธิบายให้ัมันเป็น formal Knowledge ได้ยังไงเพื่อให้ computer สามารถทำตามและแก้ไขปัญหาได้ เช่น การมองและบอกได้ว่าเราเห็นช้างนะ ถ้าให้เราอธิบายว่าทำไมเราถึงเห็นสิ่งที่มองอยู่เป็นช้างได้ ลองถามคนที่มองสิ่งเดียวกันซัก 100 คน ก็จะเกิด Rule ที่ใช้อธิบายแตกต่างกันไปตามแต่ละคน ที่แปลกคือสมองคนเราสามารถเข้าใจสิ่งนี้ได้ง่ายมาก เรามองดูแล้วรู้ว่ามันคืออะไรเพียงไม่กี่ครั้ง สงสัยเป็นไปได้ว่าสมองของคนเรามีการพัฒนามาตั้งแต่อดีตอย่างต่อเนื่อง (มี pretrained weights เรียบร้อยแล้วสินะ) . ดังนั้นถ้าเราจะสร้าง intelligence computer เราก็ต้องย่อยข้อมูลทั้งหมดบนโลกเราและยัดมันเข้าไปในระบบที่เราสร้าง ซึ่งเคยมีคนทำแล้วโดยการ hardcode knowledge (formal knowledge) เข้าไปในระบบ ระบบสามารถทำงานได้ แต่ แต่... ด้วย rule ทั้งหมดก็ยังไม่ซับซ้อนพอที่จะอธิบายโลกเราได้อย่างสมบูรณ์ สุดท้ายก็เลยไม่ไหวแล้ว เราต้ิองหาวิธีใหม่ที่จะสร้าง informal knowledge เหล่านี้ให้กับ computer เพื่อที่จะบรรลุ (artificial) intelligence computer แล้วเราต้องทำยังไงกันดี? . Machine Learning &#3591;&#3633;&#3657;&#3609;&#3648;&#3627;&#3619;&#3629; . ในเมื่อมันยากนักที่จะย่อยความรู้บนโลกให้เป็น Formal knowledge งั้นเราก็ให้ Computer เรียนรู้และสร้างความรู้เองเลยเป็นไงละ การที่ Computer เรียนรู้จากข้อมูลและสร้างองค์ความรู้ขึ้นมาเองในบริบทนั้นๆ เราเรียกว่า Machine (Automatic) Learning (ตรงตัวมากจริงนั่นคือ เครื่องจักรที่เกิดการเรียนรู้) . &#3649;&#3609;&#3623;&#3588;&#3636;&#3604;&#3604;&#3641;&#3648;&#3592;&#3659;&#3591;&#3604;&#3637;&#3649;&#3605;&#3656;&#3617;&#3633;&#3609;&#3607;&#3635;&#3591;&#3634;&#3609;&#3592;&#3619;&#3636;&#3591;&#3654;&#3618;&#3633;&#3591;&#3652;&#3591;? . ให้จินตาการว่าเราต้องการสร้างหุ่นยนต์ที่สามารถบอกเราว่า วันนี้เหมาะไปเที่ยวไหม ได้แก่ เที่ยว, ไม่เที่ยว โดยหุ่นยนต์ตัวนี้จะบอกคำตอบ(ที่น่าจะเป็น)กับเราได้ก็ต่อเมื่อมันสามารถรับสัญญาณได้ว่าวันนี้ อากาศเ็นยังไงและ วันนี้คือวันอะไรของสัปดาห์ . Note: Input: [สภาพอากาศ, วันของสัปดาห์], Output: [เที่ยว, ไม่เที่ยว] . จินตนาการว่าสมองของหุ่นยนต์เราคือสมการคณิตศาสตร์ (อย่าพึ่งตกใจนะ เรายังไม่ได้เข้าไปสู่โลกของคณิตศาสตร์ แต่ที่ยกมาเพราะอยากสะท้อนสิ่งที่เกิดจริงๆในสมองข้างในของหุ่นยนต์) ดังนั้นตัวแปรอิสระเราก็คือ input และตัวแปรตามคือ output เรานำ input ไปทำ operation (+,-,x,/) กับตัวแปรอีกชุดซึ่งต่อไปจะขอเรียกว่า weights ถ้าเราสุ่มค่าของ weights แต่ละตัว แล้วนำมาทำ operation กับ input สุดท้ายจะได้คำตอบออกมา แต่คำตอบพวกนี้คงอาจจะมีถูกบ้าง ไม่ถูกบ้าง (แน่นอนเพราะเกิดจากการสุ่ม) ดังนั้นการเรียนรู้ของหุ่นยนต์เลยเกิดขึ้น ณ จุดๆนี้ สมองของหุ่นยนต์พยายามปรับ weights (ตัวแปรที่มีความสัมพันธ์กับ input เพื่อผลิต output) ให้ผลลัพธ์จากสมการมีความถูกต้องมากขึ้นเรื่อยๆ จนสุดท้ายเราจะได้ weights ที่เมื่อเจอกับ input จะสามารถให้คำตอบที่ถูกต้องได้ แน่นอนว่า input ของเราสามารถเป็นสิ่งที่ไม่ได้อยู่ในช่วงเรียนรู้ของหุ่นยนต์ได้ นั่นคือเราเอาหุ่นยนต์ไปใช้กับบ้านอื่นๆได้โดยมั่นใจได้ว่าหุ่นยนต์สามารถแก้ปัญหานี้ให้กับคนอื่นได้ เย้! . Important: เมื่อสมการผลิต output จะมีการนำ output ไปเทียบกับ output จริง เพื่อดูความถูกต้อง ดังนั้นเมื่อเกิด error จะมีการแจ้งเตือนไปยัง weights เพื่อบอกว่าสถานะพวกคุณตอนนี้ทำให้ผลลัพธ์ที่ได้จากสมการมีความผิดพลาด กรุณาทำการปรับค่าของพวกคุณให้เหมาะสมด้วย ขอบคุณครับ สุดท้าย weights ก็จะมีการปรับปรุงค่าจนถึงเวลาอันสมควรหรือ ผลลัพธ์ที่ได้มีความถูกต้องจนรับได้ ก็จะหยุดเรียนรู้ และเราสามารถนำ weights นั้นไปใช้งานจริงได้ . Note: Weight: Knowledge (Based On Context), Output: Information (Independent Variable) . &#3614;&#3629;&#3649;&#3621;&#3657;&#3623; Machine Learning &#3649;&#3621;&#3657;&#3623; Deep Learning &#3617;&#3633;&#3609;&#3588;&#3639;&#3629;&#3629;&#3632;&#3652;&#3619;? . ก่อนจะพูดถึง deep learning มาพูดถึงสิ่งหนึ่งก่อน . Neural Network . Neural network เกิดขึ้นมาจากการพยายามสร้างต้นแบบเพื่อจำลองการทำงานของสมองมนุษย์ แต่ต้นแบบที่สร้างมาเอาจริงๆก็ไม่ใช่สิ่งที่สมองเป็นหรือทำงานจริงๆตามรูปแบบนี้ที่จำลองขึ้นมา โดย model ที่สร้างขึ้นมาเรียกว่า perceptron โดยการทำงานคร่าวๆคือ neuron (ต่อไปขอเรียกว่า node) รับกระแสไฟฟ้าจาก nodes ที่เชื่อมต่อกับมัน โดยเส้นที่เชื่อมต่อจะมีค่าพลังงานความเข้มข้นในการเชื่อม (โดยต่อไปจะขอเรียกว่า weight) โดยค่าพลังงานไฟฟ้าที่เข้ามาใน node ปัจจุบันในที่นี้คือค่าพลังงานที่ปล่อยออกจาก node ก่อนหน้า มาคูณกับ weight ที่เชื่อม และเนื่องจาก node หนึ่งสามารถถูกเชื่อมด้วย node ก่อนหน้าได้หลาย nodes ทำให้เกิดการคูณกันของ node กับ weight ที่เชื่อมต่อ เราเลยได้ค่าออกมาเป็น list ที่มีจำนวนสมาชิกเท่ากับจำนวน nodes ก่อนหน้าที่เชื่อมต่อกับ node ปัจจุบัน ดังนั้นเราเลยเอามันมารวมพลังกันเพื่อดูว่าผ่าน threshold หรือไม่(ในอนาคตสิ่งนี้จะกลายเป็น activation function) ถ้าผ่าน node ปัจจุบันจะสามารถปล่อยค่าพลังไปยัง node อื่นต่อไป . แน่นอนว่า model ที่สร้างไม่ได้สะท้อนว่าจริงๆแล้วสมองเราทำงานแบบนี้ ดังนั้นเลยมีการปรับปรุงและพัฒนาต่อไปเรื่อยๆ แต่สุดท้าย deep learning ก็มีช่วงที่คนหมดความเชื่อถืออยู่ 2 ครั้ง . Note: ครั้งที่ 1 เกิดจากการที่ neural network program ไม่สามารถแก้ไขปัญหาที่มีชื่อว่า XOR (neural network มีเพียง 1 layer) ได้ซึ่งเป็นปัญหาที่ simple แต่ในบทความที่เขียนถึงข้อสังเกตนี้ก็ได้บอกต่อว่าปัญหา XOR นี้สามารถแก้ไขได้เมื่อเราเพิ่มจำนวน layer ให้กับ network program แต่สุดท้ายประเด็นแรกก็ถูกยกขึ้นมาและถูกสนใจมากกว่าประเด็นที่ 2 มาก จนคนหมดความหวังกับ neural network ขอต้อนรับสู่ deep learning winter ครั้งที่ 1 . Note: ครั้งที่ 2 เรารู้ถึงไอเดียแล้วว่า neural network สามารถแก้ไขทุกอย่างได้โดยการเพิ่ม layer ทั้งหมดเป็น 2 layers แต่สุดท้ายจะแก้ไขปัญหาให้จบเพียง 2 layers บางปัญหามีความซับซ้อนมากต้องอาศัย patameters (weights) จำนวนมาก นั้นหมายถึงต้องการพลังในการประมวลผลที่สูงและจำนวนข้อมูลที่มาก (prevent overfitting) นั้นทำให้คนเริ่มหมดความหวังกับ deep learning ขอต้อนรับสู่ deep learning winter ครั้งที่ 2 ต่อมามีคนค้นพบว่าปัญหานี้แก้ได้่โดยการเพิ่ม layer มากขึ้นไปอีก โดยมากกว่า 2 layers จำนวน parameters สามารถเพิ่มขึ้นได้โดยที่ไม่ทำให้เกิดการคำนวณมากเกินไป . &#3611;&#3632;&#3623;&#3633;&#3605;&#3636;&#3650;&#3604;&#3618;&#3618;&#3656;&#3629;&#3617;&#3634;&#3585;&#3654;&#3586;&#3629;&#3591; Deep Learning &#3649;&#3621;&#3632;&#3606;&#3639;&#3629;&#3650;&#3629;&#3585;&#3634;&#3626;&#3586;&#3629;&#3610;&#3588;&#3640;&#3603;&#3607;&#3640;&#3585;&#3588;&#3609;&#3607;&#3637;&#3656;&#3617;&#3637;&#3626;&#3656;&#3623;&#3609;&#3619;&#3656;&#3623;&#3617;&#3651;&#3609;&#3611;&#3619;&#3632;&#3623;&#3633;&#3605;&#3636;&#3624;&#3634;&#3626;&#3605;&#3619;&#3660;&#3588;&#3619;&#3633;&#3657;&#3591;&#3609;&#3637;&#3657; . Deep learning ที่กำลังเป็นส่วนสำคัญในการสร้างอนาคตไปด้วยกันกับเรา เกือบที่จะหายไปหลายครั้งแล้วแต่โชคดีที่เรามีคนที่อยู่เบื้องหลังที่ยังคอยพัฒนา, วิจัยและผลักดัน deep learning ให้สามารถดำเนินต่อไปได้ หลังจากที่ผ่านช่วง hype และมีความคาดหวังที่สูง neural network ก็ถูกปัดออกจากการเป็นสิ่งที่น่าสนใจของคนหมู่มากในช่วง 1990s และ 2000s และมีเพียงนักวิจัยไม่มากที่ยังคอยพัฒนาและมีความเชื่อว่าซักวันจะถึงเวลาของพระเอกของเรา deep learning! . โดยผู้อยู่เบื้องหลังทั้ง 3 คนที่ยังคงมีความเชื่อมั่นว่า deep learning คืออนาคตของ AI ได้แก่ Yann Lecun, Yoshua Bengio และ Geoffrey Hinton ทั้ง 3 เป็นผู้ที่ได้รับรางวัล Turing Award ซึ่งเป็นรางวัลสูงสุดของสายงาน Computer Science เปรียบได้กับรางวัลออสการ์จากวงการภาพยนตร์ Lecun ได้ทำงานเกี่ยวกับ convolutional neural network (CNN) โดยได้สร้าง deep learning program ที่สามารถบอกได้ว่าตัวเลขที่เขียนด้วยลายมือเป็นเลขอะไร (MNIST dataset) และถูกนำไปใช้ในการแง่มุมของธุรกิจเพื่อใช้อ่านตัวเลขจากลายมือ โดยคิดเป็น 10% จากงานทั้งหมดที่เกิดขึ้นใน US . จริงๆแล้วไม่ได้มีแค่ 3 คนที่ยังคอยผลักดัน ยังมีนักวิจัยอีกบางส่วนที่ได้สร้างไอเดียที่สำคัญและปัจจุบันก็เป็นเทคนิคที่นิยมมากๆนั้นคือ Long Shot Term Memory (LSTM) โดย Jurgen Schmidhuber และนักเรียนชื่อ Sepp Hocheriter . นอกจากนี้ในปี 1974 Paul Werbos ได้คิดค้นเทคนิคที่มีชื่อว่า back propagation เทคนิคที่ปัจจุบันถูกใช้เป็นเทคนิคหลักในการเรียนรู้ของ neural network program ถึงแม้ว่าเทคนิคนี้จะทรงพลังแค่ไหนก็ยังถูกมองข้ามเป็นหลักสิบปี เพราะคนไม่ให้ความสำคัญกับ neural network แต่ในปัจจุบัน back propagation คือเทคนิคสำคัญสำหรับการสร้าง AI ด้วย deep learning . จะเห็นว่า deep learning ได้ผ่านช่วงที่อยากลำบากมาหลายครั้ง ถ้าเราไม่มีคนที่คอยวิจัยและผลักดันก็จะไม่เกิดสิ่งที่เราให้ความสนใจอยู่ในปัจจุบันและสิ่งที่เป็นกำลังหลักในการสร้างอนาคตและยกระดับความสามารถของมนุษย์ขึ้นไปอีกขั้น . ต้องขอขอบคุณทุกคนที่มีส่วนในการสร้างและผลักดันทั้งในอดีตและปัจจุบันรวมถึงอนาคต โดยปัจจุบันผลลัพธ์ของสิ่งนั้นได้เห็นผลแล้วและคนก็มีความสนใจกันมากขึ้น รวมถึงการทำให้ deep learning เข้าถึงได้ง่ายมากขึ้นด้วย framework ต่างๆ ความคาดหวังของผมต่อไปคือการไปต่อและสร้างความก้าวหน้าในสิ่งที่เรายังไม่รู้ในตอนนี้ . ในปี 1943 เรามี model แรก ที่พยายามใช้ในการทำความเข้าใจการทำงานของสมองมนุษย์ | ช่วงปี 1950 perceptron model สามารถปรับ weights ได้ด้วยตนเอง (ไม่มีมนุษย์มาคอยดูและปรับ) ด้วย optimization algorithm ที่มีชื่อว่า SGD แต่ก็แก้ไขได้แค่่ปัญหาที่เป็น linear function | ในปี 1969 perceptron model ไม่สามารถแก้ไขปัญหา non linear functions ได้ ซึ่งประเด็นนี้ทำให้งานวิจัย neural network แทบจะถูกทิ้งและไม่ได้รับความสนใจ | มีกลุ่มของนักวิจัยที่ยังพัฒนาและสร้าง neural network อย่างเงียบๆ | ประมาณช่วง 1970s มีการคิดค้น back propagation algorithm ใช้สำหรับการเรียนรู้ของ neural network ที่มีมากกว่า 2 layers แต่ก็ยังไม่เป็นที่นิยมเพราะต้องการพลังงานในการประมวลผลและข้อมูลจำนวนมาก | ในปี 1988 เรามี CNN เกิดขึ้นบนโลกเรา | ประมาณปี 2000 deep learning เริ่มกับมาเป็นที่สนใจ ต้องขอบคุณพลังของการประมวลผล(ขอบคุณ GPUs) จำนวนข้อมูลที่มากและมีการสร้าง datasets กันแบบจริงจัง รวมถึงการทำเป็น standard เพื่อกระตุ้นให้เกิดการพัฒนาอย่างต่อเนื่อง รวมถึง software ในแง่การสร้างพัฒนา และในแง่การเรียนรู้ของ computer ผ่านการทำงานแบบ distributed | . &#3649;&#3621;&#3657;&#3623;&#3592;&#3619;&#3636;&#3591;&#3654; Deep Laerning &#3588;&#3639;&#3629;&#3629;&#3632;&#3652;&#3619;&#3606;&#3657;&#3634;&#3648;&#3619;&#3634;&#3652;&#3617;&#3656;&#3617;&#3629;&#3591;&#3617;&#3633;&#3609;&#3648;&#3611;&#3655;&#3609;&#3585;&#3634;&#3619;&#3592;&#3635;&#3621;&#3629;&#3591;&#3585;&#3634;&#3619;&#3607;&#3635;&#3591;&#3634;&#3609;&#3586;&#3629;&#3591;&#3626;&#3617;&#3629;&#3591;&#3617;&#3609;&#3640;&#3625;&#3618;&#3660; . ผมมอง deep learning ในแง่ของการเป็น program หรือจะพูดแบบเจาะจงคือ multi step program โดยแต่ละ layer คือ represntation state ของข้อมูล (หรือ state ความคิดของ program ณ จุดนั้นๆ) โดยในแต่ละ step เราทำการปรับปรุง state (representation) โดยการนำมาผ่าน simple non linear function เพื่อที่จะได้ state ใหม่ที่ถูกปรับปรุง จะสังเกตได่ว่า program มีการคำนวณเป็น step และทุก step เราจะได้ representation state ที่มีความ high level, abstract และ meaningful ลองจินตนาการว่า ถ้าเราต้องเขียนสมการที่ทำการแปลงรูปภาพให้กลายเป็นผลลัพธ์จากการตรวจจับว่ามีรถหรือคนในรูปภาพไหม ตัวสมการจะมีความยากและซับซ้อนมากๆ แต่ถ้าเราแบ่งมันออกมาเป็น step by step โดยการผ่านรูปภาพไปยัง step แรกเราจะได้ representation ใหม่ออกมาที่บอกว่าในรูปเรามี edge ที่ทำมุมต่างๆที่เราสนใจไหม จากนั้นเอาข้อมูลนี้ไปผ่าน step ที่ 2 จะได้ representation ออกมาว่ามีรูปทรงเรขาคณิตต่างๆเหล่านี้บ้างไหม แล้วเมื่อเราผ่านแต่ละ steps ไปเรื่อยๆ สุดท้ายเราก็จะเจอ step ที่ทำหน้าที่ในการตรวจจับว่าจาก representation state ปัจจุบันนี้ มีคนหรือรถอยู่บ้างไหม แทนที่เราจะคิดสมการที่ซับซ้อนที่สามารถให้คำตอบว่ามีรถหรือคนไหมจาก input ที่เป็นรูปภาพ ซึ่งต่ออาศัย parameters และการคำนวณมหาศาล เราแตกย่อยออกมาเป็นหลายๆ step โดยแต่ละ step ทำการ transform ด้วยสมการที่ไม่ซับซ้อน . &#3617;&#3637;&#3585;&#3634;&#3619;&#3614;&#3641;&#3604;&#3606;&#3638;&#3591; Representation &#3610;&#3656;&#3629;&#3618;&#3617;&#3634;&#3585;&#3649;&#3626;&#3604;&#3591;&#3623;&#3656;&#3634;&#3617;&#3633;&#3609;&#3626;&#3635;&#3588;&#3633;&#3597;&#3651;&#3594;&#3656;&#3652;&#3627;&#3617; . ประเด็นหลักของ deep learning คือเราจะหา representation ที่ดีหรือเหมาะสมยังไง ลองคิดดูว่าถ้าเราต้องค้นหาเลขที่เราสนใจจากข้อมูล array ที่มีขนาดใหญ่มาก จาก 2 array ที่มีข้อมูลข้างในเหมือนกันทุกอย่างแต่ต่างกันตรงที่ . Array ที่มีการเรียงจากน้อยไปหามาก | Array ที่ไม่มีการเรียงลำดับ | คิดว่าแบบไหนจะทำให้เราหาเลขที่เราสนใจได้สะดวกกว่ากัน คำตอบก็คือแบบที่ 2 ดังนั้นสิ่งที่ deep learning ทำคือการหา representation ที่ดี เพื่อที่เราจะสามารถแก้ไขปัญหาได้โดยการใช้เพียง simple linear function (output layer ใช้ weight sum ในกรณีต้องการคำตอบที่เป็น continuous value) deep learning ทำการสร้าง hirarchical representation learning โดยเมื่อผ่าน step 1 มาแล้วอาจจะยังไม่ใช่ Representation ที่ดีที่สุด . Note: representation ที่บอกว่าเจอ edges มุมต่างๆไหม ยังไม่มีความหมายในการเอามาใช้บอกได้ว่ามีคนหรือรถอยู่ในภาพไหม เพราะผลลัพธ์จากการตรวจจับว่าเกิดเส้นด้วยมุมต่างๆ มันสามารถเปลี่ยนแปลงได้ตามปัจจัยที่ส่งผลให้ภาพเปลี่ยนแปลงเช่น การเคลื่อนที่ของ object, แสง เป็นต้น (ยกเว้นภาพนี้จะไม่มีปัจจัยเหล่านี้เข้ามากระทบ เราก็อาจจะตั้งสมมุติฐานได้ว่าถ้าเกิดเส้นองศาตามนี้ ที่ตำแหน่งนี้ ก็แปลว่ามีคน แต่ในโลกความเป็นจริงจะไม่เป็นเช่นนี้) . ดังนั้นจึงทำการไป step ต่อไปเรื่อยๆ จนสุดท้ายได้ representation ที่เหมาะสมที่สามารถให้คำตอบของปัญหาได้ง่ายๆโดยใช้สมการที่ไม่ซับซ้อน . &#3648;&#3619;&#3634;&#3626;&#3634;&#3617;&#3634;&#3619;&#3606;&#3617;&#3629;&#3591; Deep Learning &#3648;&#3611;&#3655;&#3609; program &#3652;&#3604;&#3657; &#3650;&#3604;&#3618;&#3617;&#3629;&#3591;&#3651;&#3609;&#3619;&#3641;&#3611;&#3649;&#3610;&#3610; Computation Graph . ความลึกของ Graph คือ steps ในรูปแบบ Sequence (Layers) | ความกว้างของ Graph คือ steps ในรูปแบบ Parallel (Nodes ในแต่ละ layer) | . &quot;We live in a twilight world.&quot; . Tenet .",
            "url": "https://burins.github.io/whyboyburin/deep%20learning/2020/09/17/Brief-Understanding-of-The-World-of-Deep-Learning.html",
            "relUrl": "/deep%20learning/2020/09/17/Brief-Understanding-of-The-World-of-Deep-Learning.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://burins.github.io/whyboyburin/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://burins.github.io/whyboyburin/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://burins.github.io/whyboyburin/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://burins.github.io/whyboyburin/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}