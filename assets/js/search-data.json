{
  
    
        "post0": {
            "title": "(TH) Brief Understanding of The World of Deep Learning",
            "content": "&#3627;&#3618;&#3640;&#3604;! &#3585;&#3656;&#3629;&#3609;&#3592;&#3632; Deep Learning &#3617;&#3634;&#3607;&#3635;&#3588;&#3623;&#3634;&#3617;&#3619;&#3641;&#3657;&#3592;&#3633;&#3585;&#3585;&#3633;&#3610; Machine Learning &#3585;&#3633;&#3609;&#3585;&#3656;&#3629;&#3609; . เมื่อ computer ถือกำเนิดขึ้นมาบนโลกของเรา มนุษย์ก็เริ่มมีความคิดและความหวังและความฝันว่า computer จะต้องมีความ intelligence ให้ได้ โดยอย่างน้อยก็มีความสามารถเทียบเท่ากับมนุษย์ หรือไปให้เหนือกว่า . Warning: ว่าแต่ที่บอกว่าเหมือนมนุษย์นี่หมายความว่ายังไงกันนะ?   . คอมพิวเตอร์สามารถแก้ไขปัญหาที่สามารถอธิบายได้ในรูปแบบของ formal rule หรือ mathematical rule ซึ่งมีขั้นตอนแบบแผนสวยงาม เราสามารถอธิบายเป็น steps ได้ ซึ่งงานที่สามารถอธิบายเป็น formal หรือ mathematical rule มนุษย์มีความท้าทายในการทำงานพวกนี้สุดๆ ลองบวกเลข 942324 + 134185 ภายใน 5 วินาที โดยห้ามใช้ computer หรือเครื่องคิดเลขนะ! แต่ประเด็นคือมนุษย์ดันเก่งเหลือเกินกับงานที่เราก็ไม่รู้จะอธิบายให้มันเป็น formal knowledge ได้ยังไงเพื่อให้ computer สามารถทำตามและแก้ไขปัญหาได้ เช่น การมองและบอกได้ว่าเราเห็นช้างนะ ถ้าให้เราอธิบายว่าทำไมเราถึงเห็นสิ่งที่มองอยู่เป็นช้างได้ ลองถามคนที่มองสิ่งเดียวกันซัก 100 คน ก็จะเกิด Rule ที่ใช้อธิบายแตกต่างกันไปตามแต่ละคน ที่แปลกคือสมองคนเราสามารถเข้าใจสิ่งนี้ได้ง่ายมาก เรามองดูแล้วรู้ว่ามันคืออะไรเพียงไม่กี่ครั้ง สงสัยเป็นไปได้ว่าสมองของคนเรามีการพัฒนามาตั้งแต่อดีตอย่างต่อเนื่อง (มี pretrained weights เรียบร้อยแล้วสินะ) . ดังนั้นถ้าเราจะสร้าง intelligence computer เราก็ต้องย่อยข้อมูลทั้งหมดบนโลกเราและยัดมันเข้าไปในระบบที่เราสร้าง ซึ่งเคยมีคนทำแล้วโดยการ hardcode knowledge (formal knowledge) เข้าไปในระบบ ระบบสามารถทำงานได้ แต่ แต่... ด้วย rule ทั้งหมดก็ยังไม่ซับซ้อนพอที่จะอธิบายโลกเราได้อย่างสมบูรณ์ สุดท้ายก็เลยไม่ไหวแล้ว เราต้ิองหาวิธีใหม่ที่จะสร้าง informal knowledge เหล่านี้ให้กับ computer เพื่อที่จะบรรลุ (artificial) intelligence computer แล้วเราต้องทำยังไงกันดี? . Machine Learning &#3591;&#3633;&#3657;&#3609;&#3648;&#3627;&#3619;&#3629; . ในเมื่อมันยากนักที่จะย่อยความรู้บนโลกให้เป็น formal knowledge งั้นเราก็ให้ Computer เรียนรู้และสร้างความรู้เองเลยเป็นไงละ การที่ Computer เรียนรู้จากข้อมูลและสร้างองค์ความรู้ขึ้นมาเองในบริบทนั้นๆ เราเรียกว่า Machine (Automatic) Learning (ตรงตัวมากจริงนั่นคือ เครื่องจักรที่เกิดการเรียนรู้) . &#3649;&#3609;&#3623;&#3588;&#3636;&#3604;&#3604;&#3641;&#3648;&#3592;&#3659;&#3591;&#3604;&#3637;&#3649;&#3605;&#3656;&#3617;&#3633;&#3609;&#3607;&#3635;&#3591;&#3634;&#3609;&#3592;&#3619;&#3636;&#3591;&#3654;&#3618;&#3633;&#3591;&#3652;&#3591;? . ให้จินตาการว่าเราต้องการสร้างหุ่นยนต์ที่สามารถบอกเราว่า วันนี้เหมาะไปเที่ยวไหม ได้แก่ เที่ยว, ไม่เที่ยว โดยหุ่นยนต์ตัวนี้จะบอกคำตอบ(ที่น่าจะเป็น)กับเราได้ก็ต่อเมื่อมันสามารถรับสัญญาณได้ว่าวันนี้ อากาศเ็นยังไงและ วันนี้คือวันอะไรของสัปดาห์ . Note: Input: [สภาพอากาศ, วันของสัปดาห์], Output: [เที่ยว, ไม่เที่ยว] . จินตนาการว่าสมองของหุ่นยนต์เราคือสมการคณิตศาสตร์ (อย่าพึ่งตกใจนะ เรายังไม่ได้เข้าไปสู่โลกของคณิตศาสตร์ แต่ที่ยกมาเพราะอยากสะท้อนสิ่งที่เกิดจริงๆในสมองข้างในของหุ่นยนต์) ดังนั้นตัวแปรอิสระเราก็คือ input และตัวแปรตามคือ output เรานำ input ไปทำ operation (+,-,x,/) กับตัวแปรอีกชุดซึ่งต่อไปจะขอเรียกว่า weights ถ้าเราสุ่มค่าของ weights แต่ละตัว แล้วนำมาทำ operation กับ input สุดท้ายจะได้คำตอบออกมา แต่คำตอบพวกนี้คงอาจจะมีถูกบ้าง ไม่ถูกบ้าง (แน่นอนเพราะเกิดจากการสุ่ม) ดังนั้นการเรียนรู้ของหุ่นยนต์เลยเกิดขึ้น ณ จุดๆนี้ สมองของหุ่นยนต์พยายามปรับ weights (ตัวแปรที่มีความสัมพันธ์กับ input เพื่อผลิต output) ให้ผลลัพธ์จากสมการมีความถูกต้องมากขึ้นเรื่อยๆ จนสุดท้ายเราจะได้ weights ที่เมื่อเจอกับ input จะสามารถให้คำตอบที่ถูกต้องได้ แน่นอนว่า input ของเราสามารถเป็นสิ่งที่ไม่ได้อยู่ในช่วงเรียนรู้ของหุ่นยนต์ได้ นั่นคือเราเอาหุ่นยนต์ไปใช้กับบ้านอื่นๆได้โดยมั่นใจได้ว่าหุ่นยนต์สามารถแก้ปัญหานี้ให้กับคนอื่นได้ เย้! . Important: เมื่อสมการผลิต output จะมีการนำ output ไปเทียบกับ output จริง เพื่อดูความถูกต้อง ดังนั้นเมื่อเกิด error จะมีการแจ้งเตือนไปยัง weights เพื่อบอกว่าสถานะพวกคุณตอนนี้ทำให้ผลลัพธ์ที่ได้จากสมการมีความผิดพลาด กรุณาทำการปรับค่าของพวกคุณให้เหมาะสมด้วย ขอบคุณครับ สุดท้าย weights ก็จะมีการปรับปรุงค่าจนถึงเวลาอันสมควรหรือ ผลลัพธ์ที่ได้มีความถูกต้องจนรับได้ ก็จะหยุดเรียนรู้ และเราสามารถนำ weights นั้นไปใช้งานจริงได้ . Note: Weight: Knowledge (Based On Context), Output: Information (Independent Variable) . &#3614;&#3629;&#3649;&#3621;&#3657;&#3623; Machine Learning &#3649;&#3621;&#3657;&#3623; Deep Learning &#3617;&#3633;&#3609;&#3588;&#3639;&#3629;&#3629;&#3632;&#3652;&#3619;? . ก่อนจะพูดถึง deep learning มาพูดถึงสิ่งหนึ่งก่อน . Neural Network . Neural network เกิดขึ้นมาจากการพยายามสร้างต้นแบบเพื่อจำลองการทำงานของสมองมนุษย์ แต่ต้นแบบที่สร้างมาเอาจริงๆก็ไม่ใช่สิ่งที่สมองเป็นหรือทำงานจริงๆตามรูปแบบนี้ที่จำลองขึ้นมา โดย model ที่สร้างขึ้นมาเรียกว่า perceptron โดยการทำงานคร่าวๆคือ neuron (ต่อไปขอเรียกว่า node) รับกระแสไฟฟ้าจาก nodes ที่เชื่อมต่อกับมัน โดยเส้นที่เชื่อมต่อจะมีค่าพลังงานความเข้มข้นในการเชื่อม (โดยต่อไปจะขอเรียกว่า weight) โดยค่าพลังงานไฟฟ้าที่เข้ามาใน node ปัจจุบันในที่นี้คือค่าพลังงานที่ปล่อยออกจาก node ก่อนหน้า มาคูณกับ weight ที่เชื่อม และเนื่องจาก node หนึ่งสามารถถูกเชื่อมด้วย node ก่อนหน้าได้หลาย nodes ทำให้เกิดการคูณกันของ node กับ weight ที่เชื่อมต่อ เราเลยได้ค่าออกมาเป็น list ที่มีจำนวนสมาชิกเท่ากับจำนวน nodes ก่อนหน้าที่เชื่อมต่อกับ node ปัจจุบัน ดังนั้นเราเลยเอามันมารวมพลังกันเพื่อดูว่าผ่าน threshold หรือไม่(ในอนาคตสิ่งนี้จะกลายเป็น activation function) ถ้าผ่าน node ปัจจุบันจะสามารถปล่อยค่าพลังไปยัง node อื่นต่อไป . แน่นอนว่า model ที่สร้างไม่ได้สะท้อนว่าจริงๆแล้วสมองเราทำงานแบบนี้ ดังนั้นเลยมีการปรับปรุงและพัฒนาต่อไปเรื่อยๆ แต่สุดท้าย deep learning ก็มีช่วงที่คนหมดความเชื่อถืออยู่ 2 ครั้ง . Note: ครั้งที่ 1 เกิดจากการที่ neural network program ไม่สามารถแก้ไขปัญหาที่มีชื่อว่า XOR (neural network มีเพียง 1 layer) ได้ซึ่งเป็นปัญหาที่ simple แต่ในบทความที่เขียนถึงข้อสังเกตนี้ก็ได้บอกต่อว่าปัญหา XOR นี้สามารถแก้ไขได้เมื่อเราเพิ่มจำนวน layer ให้กับ network program แต่สุดท้ายประเด็นแรกก็ถูกยกขึ้นมาและถูกสนใจมากกว่าประเด็นที่ 2 มาก จนคนหมดความหวังกับ neural network ขอต้อนรับสู่ deep learning winter ครั้งที่ 1 . Note: ครั้งที่ 2 เรารู้ถึงไอเดียแล้วว่า neural network สามารถแก้ไขทุกอย่างได้โดยการเพิ่ม layer ทั้งหมดเป็น 2 layers แต่สุดท้ายจะแก้ไขปัญหาให้จบเพียง 2 layers บางปัญหามีความซับซ้อนมากต้องอาศัย patameters (weights) จำนวนมาก นั้นหมายถึงต้องการพลังในการประมวลผลที่สูงและจำนวนข้อมูลที่มาก (prevent overfitting) นั้นทำให้คนเริ่มหมดความหวังกับ deep learning ขอต้อนรับสู่ deep learning winter ครั้งที่ 2 ต่อมามีคนค้นพบว่าปัญหานี้แก้ได้่โดยการเพิ่ม layer มากขึ้นไปอีก โดยมากกว่า 2 layers จำนวน parameters สามารถเพิ่มขึ้นได้โดยที่ไม่ทำให้เกิดการคำนวณมากเกินไป . &#3611;&#3632;&#3623;&#3633;&#3605;&#3636;&#3650;&#3604;&#3618;&#3618;&#3656;&#3629;&#3617;&#3634;&#3585;&#3654;&#3586;&#3629;&#3591; Deep Learning &#3649;&#3621;&#3632;&#3606;&#3639;&#3629;&#3650;&#3629;&#3585;&#3634;&#3626;&#3586;&#3629;&#3610;&#3588;&#3640;&#3603;&#3607;&#3640;&#3585;&#3588;&#3609;&#3607;&#3637;&#3656;&#3617;&#3637;&#3626;&#3656;&#3623;&#3609;&#3619;&#3656;&#3623;&#3617;&#3651;&#3609;&#3611;&#3619;&#3632;&#3623;&#3633;&#3605;&#3636;&#3624;&#3634;&#3626;&#3605;&#3619;&#3660;&#3588;&#3619;&#3633;&#3657;&#3591;&#3609;&#3637;&#3657; . Deep learning ที่กำลังเป็นส่วนสำคัญในการสร้างอนาคตไปด้วยกันกับเรา เกือบที่จะหายไปหลายครั้งแล้วแต่โชคดีที่เรามีคนที่อยู่เบื้องหลังที่ยังคอยพัฒนา, วิจัยและผลักดัน deep learning ให้สามารถดำเนินต่อไปได้ หลังจากที่ผ่านช่วง hype และมีความคาดหวังที่สูง neural network ก็ถูกปัดออกจากการเป็นสิ่งที่น่าสนใจของคนหมู่มากในช่วง 1990s และ 2000s และมีเพียงนักวิจัยไม่มากที่ยังคอยพัฒนาและมีความเชื่อว่าซักวันจะถึงเวลาของพระเอกของเรา deep learning! . โดยผู้อยู่เบื้องหลังทั้ง 3 คนที่ยังคงมีความเชื่อมั่นว่า deep learning คืออนาคตของ AI ได้แก่ Yann Lecun, Yoshua Bengio และ Geoffrey Hinton ทั้ง 3 เป็นผู้ที่ได้รับรางวัล Turing Award ซึ่งเป็นรางวัลสูงสุดของสายงาน Computer Science เปรียบได้กับรางวัลออสการ์จากวงการภาพยนตร์ Lecun ได้ทำงานเกี่ยวกับ convolutional neural network (CNN) โดยได้สร้าง deep learning program ที่สามารถบอกได้ว่าตัวเลขที่เขียนด้วยลายมือเป็นเลขอะไร (MNIST dataset) และถูกนำไปใช้ในการแง่มุมของธุรกิจเพื่อใช้อ่านตัวเลขจากลายมือ โดยคิดเป็น 10% จากงานทั้งหมดที่เกิดขึ้นใน US . จริงๆแล้วไม่ได้มีแค่ 3 คนที่ยังคอยผลักดัน ยังมีนักวิจัยอีกบางส่วนที่ได้สร้างไอเดียที่สำคัญและปัจจุบันก็เป็นเทคนิคที่นิยมมากๆนั้นคือ Long Shot Term Memory (LSTM) โดย Jurgen Schmidhuber และนักเรียนชื่อ Sepp Hocheriter . นอกจากนี้ในปี 1974 Paul Werbos ได้คิดค้นเทคนิคที่มีชื่อว่า back propagation เทคนิคที่ปัจจุบันถูกใช้เป็นเทคนิคหลักในการเรียนรู้ของ neural network program ถึงแม้ว่าเทคนิคนี้จะทรงพลังแค่ไหนก็ยังถูกมองข้ามเป็นหลักสิบปี เพราะคนไม่ให้ความสำคัญกับ neural network แต่ในปัจจุบัน back propagation คือเทคนิคสำคัญสำหรับการสร้าง AI ด้วย deep learning . จะเห็นว่า deep learning ได้ผ่านช่วงที่อยากลำบากมาหลายครั้ง ถ้าเราไม่มีคนที่คอยวิจัยและผลักดันก็จะไม่เกิดสิ่งที่เราให้ความสนใจอยู่ในปัจจุบันและสิ่งที่เป็นกำลังหลักในการสร้างอนาคตและยกระดับความสามารถของมนุษย์ขึ้นไปอีกขั้น . ต้องขอขอบคุณทุกคนที่มีส่วนในการสร้างและผลักดันทั้งในอดีตและปัจจุบันรวมถึงอนาคต โดยปัจจุบันผลลัพธ์ของสิ่งนั้นได้เห็นผลแล้วและคนก็มีความสนใจกันมากขึ้น รวมถึงการทำให้ deep learning เข้าถึงได้ง่ายมากขึ้นด้วย framework ต่างๆ ความคาดหวังของผมต่อไปคือการไปต่อและสร้างความก้าวหน้าในสิ่งที่เรายังไม่รู้ในตอนนี้ . ในปี 1943 เรามี model แรก ที่พยายามใช้ในการทำความเข้าใจการทำงานของสมองมนุษย์ | ช่วงปี 1950 perceptron model สามารถปรับ weights ได้ด้วยตนเอง (ไม่มีมนุษย์มาคอยดูและปรับ) ด้วย optimization algorithm ที่มีชื่อว่า SGD แต่ก็แก้ไขได้แค่่ปัญหาที่เป็น linear function | ในปี 1969 perceptron model ไม่สามารถแก้ไขปัญหา non linear functions ได้ ซึ่งประเด็นนี้ทำให้งานวิจัย neural network แทบจะถูกทิ้งและไม่ได้รับความสนใจ | มีกลุ่มของนักวิจัยที่ยังพัฒนาและสร้าง neural network อย่างเงียบๆ | ประมาณช่วง 1970s มีการคิดค้น back propagation algorithm ใช้สำหรับการเรียนรู้ของ neural network ที่มีมากกว่า 2 layers แต่ก็ยังไม่เป็นที่นิยมเพราะต้องการพลังงานในการประมวลผลและข้อมูลจำนวนมาก | ในปี 1988 เรามี CNN เกิดขึ้นบนโลกเรา | ประมาณปี 2000 deep learning เริ่มกับมาเป็นที่สนใจ ต้องขอบคุณพลังของการประมวลผล(ขอบคุณ GPUs) จำนวนข้อมูลที่มากและมีการสร้าง datasets กันแบบจริงจัง รวมถึงการทำเป็น standard เพื่อกระตุ้นให้เกิดการพัฒนาอย่างต่อเนื่อง รวมถึง software ในแง่การสร้างพัฒนา และในแง่การเรียนรู้ของ computer ผ่านการทำงานแบบ distributed | . &#3649;&#3621;&#3657;&#3623;&#3592;&#3619;&#3636;&#3591;&#3654; Deep Learning &#3588;&#3639;&#3629;&#3629;&#3632;&#3652;&#3619;&#3606;&#3657;&#3634;&#3648;&#3619;&#3634;&#3652;&#3617;&#3656;&#3617;&#3629;&#3591;&#3617;&#3633;&#3609;&#3648;&#3611;&#3655;&#3609;&#3585;&#3634;&#3619;&#3592;&#3635;&#3621;&#3629;&#3591;&#3585;&#3634;&#3619;&#3607;&#3635;&#3591;&#3634;&#3609;&#3586;&#3629;&#3591;&#3626;&#3617;&#3629;&#3591;&#3617;&#3609;&#3640;&#3625;&#3618;&#3660; . ผมมอง deep learning ในแง่ของการเป็น program หรือจะพูดแบบเจาะจงคือ multi step program โดยแต่ละ layer คือ represntation state ของข้อมูล (หรือ state ความคิดของ program ณ จุดนั้นๆ) โดยในแต่ละ step เราทำการปรับปรุง state (representation) โดยการนำมาผ่าน simple non linear function เพื่อที่จะได้ state ใหม่ที่ถูกปรับปรุง จะสังเกตได่ว่า program มีการคำนวณเป็น step และทุก step เราจะได้ representation state ที่มีความ high level, abstract และ meaningful ลองจินตนาการว่า ถ้าเราต้องเขียนสมการที่ทำการแปลงรูปภาพให้กลายเป็นผลลัพธ์จากการตรวจจับว่ามีรถหรือคนในรูปภาพไหม ตัวสมการจะมีความยากและซับซ้อนมากๆ แต่ถ้าเราแบ่งมันออกมาเป็น step by step โดยการผ่านรูปภาพไปยัง step แรกเราจะได้ representation ใหม่ออกมาที่บอกว่าในรูปเรามี edge ที่ทำมุมต่างๆที่เราสนใจไหม จากนั้นเอาข้อมูลนี้ไปผ่าน step ที่ 2 จะได้ representation ออกมาว่ามีรูปทรงเรขาคณิตต่างๆเหล่านี้บ้างไหม แล้วเมื่อเราผ่านแต่ละ steps ไปเรื่อยๆ สุดท้ายเราก็จะเจอ step ที่ทำหน้าที่ในการตรวจจับว่าจาก representation state ปัจจุบันนี้ มีคนหรือรถอยู่บ้างไหม แทนที่เราจะคิดสมการที่ซับซ้อนที่สามารถให้คำตอบว่ามีรถหรือคนไหมจาก input ที่เป็นรูปภาพ ซึ่งต่ออาศัย parameters และการคำนวณมหาศาล เราแตกย่อยออกมาเป็นหลายๆ step โดยแต่ละ step ทำการ transform ด้วยสมการที่ไม่ซับซ้อน . &#3617;&#3637;&#3585;&#3634;&#3619;&#3614;&#3641;&#3604;&#3606;&#3638;&#3591; Representation &#3610;&#3656;&#3629;&#3618;&#3617;&#3634;&#3585;&#3649;&#3626;&#3604;&#3591;&#3623;&#3656;&#3634;&#3617;&#3633;&#3609;&#3626;&#3635;&#3588;&#3633;&#3597;&#3651;&#3594;&#3656;&#3652;&#3627;&#3617; . ประเด็นหลักของ deep learning คือเราจะหา representation ที่ดีหรือเหมาะสมยังไง ลองคิดดูว่าถ้าเราต้องค้นหาเลขที่เราสนใจจากข้อมูล array ที่มีขนาดใหญ่มาก จาก 2 array ที่มีข้อมูลข้างในเหมือนกันทุกอย่างแต่ต่างกันตรงที่ . Array ที่มีการเรียงจากน้อยไปหามาก | Array ที่ไม่มีการเรียงลำดับ | คิดว่าแบบไหนจะทำให้เราหาเลขที่เราสนใจได้สะดวกกว่ากัน คำตอบก็คือแบบที่ 2 ดังนั้นสิ่งที่ deep learning ทำคือการหา representation ที่ดี เพื่อที่เราจะสามารถแก้ไขปัญหาได้โดยการใช้เพียง simple linear function (output layer ใช้ weight sum ในกรณีต้องการคำตอบที่เป็น continuous value) deep learning ทำการสร้าง hirarchical representation learning โดยเมื่อผ่าน step 1 มาแล้วอาจจะยังไม่ใช่ Representation ที่ดีที่สุด . Note: representation ที่บอกว่าเจอ edges มุมต่างๆไหม ยังไม่มีความหมายในการเอามาใช้บอกได้ว่ามีคนหรือรถอยู่ในภาพไหม เพราะผลลัพธ์จากการตรวจจับว่าเกิดเส้นด้วยมุมต่างๆ มันสามารถเปลี่ยนแปลงได้ตามปัจจัยที่ส่งผลให้ภาพเปลี่ยนแปลงเช่น การเคลื่อนที่ของ object, แสง เป็นต้น (ยกเว้นภาพนี้จะไม่มีปัจจัยเหล่านี้เข้ามากระทบ เราก็อาจจะตั้งสมมุติฐานได้ว่าถ้าเกิดเส้นองศาตามนี้ ที่ตำแหน่งนี้ ก็แปลว่ามีคน แต่ในโลกความเป็นจริงจะไม่เป็นเช่นนี้) . ดังนั้นจึงทำการไป step ต่อไปเรื่อยๆ จนสุดท้ายได้ representation ที่เหมาะสมที่สามารถให้คำตอบของปัญหาได้ง่ายๆโดยใช้สมการที่ไม่ซับซ้อน . &#3648;&#3619;&#3634;&#3626;&#3634;&#3617;&#3634;&#3619;&#3606;&#3617;&#3629;&#3591; Deep Learning &#3648;&#3611;&#3655;&#3609; program &#3652;&#3604;&#3657; &#3650;&#3604;&#3618;&#3617;&#3629;&#3591;&#3651;&#3609;&#3619;&#3641;&#3611;&#3649;&#3610;&#3610; Computation Graph . ความลึกของ Graph คือ steps ในรูปแบบ Sequence (Layers) | ความกว้างของ Graph คือ steps ในรูปแบบ Parallel (Nodes ในแต่ละ layer) | . &quot;We live in a twilight world.&quot; . Tenet .",
            "url": "https://burins.github.io/whyboyburin/deep%20learning/2020/09/17/Brief-Understanding-of-The-World-of-Deep-Learning.html",
            "relUrl": "/deep%20learning/2020/09/17/Brief-Understanding-of-The-World-of-Deep-Learning.html",
            "date": " • Sep 17, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://burins.github.io/whyboyburin/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://burins.github.io/whyboyburin/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}